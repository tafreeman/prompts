# Prompt Effectiveness Scoring Rubric
# Version: 2.0.0
# Updated: 2026-01-04
# Based on: Research from OpenAI Evals, LangSmith, DeepEval, and industry practices
# Reference: docs/RESEARCH_REPORT_2025-11-30.md

# =============================================================================
# OVERVIEW
# =============================================================================
# This rubric defines how to score prompts using a 5-dimensional model with
# weighted scoring. The dimensions focus on actionable, measurable qualities.
#
# Key Changes in v2.0:
#   - Replaced "reusability" with "structure" (more measurable)
#   - Replaced "simplicity" with "specificity" (more actionable)  
#   - Replaced "examples" with "completeness" (broader coverage)
#   - Added detailed evidence requirements
#   - Added example improvement generation
#
# Usage:
#   - Run: python -m prompteval prompts/ --verbose
#   - Results include specific fixes for each criterion
#   - Priority fixes guide improvement order

# =============================================================================
# SCORING DIMENSIONS (v2.0)
# =============================================================================

dimensions:
  clarity:
    weight: 0.25
    description: "Is the prompt unambiguous and easy to understand?"
    focus: "Purpose clarity, variable names, instruction flow, jargon-free"
    criteria:
      9-10: "Crystal clear purpose, zero ambiguity, perfect structure, self-explanatory variables"
      7-8: "Clear purpose, minimal ambiguity, good structure, mostly clear variables"
      5-6: "Understandable but some interpretation needed, decent structure"
      3-4: "Confusing in places, unclear variables, needs improvement"
      1-2: "Very difficult to understand, major rewrites needed"
    evidence_required:
      - "Quote specific unclear passages"
      - "Identify ambiguous variable names"
      - "Note structural issues"
    improvement_template: "Change '{original}' to '{improved}' for clarity"

  effectiveness:
    weight: 0.30
    description: "Does it consistently produce quality output?"
    focus: "Output consistency, edge case handling, platform compatibility"
    criteria:
      9-10: "Produces excellent results 95%+ of time, handles edge cases, works across platforms"
      7-8: "Good results 85%+ of time, decent edge case handling"
      5-6: "Acceptable results 70%+ of time, some inconsistency"
      3-4: "Mixed results 50-70%, often unpredictable"
      1-2: "Frequently fails or produces poor results"
    evidence_required:
      - "Identify potential failure modes"
      - "Note missing edge case handling"
      - "Check output format constraints"
    improvement_template: "Add constraint: '{constraint}' to prevent {failure_mode}"

  structure:
    weight: 0.20
    description: "How well organized and professionally formatted is the prompt?"
    focus: "Markdown formatting, section organization, visual hierarchy"
    criteria:
      9-10: "Perfect markdown, clear sections, excellent headers/lists, professional"
      7-8: "Good formatting, well-organized sections, minor improvements possible"
      5-6: "Adequate structure but could be better organized"
      3-4: "Poorly organized, inconsistent formatting"
      1-2: "No structure, wall of text, unprofessional"
    evidence_required:
      - "Note formatting inconsistencies"
      - "Identify missing sections"
      - "Check heading hierarchy"
    improvement_template: "Add section: '## {section_name}' with {content}"

  specificity:
    weight: 0.15
    description: "How specific and actionable are the instructions?"
    focus: "Concrete instructions, success criteria, defined constraints"
    criteria:
      9-10: "Highly specific instructions, clear success criteria, defined constraints"
      7-8: "Good specificity, reasonable constraints, clear expectations"
      5-6: "Somewhat vague, could be more specific"
      3-4: "Too vague or too restrictive, unclear expectations"
      1-2: "Extremely vague or unusable constraints"
    evidence_required:
      - "Quote vague instructions"
      - "Note missing constraints"
      - "Identify unclear success criteria"
    improvement_template: "Make specific: '{vague}' → '{specific_instruction}'"

  completeness:
    weight: 0.10
    description: "Does the prompt have all necessary components?"
    focus: "Context, instructions, examples, output format, error handling"
    criteria:
      9-10: "Has context, instructions, examples, output format, error handling guidance"
      7-8: "Has most key elements, minor additions needed"
      5-6: "Missing some important elements"
      3-4: "Missing multiple key components"
      1-2: "Severely incomplete, missing critical information"
    evidence_required:
      - "List missing components"
      - "Note incomplete sections"
      - "Check for example coverage"
    improvement_template: "Add missing {component}: {example_content}"

# =============================================================================
# SCORING SCALE
# =============================================================================

scoring_scale:
  excellent:
    range: [4.5, 5.0]
    label: "⭐⭐⭐⭐⭐ Excellent"
    description: "Production-ready, high-quality prompt"
    action: "Promote as featured/recommended"
  
  good:
    range: [4.0, 4.4]
    label: "⭐⭐⭐⭐ Good"
    description: "High-quality prompt ready for use"
    action: "Approve for production use"
  
  acceptable:
    range: [3.0, 3.9]
    label: "⭐⭐⭐ Acceptable"
    description: "Functional prompt, meets minimum standards"
    action: "Approve with optional improvements noted"
  
  below_average:
    range: [2.0, 2.9]
    label: "⭐⭐ Below Average"
    description: "Needs improvement before production use"
    action: "Return for revision, do not publish"
  
  poor:
    range: [1.0, 1.9]
    label: "⭐ Poor"
    description: "Significant issues, not ready for use"
    action: "Major rewrite required or deprecate"

# =============================================================================
# MINIMUM REQUIREMENTS
# =============================================================================

minimum_requirements:
  new_prompts: 3.0
  existing_prompts: 2.5
  featured_prompts: 4.5

# =============================================================================
# QUICK SCORING GUIDE
# =============================================================================
# 
# For rapid scoring, use this abbreviated checklist:
#
# CLARITY (25%):
#   □ Purpose clear within 10 seconds? (+1)
#   □ Variables self-explanatory? (+1)
#   □ No ambiguous phrases? (+1)
#   □ Logical structure? (+1)
#   □ Would work for a newcomer? (+1)
#
# EFFECTIVENESS (30%):
#   □ Works on first try most times? (+1)
#   □ Handles edge cases? (+1)
#   □ Output quality meets expectations? (+1)
#   □ Works across platforms? (+1)
#   □ Consistent results? (+1)
#
# REUSABILITY (20%):
#   □ Works for similar tasks? (+1)
#   □ Variables generic? (+1)
#   □ Minimal per-use modification? (+1)
#   □ Useful to other teams? (+1)
#   □ Not too domain-specific? (+1)
#
# SIMPLICITY (15%):
#   □ No redundant sections? (+1)
#   □ Appropriate length? (+1)
#   □ Every instruction needed? (+1)
#   □ Follows minimal template? (+1)
#   □ Easy to maintain? (+1)
#
# EXAMPLES (10%):
#   □ Has examples? (+1)
#   □ Realistic scenarios? (+1)
#   □ Clear input/output? (+1)
#   □ Demonstrates variables? (+1)
#   □ Shows expected quality? (+1)
#
# Calculate: (clarity_score + effectiveness_score + reusability_score + 
#            simplicity_score + examples_score) / 5 = final_score

# =============================================================================
# FRONTMATTER FIELD
# =============================================================================

frontmatter_field:
  name: "effectivenessScore"
  type: "number"
  format: "decimal (1 decimal place)"
  range: [1.0, 5.0]
  example: "effectivenessScore: 4.2"
  placement: "After reviewStatus, before ---"

# =============================================================================
# EXAMPLE SCORING
# =============================================================================

example_scoring:
  prompt: "code-review-assistant.md"
  scores:
    clarity: 4
    effectiveness: 4
    reusability: 5
    simplicity: 3
    examples: 4
  calculation: "(4*0.25) + (4*0.30) + (5*0.20) + (3*0.15) + (4*0.10) = 4.05"
  final_score: 4.1
  rating: "⭐⭐⭐⭐ Good"
