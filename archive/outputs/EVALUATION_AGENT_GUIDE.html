<!DOCTYPE html>
<html>
<head>
<title>EVALUATION_AGENT_GUIDE.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="evaluation-agent-user-guide">Evaluation Agent User Guide</h1>
<p>The Evaluation Agent is an autonomous system that runs comprehensive evaluations of the prompt library.</p>
<h2 id="overview">Overview</h2>
<p>The evaluation agent automatically:</p>
<ol>
<li>Generates evaluation files for all prompt categories</li>
<li>Runs evaluations using multiple AI models</li>
<li>Analyzes results and calculates pass rates</li>
<li>Identifies failing prompts</li>
<li>Generates improvement recommendations</li>
<li>Creates comprehensive reports</li>
</ol>
<h2 id="quick-start">Quick Start</h2>
<h3 id="run-full-evaluation">Run Full Evaluation</h3>
<pre class="hljs"><code><div><span class="hljs-comment"># Full autonomous evaluation</span>
python tools/evaluation_agent.py --full

<span class="hljs-comment"># Full evaluation in dry-run mode (shows what would happen)</span>
python tools/evaluation_agent.py --full --dry-run

<span class="hljs-comment"># Full evaluation with verbose logging</span>
python tools/evaluation_agent.py --full --verbose
```text
<span class="hljs-comment">### Run Specific Phase</span>

The evaluation is organized into 4 phases based on category priority:

```bash
<span class="hljs-comment"># Run phase 1 only (analysis, business)</span>
python tools/evaluation_agent.py --phase 1

<span class="hljs-comment"># Run phase 2 only (m365, developers)</span>
python tools/evaluation_agent.py --phase 2

<span class="hljs-comment"># Run phase 3 only (system, advanced)</span>
python tools/evaluation_agent.py --phase 3

<span class="hljs-comment"># Run phase 4 only (creative, governance)</span>
python tools/evaluation_agent.py --phase 4
```sql
<span class="hljs-comment">### Resume from Checkpoint</span>

If an evaluation is interrupted, you can resume from the last checkpoint:

```bash
python tools/evaluation_agent.py --resume
```text
<span class="hljs-comment">### Clear Checkpoint</span>

To start fresh and clear any existing checkpoint:

```bash
python tools/evaluation_agent.py --clear-checkpoint
```yaml
<span class="hljs-comment">## Prerequisites</span>

Before running the evaluation agent, ensure you have:

1. **Python 3.8+** installed
2. **GitHub CLI (`gh`)** installed: https://cli.github.com/
3. **gh-models extension** installed: `gh extension install github/gh-models`
4. **Authentication** configured: `gh auth login`

The agent will check these prerequisites automatically and provide guidance <span class="hljs-keyword">if</span> anything is missing.

<span class="hljs-comment">## Configuration</span>

The agent is configured <span class="hljs-keyword">in</span> `tools/evaluation_agent.py` via the `AgentConfig` class:

<span class="hljs-comment">### Categories</span>

Evaluated categories (<span class="hljs-keyword">in</span> priority order):
- **Phase 1**: analysis, business
- **Phase 2**: m365, developers
- **Phase 3**: system, advanced
- **Phase 4**: creative, governance

<span class="hljs-comment">### Models</span>

Each category is evaluated with a specific AI model:
- `analysis`: openai/gpt-5-mini
- `business`: xai/grok-3-mini
- `m365`: openai/gpt-5-mini
- `developers`: openai/o4-mini
- `system`: openai/gpt-4.1-mini
- `advanced`: openai/o3-mini
- `creative`: xai/grok-3-mini
- `governance`: openai/gpt-5-mini

<span class="hljs-comment">### Thresholds</span>

- **Pass Threshold**: 7.0 (out of 10)
- **Target Pass Rate**: 90%
- **Max Parallel Evaluations**: 3

<span class="hljs-comment">## Output</span>

The agent generates several reports:

<span class="hljs-comment">### Evaluation Report</span>
Location: `docs/reports/EVALUATION_REPORT.md`
- Detailed evaluation results
- Category-by-category breakdown
- Individual prompt scores

<span class="hljs-comment">### Agent Execution Summary</span>
Location: `docs/reports/AGENT_EXECUTION_SUMMARY.md`
- Overall statistics
- Pass/fail rates by category
- List of failing prompts
- Execution timeline

<span class="hljs-comment">### Improvement Plan</span>
Location: `docs/reports/IMPROVEMENT_PLAN.md`
- Generated only <span class="hljs-keyword">if</span> prompts fail
- Recommendations <span class="hljs-keyword">for</span> improving failing prompts
- Priority-ordered action items

<span class="hljs-comment">## Checkpoint System</span>

The agent saves checkpoints to `.eval_checkpoint.json` allowing:
- Resume capability after interruption
- State preservation across runs
- Progress tracking

The checkpoint includes:
- Current phase and category
- Completed categories
- Category results
- Task execution <span class="hljs-built_in">history</span>
- Overall metrics

<span class="hljs-comment">## Testing</span>

Run the <span class="hljs-built_in">test</span> suites to verify the agent works correctly:

```bash
<span class="hljs-comment"># Unit tests</span>
python tools/test_evaluation_agent.py

<span class="hljs-comment"># Integration tests</span>
python tools/test_evaluation_agent_integration.py
```sql
<span class="hljs-comment">## Troubleshooting</span>

<span class="hljs-comment">### "GitHub CLI (gh) not found"</span>
Install GitHub CLI from https://cli.github.com/

<span class="hljs-comment">### "gh-models extension not found"</span>
Run: `gh extension install github/gh-models`

<span class="hljs-comment">### "Command failed: authentication required"</span>
Run: `gh auth login` and follow the prompts

<span class="hljs-comment">### Evaluation times out</span>
- Increase `EVAL_TIMEOUT_SECONDS` <span class="hljs-keyword">in</span> `AgentConfig`
- Run with fewer parallel evaluations
- Run specific phases instead of full pipeline

<span class="hljs-comment">### Rate limiting errors</span>
- Increase `DELAY_BETWEEN_EVALS_SECONDS`
- Reduce `MAX_PARALLEL_EVALS`

<span class="hljs-comment">## Advanced Usage</span>

<span class="hljs-comment">### Dry Run Mode</span>

Test what would happen without actually running evaluations:

```bash
python tools/evaluation_agent.py --full --dry-run
```text
This mode:
- Validates prerequisites
- Shows what would be executed
- Doesn<span class="hljs-string">'t make API calls
- Doesn'</span>t generate reports

<span class="hljs-comment">### Verbose Mode</span>

Get detailed debug information:

```bash
python tools/evaluation_agent.py --full --verbose
```text
Useful <span class="hljs-keyword">for</span>:
- Debugging issues
- Understanding execution flow
- Monitoring API calls

<span class="hljs-comment">### Custom Configuration</span>

To customize the evaluation:

1. Edit `AgentConfig` <span class="hljs-keyword">in</span> `tools/evaluation_agent.py`
2. Modify category configurations
3. Adjust thresholds and timeouts
4. Change model assignments

<span class="hljs-comment">## Architecture</span>

The evaluation agent consists of:

<span class="hljs-comment">### Core Components</span>

- **AgentConfig**: Central configuration
- **AgentState**: Persistent state management
- **EvaluationAgent**: Main orchestration class
- **TaskResult**: Task execution tracking
- **CategoryResult**: Per-category results

<span class="hljs-comment">### Execution Flow</span>

1. **Initialization**: Check prerequisites, load/create state
2. **Phase Execution**: Run categories by priority
3. **Evaluation**: Generate <span class="hljs-built_in">eval</span> files, run model evaluations
4. **Analysis**: Parse results, calculate metrics
5. **Reporting**: Generate comprehensive reports

<span class="hljs-comment">### Key Features</span>

- **Parallel Execution**: Run multiple evaluations concurrently
- **Retry Logic**: Automatic retries <span class="hljs-keyword">for</span> failed operations
- **Checkpoint Management**: Resume capability
- **Progress Tracking**: Detailed execution <span class="hljs-built_in">history</span>
- **Error Handling**: Graceful failure recovery

<span class="hljs-comment">## Best Practices</span>

1. **Start with dry-run** to understand what will happen
2. **Run specific phases** during development
3. **Use verbose mode** when troubleshooting
4. **Monitor checkpoints** to track progress
5. **Review reports** after each run

<span class="hljs-comment">## Support</span>

For issues or questions:
1. Check the troubleshooting section above
2. Review <span class="hljs-built_in">test</span> files <span class="hljs-keyword">for</span> usage examples
3. Examine the <span class="hljs-built_in">source</span> code documentation
4. Open an issue <span class="hljs-keyword">in</span> the repository

<span class="hljs-comment">## Version History</span>

- **v1.0** (2025-12-03): Initial release
  - Full autonomous evaluation pipeline
  - Multi-phase execution
  - Checkpoint system
  - Comprehensive reporting
</div></code></pre>

</body>
</html>
