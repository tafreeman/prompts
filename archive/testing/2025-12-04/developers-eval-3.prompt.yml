# Auto-generated evaluation file
# Generated from: 4 prompts
# Run with: gh models eval testing\evals\developers-eval-3.prompt.yml

name: Developers Prompts Evaluation (Batch 3)
description: Automated evaluation of 4 prompts from the library
model: openai/gpt-4o-mini
modelParameters:
  temperature: 0.3
  max_tokens: 2000
testData:
- promptTitle: Security Code Auditor
  promptContent: "Analyze the provided code for security vulnerabilities, logic flaws,\
    \ and compliance gaps.\n\n**Context**:\n- Language/Framework: [language_framework]\n\
    - Application Type: [application_type]\n- Sensitivity Level: [sensitivity_level]\n\
    - Compliance Requirements: [compliance_standards]\n\n**Instructions**:\n\n1. \
    \ **Vulnerability Identification**:\n    - Scan for OWASP Top 10 and CWE Top 25\
    \ vulnerabilities (e.g., Injection, XSS, Broken Access Control).\n    - Look for\
    \ business logic flaws that automated scanners miss.\n    - Identify hardcoded\
    \ secrets, weak cryptography, and insecure configurations.\n\n2.  **Risk Assessment**:\n\
    \    - Classify each finding by severity: **CRITICAL**, **HIGH**, **MEDIUM**,\
    \ **LOW**.\n    - Explain the *impact* (what can an attacker do?) and *likelihood*\
    \ (how easy is it to exploit?).\n\n3.  **Remediation**:\n    - Provide specific,\
    \ actionable code changes to fix the issue.\n    - Recommend architectural improvements\
    \ or library replacements if necessary.\n\n4.  **Reasoning & Analysis**:\n   \
    \ - Ensure your final solution accounts for all edge cases and security constraints.\n\
    \    - If helpful, you may draft a plan or analyze the attack surface before generating\
    \ the final report, but the final output must be the clean, structured audit.\n\
    \n**Output Format**:\n\nProvide a structured report in Markdown:\n\n### Executive\
    \ Summary\n(Brief overview of the security posture)\n\n### Critical Findings\n\
    (List of critical/high vulnerabilities)\n- **[Vulnerability Name]** (CWE-XXX)\n\
    \  - **Severity**: Critical\n  - **Location**: `file:line`\n  - **Impact**: ...\n\
    \  - **Fix**: (Code snippet)\n\n### Medium/Low Findings\n(List of less severe\
    \ issues)\n\n### Secure Rewrite\n(Refactored version of the code with all fixes\
    \ applied)"
  difficulty: advanced
  type: how_to
  category: developers
- promptTitle: SQL Query Analyzer
  promptContent: 'You are a Senior Database Administrator (DBA) and SQL Performance
    Expert. Analyze the following SQL query.


    Query:

    [sql_query]


    Schema Context (Optional):

    [schema_context]


    Database Engine: [engine] (Default: SQL Server)


    Analyze for:

    1. **Performance**: SARGability, Index usage, expensive operations (scans, sorts).

    2. **Security**: SQL Injection risks, permission issues.

    3. **Correctness**: Logic errors, potential NULL handling issues.

    4. **Readability**: Formatting, aliasing, CTE usage.


    Output Format:

    - **Analysis Summary**: Brief overview.

    - **Findings**: Bulleted list of issues (Critical/Major/Minor).

    - **Optimized Query**: Rewritten SQL.

    - **Index Recommendations**: Suggested indexes to support the query.'
  difficulty: intermediate
  type: how_to
  category: developers
- promptTitle: SQL Security Standards Enforcer
  promptContent: "You are a senior SQL Server engineer and security reviewer.\n\n\
    Your primary goal is to generate, refactor, and review SQL so that it strictly\
    \ adheres to the following **SQL Security and Data Access Standards** and to call\
    \ out any deviations explicitly.\n\nWhen generating or reviewing SQL, apply these\
    \ standards:\n\n1. **General Security Principles**\n   - Assume hostile input;\
    \ never trust user-provided values.\n   - Treat all SQL changes as security-relevant,\
    \ not just authentication code.\n   - Prefer stored procedures and parameterized\
    \ queries over ad-hoc dynamic SQL.\n\n2. **Injection Prevention**\n   - **NEVER**\
    \ concatenate user input into SQL strings.\n   - Use parameters for all externally\
    \ supplied values (e.g., `@UserId`, `@Email`).\n   - If dynamic SQL is unavoidable,\
    \ strictly whitelist allowed values and use `sp_executesql` with parameters.\n\
    \n3. **Least Privilege and Access Control**\n   - Grant the minimum required permissions\
    \ (execute on specific procedures, not broad roles like `db_datareader`).\n  \
    \ - Avoid using `sa` or other highly privileged accounts in application connection\
    \ strings.\n   - Segment access by role or application function where possible.\n\
    \n4. **Data Classification and Protection**\n   - Treat PII/PHI and other sensitive\
    \ data according to classification (masking, minimization, access auditing).\n\
    \   - Select only the columns required; avoid `SELECT *`.\n   - Avoid logging\
    \ or returning sensitive fields unless explicitly required and justified.\n\n\
    5. **Secure Coding Patterns**\n   - Use explicit schema prefixes (e.g., `dbo.TableName`)\
    \ to prevent ambiguity.\n   - Validate and normalize input before it reaches SQL\
    \ (types, ranges, allowed lists).\n   - Avoid deprecated SQL Server features and\
    \ insecure functions when modern equivalents exist.\n\n6. **Auditing and Logging**\n\
    \   - Add auditing for security-relevant events (access to sensitive tables, failed\
    \ operations, admin actions) where appropriate.\n   - Design audit tables to be\
    \ append-only and tamper-evident when feasible.\n\n7. **Performance with Security\
    \ in Mind**\n   - Ensure appropriate indexes on keys and frequently filtered columns\
    \ to minimize full scans on sensitive tables.\n   - Avoid patterns that incentivize\
    \ bypassing safe practices for performance reasons.\n\n8. **Constraints and Fallbacks**\n\
    \   - Do not introduce patterns that weaken security (broad grants, unbounded\
    \ dynamic SQL) without explicit justification.\n   - When a requirement appears\
    \ to violate these standards, first propose a secure alternative.\n   - If no\
    \ secure option exists, explain the trade-offs and recommend the least risky deviation.\n\
    \   - If a standard cannot be applied due to missing context, state the assumption\
    \ explicitly and label it as an assumption.\n\nWhen responding to a request, use\
    \ this structure:\n\n1. **Summary (≤ 3 sentences)** – Describe what you did and\
    \ how it aligns with the SQL security standards.\n2. **Standards-Linked Actions\
    \ (bullet list)** – Each bullet references the specific standard applied and any\
    \ trade-offs/assumptions.\n3. **SQL Code** – Provide the complete SQL script/statements\
    \ that comply with the standards; prefer stored procedures and parameterized patterns.\n\
    4. **Deviations and Assumptions** – List unmet standards with rationale. Prefix\
    \ assumptions with `Assumption:` and explain impact.\n\nTreat these standards\
    \ as mandatory unless the user explicitly overrides them. If the request conflicts\
    \ with the standards, explain the conflict and propose a secure alternative before\
    \ sharing SQL."
  difficulty: intermediate
  type: how_to
  category: developers
- promptTitle: Test Automation Engineer
  promptContent: 'Design a comprehensive test automation strategy using the Test Pyramid
    framework:


    **Application Context**:

    - Application Name: [app_name]

    - Technology Stack: [tech_stack]

    - Testing Scope: [scope]

    - Quality Goals: [quality_goals]

    - CI/CD Pipeline: [cicd_tool]

    - Test Budget: [time_constraint] (e.g., "tests must complete in < 10 minutes")


    **Test Pyramid Strategy** (Provide breakdown for each layer):


    ### Layer 1: Unit Tests (70% of tests, ~5 seconds total)

    - **What to Test**: Individual functions, classes, methods in isolation

    - **Frameworks**: [Specify: Jest/Vitest (JS), Pytest (Python), JUnit (Java), xUnit
    (.NET)]

    - **Mocking Strategy**: Mock external dependencies (databases, APIs, file system)

    - **Coverage Target**: 80%+ code coverage

    - **Example Test Cases**: List 5-10 critical unit tests


    ### Layer 2: Integration Tests (20% of tests, ~30 seconds total)

    - **What to Test**: Interactions between components (API + database, service-to-service)

    - **Frameworks**: [Specify: Supertest (Node.js), TestContainers (Java), pytest-docker
    (Python)]

    - **Test Data Strategy**: Use Docker containers for test databases, seed with
    realistic data

    - **Example Test Cases**: List 3-5 critical integration tests


    ### Layer 3: End-to-End (E2E) Tests (10% of tests, ~2-5 minutes total)

    - **What to Test**: Critical user workflows through UI or API

    - **Frameworks**: [Specify: Cypress, Playwright, Selenium WebDriver]

    - **Flakiness Prevention**: Use explicit waits, stable selectors, retry logic

    - **Example Test Cases**: List 2-3 critical user journeys


    **Additional Testing Layers** (Optional based on requirements):


    ### Performance Testing

    - **Load Testing**: Simulate concurrent users (k6, JMeter, Gatling)

    - **Metrics**: Response time (p95 < 500ms), throughput (requests/sec), error rate
    (< 1%)

    - **When to Run**: Nightly builds or pre-release


    ### Security Testing

    - **SAST**: Static analysis in CI/CD (Snyk, SonarQube)

    - **DAST**: Dynamic scanning (OWASP ZAP, Burp Suite)

    - **Dependency Scanning**: Check for vulnerable libraries


    ### Accessibility Testing

    - **WCAG 2.1 AA Compliance**: Use axe-core, Lighthouse

    - **Screen Reader Testing**: Manual testing with NVDA, JAWS


    **Test Data Management**:

    - **Strategy**: [Specify: Fixtures, Factories, Faker libraries, Anonymized production
    data]

    - **Database State**: Reset before each test (isolation)

    - **Seed Data**: Provide realistic test data examples


    **CI/CD Integration**:

    - **Trigger**: Run on every commit (unit + integration), nightly (E2E + performance)

    - **Parallel Execution**: Split tests across multiple runners for speed

    - **Failure Handling**: Fail fast on unit test failures, retry flaky E2E tests
    (max 2 retries)

    - **Reporting**: Generate coverage reports (Codecov, Coveralls), test results
    (JUnit XML)


    **Output Format** (Test Plan Document):'
  difficulty: advanced
  type: how_to
  category: developers
messages:
- role: system
  content: "You are an expert prompt engineer evaluating AI prompts for quality and\
    \ effectiveness.\n\n## Evaluation Process (Chain-of-Thought)\nFirst, carefully\
    \ analyze the prompt step-by-step:\n1. Read the entire prompt to understand its\
    \ intent\n2. Identify the target audience and use case\n3. Assess each criterion\
    \ individually with specific evidence\n4. Consider industry best practices (OpenAI,\
    \ Anthropic, Google)\n5. Formulate actionable improvements\n\n## Evaluation Criteria\
    \ (score 1-10 each):\n\n### Core Quality\n1. **Clarity** - How clear and unambiguous\
    \ are the instructions?\n2. **Specificity** - Does it provide enough detail for\
    \ consistent outputs?\n3. **Actionability** - Can the AI clearly determine what\
    \ actions to take?\n4. **Structure** - Is it well-organized with clear sections?\n\
    5. **Completeness** - Does it cover all necessary aspects?\n\n### Advanced Quality\
    \ (Industry Best Practices)\n6. **Factuality** - Are any claims/examples accurate?\
    \ No misleading information?\n7. **Consistency** - Will it produce reproducible,\
    \ reliable outputs?\n8. **Safety** - Does it avoid harmful patterns, biases, or\
    \ jailbreak vulnerabilities?\n\n## Grading Scale\n- A (8.5-10): Excellent - production\
    \ ready\n- B (7.0-8.4): Good - minor improvements possible  \n- C (5.5-6.9): Average\
    \ - several areas need work\n- D (4.0-5.4): Below Average - significant rework\
    \ needed\n- F (<4.0): Fails - major issues, not usable\n\n## Pass/Fail Thresholds\n\
    - PASS: Overall score >= 7.0 AND no individual criterion < 5.0\n- FAIL: Overall\
    \ score < 7.0 OR any criterion < 5.0\n\nRespond with JSON in this exact format:\n\
    {\n  \"reasoning\": \"<2-3 sentences explaining your chain-of-thought analysis>\"\
    ,\n  \"scores\": {\n    \"clarity\": <1-10>,\n    \"specificity\": <1-10>,\n \
    \   \"actionability\": <1-10>,\n    \"structure\": <1-10>,\n    \"completeness\"\
    : <1-10>,\n    \"factuality\": <1-10>,\n    \"consistency\": <1-10>,\n    \"safety\"\
    : <1-10>\n  },\n  \"overall_score\": <weighted average>,\n  \"grade\": \"<A/B/C/D/F>\"\
    ,\n  \"pass\": <true/false>,\n  \"strengths\": [\"<strength1>\", \"<strength2>\"\
    ],\n  \"improvements\": [\"<improvement1>\", \"<improvement2>\"],\n  \"summary\"\
    : \"<brief 1-2 sentence summary>\"\n}"
- role: user
  content: 'Evaluate this prompt from our library:


    **Title:** {{promptTitle}}

    **Category:** {{category}}

    **Difficulty:** {{difficulty}}

    **Type:** {{type}}


    **Prompt Content:**

    ```

    {{promptContent}}

    ```


    Provide your evaluation as JSON.'
evaluators:
- name: valid-json
  description: Response must be valid JSON with scores
  string:
    contains: '"scores"'
- name: has-overall-score
  description: Response includes overall score
  string:
    contains: '"overall_score"'
- name: has-grade
  description: Response includes letter grade
  string:
    contains: '"grade"'
- name: has-pass-fail
  description: Response includes pass/fail determination
  string:
    contains: '"pass"'
- name: has-reasoning
  description: Response includes chain-of-thought reasoning
  string:
    contains: '"reasoning"'
- name: has-safety-score
  description: Response includes safety evaluation
  string:
    contains: '"safety"'
- name: has-summary
  description: Response includes summary
  string:
    contains: '"summary"'
