# Prompt Effectiveness Scoring Rubric
# Version: 1.0.0
# Created: 2025-11-30
# Based on: Research from OpenAI Evals, LangSmith, and industry practices
# Reference: docs/RESEARCH_REPORT_2025-11-30.md

# =============================================================================
# OVERVIEW
# =============================================================================
# This rubric defines how to score prompts in the library using a 5-dimensional
# model with weighted scoring. Each prompt receives a single `effectivenessScore`
# value (1.0-5.0) that represents its overall quality.
#
# Usage:
#   - Add `effectivenessScore: 4.2` to prompt frontmatter after evaluation
#   - Minimum score for new prompts: 3.0
#   - Prompts below 3.0 should be improved or removed

# =============================================================================
# SCORING DIMENSIONS
# =============================================================================

dimensions:
  clarity:
    weight: 0.25
    description: "Is the prompt unambiguous and easy to understand?"
    criteria:
      5: "Immediately understandable, no ambiguity, clear structure"
      4: "Clear with minor room for interpretation"
      3: "Understandable but requires some thought"
      2: "Confusing in places, needs improvement"
      1: "Very difficult to understand, major rewrites needed"
    evaluation_questions:
      - "Can a user understand the purpose within 10 seconds?"
      - "Are variable names self-explanatory?"
      - "Is the instruction structure logical?"
      - "Are there ambiguous phrases or jargon?"

  effectiveness:
    weight: 0.30
    description: "Does it consistently produce quality output?"
    criteria:
      5: "Works reliably 95%+ of the time with excellent results"
      4: "Works well 85%+ of the time with good results"
      3: "Works 70%+ of the time with acceptable results"
      2: "Works 50-70% of the time with mixed results"
      1: "Frequently fails or produces poor results"
    evaluation_questions:
      - "Does the prompt produce consistent outputs?"
      - "Does it handle edge cases gracefully?"
      - "Is the output quality meeting expectations?"
      - "Does it work across different AI platforms?"

  reusability:
    weight: 0.20
    description: "Works across different contexts/inputs?"
    criteria:
      5: "Works across many contexts unchanged"
      4: "Works in most contexts with minor adjustments"
      3: "Works in several contexts with some modifications"
      2: "Limited to specific contexts"
      1: "Single-use only, highly specific"
    evaluation_questions:
      - "Can this prompt be used for similar tasks?"
      - "Are variables generic enough for multiple use cases?"
      - "Does it require significant modification per use?"
      - "Would other teams find this useful?"

  simplicity:
    weight: 0.15
    description: "Minimal without losing value?"
    criteria:
      5: "Minimal, nothing to remove, everything essential"
      4: "Concise with minor redundancy"
      3: "Some unnecessary content but not excessive"
      2: "Significant bloat or redundancy"
      1: "Significantly over-engineered"
    evaluation_questions:
      - "Can any section be removed without losing value?"
      - "Is the prompt length appropriate for its purpose?"
      - "Are there redundant instructions?"
      - "Does it follow the minimal structure template?"

  examples:
    weight: 0.10
    description: "Are examples helpful and realistic?"
    criteria:
      5: "Excellent examples with clear input/output, realistic scenarios"
      4: "Good examples that demonstrate usage well"
      3: "Adequate examples but could be improved"
      2: "Examples present but not very helpful"
      1: "Missing or poor examples"
    evaluation_questions:
      - "Do examples show realistic use cases?"
      - "Is input/output clearly demonstrated?"
      - "Are examples comprehensive enough?"
      - "Do examples help understand edge cases?"

# =============================================================================
# SCORING SCALE
# =============================================================================

scoring_scale:
  excellent:
    range: [4.5, 5.0]
    label: "⭐⭐⭐⭐⭐ Excellent"
    description: "Production-ready, high-quality prompt"
    action: "Promote as featured/recommended"
  
  good:
    range: [4.0, 4.4]
    label: "⭐⭐⭐⭐ Good"
    description: "High-quality prompt ready for use"
    action: "Approve for production use"
  
  acceptable:
    range: [3.0, 3.9]
    label: "⭐⭐⭐ Acceptable"
    description: "Functional prompt, meets minimum standards"
    action: "Approve with optional improvements noted"
  
  below_average:
    range: [2.0, 2.9]
    label: "⭐⭐ Below Average"
    description: "Needs improvement before production use"
    action: "Return for revision, do not publish"
  
  poor:
    range: [1.0, 1.9]
    label: "⭐ Poor"
    description: "Significant issues, not ready for use"
    action: "Major rewrite required or deprecate"

# =============================================================================
# MINIMUM REQUIREMENTS
# =============================================================================

minimum_requirements:
  new_prompts: 3.0
  existing_prompts: 2.5
  featured_prompts: 4.5

# =============================================================================
# QUICK SCORING GUIDE
# =============================================================================
# 
# For rapid scoring, use this abbreviated checklist:
#
# CLARITY (25%):
#   □ Purpose clear within 10 seconds? (+1)
#   □ Variables self-explanatory? (+1)
#   □ No ambiguous phrases? (+1)
#   □ Logical structure? (+1)
#   □ Would work for a newcomer? (+1)
#
# EFFECTIVENESS (30%):
#   □ Works on first try most times? (+1)
#   □ Handles edge cases? (+1)
#   □ Output quality meets expectations? (+1)
#   □ Works across platforms? (+1)
#   □ Consistent results? (+1)
#
# REUSABILITY (20%):
#   □ Works for similar tasks? (+1)
#   □ Variables generic? (+1)
#   □ Minimal per-use modification? (+1)
#   □ Useful to other teams? (+1)
#   □ Not too domain-specific? (+1)
#
# SIMPLICITY (15%):
#   □ No redundant sections? (+1)
#   □ Appropriate length? (+1)
#   □ Every instruction needed? (+1)
#   □ Follows minimal template? (+1)
#   □ Easy to maintain? (+1)
#
# EXAMPLES (10%):
#   □ Has examples? (+1)
#   □ Realistic scenarios? (+1)
#   □ Clear input/output? (+1)
#   □ Demonstrates variables? (+1)
#   □ Shows expected quality? (+1)
#
# Calculate: (clarity_score + effectiveness_score + reusability_score + 
#            simplicity_score + examples_score) / 5 = final_score

# =============================================================================
# FRONTMATTER FIELD
# =============================================================================

frontmatter_field:
  name: "effectivenessScore"
  type: "number"
  format: "decimal (1 decimal place)"
  range: [1.0, 5.0]
  example: "effectivenessScore: 4.2"
  placement: "After reviewStatus, before ---"

# =============================================================================
# EXAMPLE SCORING
# =============================================================================

example_scoring:
  prompt: "code-review-assistant.md"
  scores:
    clarity: 4
    effectiveness: 4
    reusability: 5
    simplicity: 3
    examples: 4
  calculation: "(4*0.25) + (4*0.30) + (5*0.20) + (3*0.15) + (4*0.10) = 4.05"
  final_score: 4.1
  rating: "⭐⭐⭐⭐ Good"
