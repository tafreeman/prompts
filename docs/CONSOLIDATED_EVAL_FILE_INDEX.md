---
title: Consolidated Evaluation & Scoring File Index
generated: 2025-12-22
description: "Repository-wide index of files related to prompt evaluation, scoring, tooling, test cases, and results. Use this as a canonical reference for building PromptEval."
---

# Consolidated Evaluation & Scoring File Index

This index lists files in the repository that are related to prompt evaluation and scoring (research, methodology, tools, eval cases, results, tests, and prompts that implement evaluators). Use this table as the canonical inventory when designing a standalone PromptEval tool.

| Path | Category | Short description |
|---|---|---|
| `docs/prompt-evaluation-research.md` | Research | Industry standards & best practices for evaluation scoring (G-Eval, RubricEval, MT-Bench) |
| `docs/prompt-effectiveness-scoring-methodology.md` | Methodology | Scoring rubric and dimensions (Clarity, Structure, Usefulness, Technical, Ease of Use) |
| `docs/PROMPTEVAL_IMPLEMENTATION_PLAN.md` | Design | High-level design and architecture plan for PromptEval tool |
| `docs/SCORECARD.md` | Artifact | Prompt scorecard (repository score outputs / reference) |
| `docs/EVALUATION_EXECUTION_PLAN.md` | Process | Workflow and execution plan for running evaluations |
| `docs/osint_tool_evaluation_report.md` | Report | Example tool evaluation report (OSINT) |
| `docs/cove_analysis_report*.json` | Data | CoVE output JSON reports (analysis outputs) |
| `docs/evaluations/PROMPT_EVAL_advanced_claude-opus-4.5.md` | Output | Prompt evaluation result example (Claude) |
| `docs/evaluations/REPO_EVAL_claude-opus-4.5.md` | Output | Repository-level evaluation report (Claude) |
| `docs/reports/FULL_EVALUATION_REPORT.md` | Output | Full library evaluation report (aggregate) |
| `docs/reports/*.md` | Output | Timestamped tier reports generated by `tiered_eval.py` (tier0/tier1 etc.) |
| `tools/tiered_eval.py` | Tooling | Multi-tier evaluation system (Tiers 0-7) â€” core executable used in CI/tasks |
| `tools/batch_evaluate.py` | Tooling | Batch evaluation runner for folders of prompts |
| `tools/evaluate_library.py` | Tooling | Library-level evaluator and report generator |
| `tools/generate_eval_files.py` | Tooling | Generator for `.prompt.yml` eval cases from prompts |
| `tools/run_gh_eval.py` | Tooling | Runner that invokes GitHub Models eval (gh models eval) |
| `tools/run_eval_geval.py` / `run_eval_geval_2.py` | Tooling | Wrappers for running "geval" style evaluations and collectors |
| `tools/run_eval_direct.py` | Tooling | Direct eval execution (provider-specific) |
| `tools/evaluation_agent.py` | Orchestration | Orchestrates generation + parallel execution + parsing of eval runs |
| `tools/cove_batch_analyzer.py` / `tools/cove_runner.py` | Tooling | CoVE integration (batch analysis) |
| `tools/benchmarks/performance_evaluator.py` | Benchmarks | Performance evaluation helpers for prompt latency/throughput tests |
| `tools/validators/score_validator.py` | Validators | Validators for score formats / thresholds |
| `testing/evals/*.prompt.yml` | Eval cases | `.prompt.yml` test cases (business/system/advanced categories) |
| `testing/evals/results/*` | Results | Evaluation run outputs (json / md) |
| `testing/evals/dual_eval.py` | Tooling | Dual-evaluator harness used by tests |
| `testing/tool_tests/test_evaluation_agent.py` | Tests | Unit tests for evaluation agent and flows |
| `testing/integration/test_evaluation_agent_e2e.py` | Tests | End-to-end integration tests for eval flows |
| `_archive/tools/EVALUATION_AGENT_GUIDE.md` | Docs (archived) | Guide for the evaluation agent (archived reference) |
| `_archive/tools/evaluation_agent.py` | Orchestration (archived) | Archived evaluation agent implementation |
| `prompts/system/prompt-quality-evaluator.md` | Prompt | Prompt pattern for quality evaluation |
| `prompts/system/tree-of-thoughts-repository-evaluator.md` | Prompt | ToT-style evaluator prompt for repo-level scoring |
| `prompts/advanced/tree-of-thoughts-architecture-evaluator.md` | Prompt | ToT architecture evaluation template (scoring matrix) |
| `toolkit/prompts/evaluation/*` | Prompts | Toolkit prompts for evaluation (ToT, quality, CoVE audit) |
| `frameworks/enterprise-prompt-evaluation-framework.md` | Reference | Framework-level recommendations and patterns for enterprise evals |
| `frameworks/*` | Reference | Framework-specific evaluation patterns (OpenAI, Anthropic, LangChain, Microsoft) |
| `eval_direct_results.json` / `eval_geval_results*.json` | Data | Aggregated result files produced by different eval runners |

> Note: This index was generated from a repository-wide audit (searching for eval/evaluation/score-related artifacts, plus manual curation). Use it as the single source of truth when mapping capabilities into a standalone PromptEval tool.
