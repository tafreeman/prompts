{
  "version": "3.0",
  "title": "Workflow Execution + Evaluation Overhaul Plan",
  "status": "draft-ready-for-execution",
  "storage": {
    "source_markdown_file": "docs/planning/workflow-eval-multiagent-execution-plan.md",
    "json_file": "docs/planning/workflow-eval-multiagent-execution-plan.json",
    "update_mode": "replace-markdown-content-with-v3-structure"
  },
  "summary": {
    "objective": "Redesign UI-driven workflow execution and evaluation for extensibility across dataset schemas, add SWE-style iterative bug-fix workflows in isolated Docker environments, and score with a hybrid policy emphasizing requirement satisfaction.",
    "scope": "Python SWE-first for v1; preserve existing DAG UX while adding iterative run-and-score features.",
    "success_criteria": [
      "Capability-driven workflow/dataset compatibility",
      "Execution-strategy abstraction for dag_once and iterative_repair",
      "Containerized repo checkout/build/test/patch loop",
      "Hybrid score with objective test dominance",
      "Live UI iteration timeline and artifact explorer",
      "Backward compatibility for existing workflows and endpoints"
    ]
  },
  "decisions": {
    "scope": "python-swe-first",
    "isolation_runtime_default": "docker-per-task",
    "scoring_mode": "hybrid-weighted",
    "scoring_weights": {
      "tests": 0.6,
      "llm_judge": 0.3,
      "patch_similarity": 0.1
    },
    "repository_source_policy": "git-mirrors-with-local-cache-and-commit-pinning"
  },
  "current_state_review": {
    "workflows_present": [
      "code_review",
      "fullstack_generation",
      "plan_implementation"
    ],
    "evaluation_api_present": [
      "GET /api/eval/datasets",
      "POST /api/run (evaluation payload)",
      "SSE streaming with evaluation_start/evaluation_complete"
    ],
    "repository_datasets_current": [
      "swe-bench",
      "swe-bench-lite",
      "swe-bench-verified",
      "humaneval",
      "humaneval-plus",
      "mbpp",
      "mbpp-sanitized",
      "codeclash"
    ],
    "local_datasets_current": [
      "agentic-workflows-v2/tests/fixtures/datasets/code_instructions_120k.json",
      "agentic-workflows-v2/tests/fixtures/datasets/code_review_instruct.json",
      "agentic-workflows-v2/tests/fixtures/datasets/codeparrot_apps.json",
      "agentic-workflows-v2/tests/fixtures/datasets/humaneval.json",
      "agentic-workflows-v2/tests/fixtures/datasets/mbpp.json",
      "agentic-workflows-v2/tests/fixtures/datasets/python_code_instructions_18k.json",
      "agentic-workflows-v2/tests/fixtures/datasets/react_code_instructions.json",
      "agentic-workflows-v2/tests/fixtures/datasets/swe_bench_lite.json",
      "agentic-workflows-v2/tests/fixtures/datasets/swe_bench_verified.json"
    ],
    "known_gaps": [
      "No native iterative repair strategy in engine",
      "No integrated containerized SWE execution lifecycle in /api/run path",
      "No workflow capability endpoint for compatibility reasoning",
      "No first-class artifact manifest endpoint",
      "Current scoring does not enforce objective repo test outcomes as primary pass gate"
    ]
  },
  "multi_pass_structure": {
    "pass_1": {
      "name": "Discovery and Capability Baseline",
      "outcomes": [
        "Canonical capability model",
        "Dataset adapter contract",
        "Execution strategy taxonomy",
        "Hybrid scoring contract",
        "No unresolved interface decisions"
      ],
      "gate": "Pass 2 can start only if all interface-level decisions are closed"
    },
    "pass_2": {
      "name": "Architecture and Interfaces",
      "outcomes": [
        "Public API and shared type definitions",
        "Runtime strategy and isolation specifications",
        "Scoring formula and gate semantics",
        "Live event protocol and UI integration contracts"
      ],
      "gate": "Pass 3 can start only if API/types are versioned and testable"
    },
    "pass_3": {
      "name": "Delivery, Testing, Rollout",
      "outcomes": [
        "Phased implementation and acceptance gates",
        "Comprehensive test matrix",
        "Security and operations controls",
        "Rollout and observability plan"
      ],
      "gate": "Execution-ready with no open design decisions"
    }
  },
  "architecture": {
    "components": [
      "WorkflowCapabilityRegistry",
      "DatasetAdapterRegistry",
      "ExecutionStrategyEngine",
      "IsolatedTaskRuntimeDocker",
      "HybridEvaluationOrchestrator",
      "RunArtifactStore",
      "LiveEventPublisher"
    ],
    "execution_strategies": {
      "dag_once": {
        "description": "Current DAG flow for existing workflows.",
        "compatibility_goal": "Backward compatible behavior and payloads."
      },
      "iterative_repair": {
        "description": "SWE bug-fix loop with retry until pass criteria or limits reached.",
        "flow": [
          "Normalize dataset sample into workflow inputs",
          "Create isolated workspace",
          "Clone from mirror and checkout base_commit",
          "Generate candidate patch",
          "Apply patch and run build/tests",
          "Evaluate attempt and decide continue/stop",
          "Persist artifacts and best result"
        ],
        "defaults": {
          "max_attempts": 5,
          "max_duration_minutes": 45
        }
      }
    },
    "data_flow": [
      "UI selects workflow, dataset, strategy, and evaluation policy",
      "Backend validates capability compatibility",
      "Execution strategy runs (dag_once or iterative_repair)",
      "Events stream to UI in real-time",
      "Artifacts and score persisted",
      "Run detail and artifacts queryable post-run"
    ]
  },
  "api_changes": {
    "new_endpoints": [
      {
        "method": "GET",
        "path": "/api/workflows/{name}/capabilities",
        "purpose": "Expose workflow capability contracts for compatibility and UI generation."
      },
      {
        "method": "GET",
        "path": "/api/runs/{run_id}/artifacts",
        "purpose": "Retrieve patch, logs, judge rationale, and runtime metadata."
      }
    ],
    "extended_endpoints": [
      {
        "method": "GET",
        "path": "/api/eval/datasets",
        "add_fields": [
          "dataset_family",
          "schema_version",
          "languages",
          "requires_container",
          "supports_gold_patch",
          "required_workflow_capabilities"
        ]
      },
      {
        "method": "POST",
        "path": "/api/run",
        "add_fields": {
          "execution_profile": [
            "strategy",
            "max_attempts",
            "max_duration_minutes",
            "container_image",
            "resource_limits"
          ],
          "dataset_selection": [
            "dataset_id",
            "sample_index",
            "adapter_id"
          ],
          "evaluation_policy": [
            "weights",
            "pass_criteria",
            "judge_profile"
          ]
        }
      }
    ],
    "event_stream_additions": [
      "iteration_start",
      "iteration_patch_generated",
      "iteration_patch_apply_result",
      "iteration_build_result",
      "iteration_test_result",
      "iteration_judge_result",
      "iteration_end",
      "run_artifact_ready"
    ]
  },
  "types_and_interfaces": {
    "new_types": [
      "WorkflowCapability",
      "DatasetDescriptor",
      "DatasetSampleNormalized",
      "ExecutionProfile",
      "EvaluationPolicy",
      "PatchArtifact",
      "HybridEvaluationResult",
      "RunArtifactManifest",
      "IterationEvent"
    ],
    "compatibility_rules": [
      "Each workflow declares required capabilities and accepted dataset families.",
      "Each dataset declares schema family and runtime requirements.",
      "Run request fails fast with explicit compatibility errors when mismatched."
    ]
  },
  "scoring_policy": {
    "mode": "hybrid_weighted",
    "weights": {
      "tests": 0.6,
      "llm_judge": 0.3,
      "patch_similarity": 0.1
    },
    "test_score_formula": "test_score = 100 * (0.7 * f2p_rate + 0.3 * p2p_rate)",
    "final_score_formula": "final_score = 0.60 * test_score + 0.30 * llm_judge_score + 0.10 * patch_similarity_score",
    "hard_gates": [
      "f2p_rate == 1.0",
      "p2p_rate >= 0.95"
    ],
    "soft_gate": "final_score >= 70",
    "notes": [
      "Golden patch similarity is advisory and not a hard pass criterion.",
      "Requirement satisfaction is prioritized over exact gold-patch equivalence."
    ]
  },
  "docker_isolation_spec": {
    "security_defaults": [
      "No privileged containers",
      "Restricted network by default",
      "Explicit CPU/memory/time limits",
      "Ephemeral workspace with guaranteed cleanup",
      "No arbitrary host path access"
    ],
    "runtime_requirements": [
      "Docker engine on workers",
      "Pinned image versions",
      "Mirror-based repo source retrieval"
    ]
  },
  "workflow_portfolio": [
    {
      "id": "swe_bugfix_iterative",
      "priority": 1,
      "strategy": "iterative_repair",
      "datasets": [
        "swe-bench-lite",
        "swe-bench-verified"
      ],
      "primary_metric": "hard-gate test pass rate with hybrid score reporting"
    },
    {
      "id": "function_codegen_tested",
      "priority": 2,
      "strategy": "iterative_with_tests",
      "datasets": [
        "humaneval-plus",
        "mbpp-sanitized",
        "apps"
      ],
      "primary_metric": "pass@k and deterministic test outcomes"
    },
    {
      "id": "repo_code_review_quality",
      "priority": 2,
      "strategy": "dag_once",
      "datasets": [
        "code_review_instruct",
        "python_code_instructions_18k"
      ],
      "primary_metric": "quality rubric with evidence-backed scoring"
    },
    {
      "id": "fullstack_feature_delivery",
      "priority": 3,
      "strategy": "staged_generation_with_validation",
      "datasets": [
        "codeclash",
        "local feature specs"
      ],
      "primary_metric": "requirement completion and integration test pass"
    }
  ],
  "delivery_phases": [
    {
      "phase": 0,
      "name": "Contracts and Compatibility Foundation",
      "tasks": [
        "Add capability and dataset descriptors",
        "Add compatibility validator",
        "Version shared API contracts"
      ],
      "exit_criteria": [
        "Schema-level tests pass",
        "Mismatches return explicit actionable errors"
      ]
    },
    {
      "phase": 1,
      "name": "Isolated Runtime Foundation",
      "tasks": [
        "Implement Docker task runtime",
        "Implement mirror+cache repo fetcher",
        "Add secure workspace lifecycle manager"
      ],
      "exit_criteria": [
        "Fixture repo clone/checkouts deterministic",
        "Container security checks pass"
      ]
    },
    {
      "phase": 2,
      "name": "Iterative Repair Strategy",
      "tasks": [
        "Integrate iterative_repair strategy",
        "Implement attempt loop controls",
        "Persist attempt-level artifacts"
      ],
      "exit_criteria": [
        "Loop converges or exits cleanly by policy",
        "Artifacts queryable per attempt"
      ]
    },
    {
      "phase": 3,
      "name": "Hybrid Evaluation Engine",
      "tasks": [
        "Compute objective test score",
        "Run judge rubric scoring",
        "Compute patch similarity and final score"
      ],
      "exit_criteria": [
        "Score math deterministic and tested",
        "Hard/soft gate behavior verified"
      ]
    },
    {
      "phase": 4,
      "name": "UI Extensibility Overhaul",
      "tasks": [
        "Build schema-driven run configuration UI",
        "Add compatibility matrix",
        "Add live iteration timeline and artifact explorer"
      ],
      "exit_criteria": [
        "Current live execution feel preserved",
        "New iterative views function end-to-end"
      ]
    },
    {
      "phase": 5,
      "name": "Dataset Expansion and Validation Packs",
      "tasks": [
        "Productionize SWE lanes",
        "Add function-level rigorous eval lanes",
        "Prepare rolling real-world benchmark lane"
      ],
      "exit_criteria": [
        "Benchmark smoke matrix green",
        "Adapters validated for each dataset family"
      ]
    },
    {
      "phase": 6,
      "name": "Rollout and Operations",
      "tasks": [
        "Feature-flag rollout by strategy and dataset family",
        "Observability dashboards and alerting",
        "Runbooks and rollback drills"
      ],
      "exit_criteria": [
        "Release readiness gate accepted",
        "On-call docs complete"
      ]
    }
  ],
  "test_plan": {
    "unit": [
      "Dataset adapter normalization for all supported schemas",
      "Capability compatibility and mismatch reason generation",
      "Hybrid scoring formulas and gate logic",
      "Patch similarity determinism",
      "Event payload schema validation"
    ],
    "integration": [
      "SWE iterative loop on fixture repos",
      "Docker lifecycle and cleanup",
      "Mirror cache hit/miss and commit pin behavior",
      "Artifact persistence and retrieval",
      "SSE event sequencing"
    ],
    "e2e_ui": [
      "Workflow + dataset + strategy selection path",
      "Live iteration timeline rendering",
      "Score breakdown and rationale display",
      "Patch and log artifact inspection",
      "Backwards-compatible dag_once run path"
    ],
    "non_functional": [
      "Security hardening checks for sandbox and path traversal",
      "Resource limit enforcement",
      "Latency and throughput under parallel load",
      "Judge failure/fallback behavior"
    ],
    "acceptance_gates": [
      "Gate A: contracts and compatibility accepted",
      "Gate B: isolated runtime and strategy accepted",
      "Gate C: scoring and evidence accepted",
      "Gate D: UI and e2e accepted",
      "Gate E: CI/security/release readiness accepted"
    ]
  },
  "dataset_recommendations": {
    "v1": [
      "SWE-bench Lite",
      "SWE-bench Verified",
      "HumanEval+ (EvalPlus)",
      "MBPP+ / MBPP sanitized"
    ],
    "v1_5": [
      "SWE-bench-Live",
      "LiveCodeBench",
      "RepoBench (repo-level retrieval/completion checks)"
    ],
    "v2": [
      "BugsInPy",
      "Defects4J",
      "SWE-bench Multilingual / Multi-SWE-bench"
    ]
  },
  "risks_and_mitigations": [
    {
      "risk": "Flaky upstream repo tests create unstable scoring.",
      "mitigation": "Introduce flaky-test quarantine profile and repeatable verification mode."
    },
    {
      "risk": "Mirror/source drift undermines reproducibility.",
      "mitigation": "Commit pinning and immutable cache metadata."
    },
    {
      "risk": "Judge model variability impacts score consistency.",
      "mitigation": "Low-temperature fixed prompts and optional adjudication run."
    },
    {
      "risk": "Execution cost escalation from iterative retries.",
      "mitigation": "Attempt/time caps and early-stop policies."
    },
    {
      "risk": "Security exposure when running untrusted repo code.",
      "mitigation": "Strict container policy, restricted network, no privileged mode, ephemeral teardown."
    }
  ],
  "assumptions": [
    "Docker is available in evaluation environments.",
    "Git mirror infrastructure is available and maintained.",
    "Python-first scope is acceptable for v1 timelines.",
    "Golden patch exact match is not a product requirement.",
    "Existing workflows must remain backward compatible under dag_once."
  ],
  "references": [
    {
      "name": "SWE-bench paper",
      "url": "https://arxiv.org/abs/2310.06770"
    },
    {
      "name": "SWE-bench website",
      "url": "https://www.swebench.com/"
    },
    {
      "name": "SWE-bench Lite",
      "url": "https://www.swebench.com/lite.html"
    },
    {
      "name": "SWE-bench Live",
      "url": "https://swe-bench-live.github.io/"
    },
    {
      "name": "SWE-bench Verified dataset",
      "url": "https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified"
    },
    {
      "name": "LiveCodeBench",
      "url": "https://arxiv.org/abs/2403.07974"
    },
    {
      "name": "EvalPlus (HumanEval+/MBPP+)",
      "url": "https://arxiv.org/abs/2305.01210"
    },
    {
      "name": "APPS benchmark",
      "url": "https://arxiv.org/abs/2105.09938"
    },
    {
      "name": "BugsInPy",
      "url": "https://github.com/soarsmu/BugsInPy"
    },
    {
      "name": "Defects4J",
      "url": "https://github.com/rjust/defects4j"
    },
    {
      "name": "RepoBench",
      "url": "https://arxiv.org/abs/2306.03091"
    },
    {
      "name": "SWE-bench Multilingual",
      "url": "https://www.swebench.com/multilingual"
    },
    {
      "name": "Multi-SWE-bench",
      "url": "https://arxiv.org/abs/2504.02605"
    }
  ]
}
