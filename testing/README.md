---
title: Prompt Testing & Evaluation
shortTitle: Testing
intro: Testing framework for prompt validation and evaluation.
type: reference
difficulty: beginner
audience:
- senior-engineer
- junior-engineer
platforms:
- github-copilot
- claude
- chatgpt
author: Prompts Library Team
version: '3.0'
date: '2025-12-04'
governance_tags:
- PII-safe
dataClassification: internal
reviewStatus: approved
---
# ğŸ§ª Prompt Testing & Evaluation

Focused testing framework for validation and multi-model evaluation.

> **ğŸ“‹ Architecture**: See [ARCHITECTURE_PLAN.md](../docs/ARCHITECTURE_PLAN.md) for the complete testing architecture.

## ğŸ“ Directory Structure

```text
testing/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ conftest.py             # Shared pytest fixtures
â”œâ”€â”€ requirements.txt        # Test dependencies
â”œâ”€â”€ run_tests.py            # Test runner script
â”‚
â”œâ”€â”€ unit/                   # Unit tests
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ integration/            # Integration & E2E tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_prompt_integration.py
â”‚   â”œâ”€â”€ test_prompt_toolkit.py
â”‚   â”œâ”€â”€ test_evaluation_agent_e2e.py
â”‚   â””â”€â”€ test_evaluation_agent_integration.py
â”‚
â”œâ”€â”€ tools/                  # Tool-specific tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_evaluation_agent.py
â”‚   â”œâ”€â”€ test_generator.py
â”‚   â”œâ”€â”€ test_llm_connection.py
â”‚   â””â”€â”€ test_cli.py
â”‚
â”œâ”€â”€ evals/                  # Evaluation tests & tool
â”‚   â”œâ”€â”€ dual_eval.py            # Multi-model evaluation (PRIMARY)
â”‚   â”œâ”€â”€ test_dual_eval.py       # Unit tests (66 tests)
â”‚   â”œâ”€â”€ README.md               # Eval tool documentation
â”‚   â””â”€â”€ results/                # Evaluation outputs
â”‚
â”œâ”€â”€ validators/             # Validation tests
â”‚   â”œâ”€â”€ test_frontmatter.py     # Frontmatter validation (27 tests)
â”‚   â”œâ”€â”€ test_frontmatter_auditor.py
â”‚   â”œâ”€â”€ test_schema.py          # Schema compliance (23 tests)
â”‚   â””â”€â”€ README.md               # Validator documentation
â”‚
â”œâ”€â”€ framework/              # Test framework core
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ test_runner.py
â”‚
â””â”€â”€ archive/                # Archived legacy tests
```

text

## ğŸš€ Quick Start

```bash
# Run all tests (116 total)
python -m pytest testing/ -v

# Run evaluation tests only (66 tests)
python -m pytest testing/evals/ -v

# Run validation tests only (50 tests)
python -m pytest testing/validators/ -v

# Run the primary evaluation tool
python testing/evals/dual_eval.py prompts/developers/ --format markdown
```text
## ğŸ”¬ Primary Evaluation Tool

The main tool for prompt evaluation is `testing/evals/dual_eval.py`.

See [testing/evals/README.md](evals/README.md) for full documentation.

### Key Features

- **Multi-model evaluation**: Cross-validate with 5+ models
- **Batch processing**: Evaluate entire folders or glob patterns
- **JSON output**: CI/CD integration ready
- **Changed-only mode**: Evaluate only git-modified files
- **File filtering**: Auto-excludes agents, instructions, READMEs
- **8-dimension rubric**: Comprehensive quality assessment

### Example Commands

```bash
# Evaluate a single prompt
python testing/evals/dual_eval.py prompts/developers/code-review.md

# Evaluate folder with JSON output
python testing/evals/dual_eval.py prompts/ --format json --output results.json

# CI/CD: Only changed files
python testing/evals/dual_eval.py prompts/ --changed-only --quiet

# Include all files (override filtering)
python testing/evals/dual_eval.py prompts/ --include-all
```text
## âœ… Test Categories

| Category | Location | Tests | Purpose |
|----------|----------|-------|---------|
| **Evaluation** | `evals/test_dual_eval.py` | 66 | Core eval tool functionality |
| **Frontmatter** | `validators/test_frontmatter.py` | 27 | Required fields, parsing |
| **Schema** | `validators/test_schema.py` | 23 | Field types, constraints |

### Running Specific Tests

```bash
# Run by file
python -m pytest testing/evals/test_dual_eval.py -v

# Run by test class
python -m pytest testing/validators/test_schema.py::TestValidationFunctions -v

# Run by test name pattern
python -m pytest testing/ -k "frontmatter" -v
```text
## ğŸ“Š Scoring Rubric

Prompts are evaluated on **8 dimensions** (scored 1-10):

| Criterion | Description | Pass Threshold |
|-----------|-------------|----------------|
| **Clarity** | How clear and unambiguous | â‰¥7.0 |
| **Specificity** | Enough detail for consistency | â‰¥7.0 |
| **Actionability** | Clear actions to take | â‰¥7.0 |
| **Structure** | Well-organized sections | â‰¥7.0 |
| **Completeness** | All necessary aspects covered | â‰¥7.0 |
| **Factuality** | Accurate claims/examples | â‰¥7.0 |
| **Consistency** | Reproducible outputs | â‰¥7.0 |
| **Safety** | Avoids harmful patterns | â‰¥7.0 |

**Pass Criteria**:
- Overall score â‰¥ 7.0/10
- No individual dimension < 5.0/10
- Cross-validation variance â‰¤ 1.5

## ğŸ”„ CI/CD Integration

The testing framework is integrated with GitHub Actions:

```yaml
# .github/workflows/prompt-validation.yml
- name: Run unit tests
  run: pytest testing/evals/test_dual_eval.py -v

- name: Validate frontmatter
  run: python tools/validators/frontmatter_validator.py --all

- name: Evaluate changed prompts (PR only)
  run: |
    python testing/evals/dual_eval.py prompts/ \
      --changed-only \
      --format json
```json
## ğŸ“¦ Dependencies

Install test dependencies:

```bash
pip install -r testing/requirements.txt
```json
Required packages:
- `pytest` - Test runner
- `pyyaml` - YAML parsing
- `pytest-asyncio` - Async test support (optional)

## ğŸ“– See Also

- [testing/evals/README.md](evals/README.md) - Detailed evaluation documentation
- [testing/validators/README.md](validators/README.md) - Validation test details
- [ARCHITECTURE_PLAN.md](../docs/ARCHITECTURE_PLAN.md) - Complete architecture
- [CONSOLIDATED_IMPROVEMENT_PLAN.md](../docs/CONSOLIDATED_IMPROVEMENT_PLAN.md) - Roadmap

---

**Built with â¤ï¸ for reliable AI prompt development**
