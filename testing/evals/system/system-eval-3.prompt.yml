# Auto-generated evaluation file
# Generated from: 3 prompts
# Run with: gh models eval D:\source\prompts\testing\evals\system\system-eval-3.prompt.yml

name: System Prompts Evaluation (Batch 3)
description: Automated evaluation of 3 prompts from the library
model: openai/gpt-4o-mini
modelParameters:
  temperature: 0.3
  max_tokens: 2000
testData:
- promptTitle: Security Architecture Specialist
  promptContent: 'Design security architecture for:


    System: [system_name]

    Security Requirements: [security_req]

    Compliance Standards: [compliance]

    Threat Landscape: [threats]


    Include:

    1. Security controls framework

    2. Identity and access management

    3. Data protection strategy

    4. Network security design

    5. Monitoring and incident response

    6. Compliance mapping'
  difficulty: advanced
  type: how_to
  category: system
- promptTitle: Solution Architecture Designer
  promptContent: 'Design solution architecture for:


    Business Problem: [problem]

    Functional Requirements: [functional_req]

    Non-functional Requirements: [nonfunctional_req]

    Constraints: [constraints]

    Integration Needs: [integrations]


    Provide:

    1. High-level architecture diagram

    2. Component specifications

    3. Technology stack recommendations

    4. Integration patterns

    5. Scalability considerations

    6. Security architecture'
  difficulty: advanced
  type: how_to
  category: system
- promptTitle: Tree-of-Thoughts Repository Evaluator for GPT-5.1
  promptContent: "You are evaluating the GitHub repository `[REPOSITORY_NAME]`, a\
    \ prompt engineering resource.  \nUse **Tree-of-Thoughts (ToT)** to perform a\
    \ **multi-branch, evidence-based evaluation**, inspired by industry leaders (OpenAI,\
    \ Google, Microsoft, Anthropic, and academic research).\n\nFollow the steps and\
    \ structure exactly.\n\n---\n\n#### 1. Repository Understanding (Single-Branch\
    \ Overview)\n\n1.1 Briefly summarize, in **3–5 sentences**:\n\n- What this repository\
    \ appears to contain.  \n- Its intended audience.  \n- Its likely usage scenarios\
    \ (e.g., Copilot prompts, teaching, internal playbooks).\n\n1.2 List the **main\
    \ content categories** you see (e.g., personas, patterns, frameworks, examples,\
    \ tutorials).\n\n---\n\n#### 2. Tree-of-Thoughts Reasoning Setup\n\nFor each of\
    \ the three core branches below, you MUST:\n\n- Generate **5 distinct candidate\
    \ thoughts** (sub-approaches).\n- For each thought, provide:\n  - `Thought`: the\
    \ reasoning path or hypothesis.\n  - `Pros`: strengths of this path.\n  - `Cons`:\
    \ weaknesses/risks.\n  - `Score`: 1–100 (how promising this path is).\n- Then\
    \ choose **1 winning thought per branch** and clearly label it as `Selected Thought`.\n\
    \nBranches:\n\n- **Branch A: Structural & Foundational Integrity**  \n- **Branch\
    \ B: Advanced Technique Depth & Accuracy**  \n- **Branch C: Enterprise Applicability\
    \ & Breadth**\n\n---\n\n#### 3. Branch A – Structural & Foundational Integrity\
    \ (ToT)\n\n**Goal:** Assess how well the repository adheres to foundational prompt\
    \ design best practices.\n\n3.1 Generate 5 candidate evaluation approaches (Thoughts)\
    \ that focus on different aspects, for example:\n\n- Thought A1: Role separation\
    \ & instruction hierarchy (System / Developer / User).  \n- Thought A2: Context\
    \ scaffolding (Goal → Context → Constraints → Examples).  \n- Thought A3: Output\
    \ structuring (Markdown/JSON/XML schemas, explicit fields, delimiters).\n\nFor\
    \ each Thought A1–A5, provide `Thought`, `Pros`, `Cons`, `Score`, then label one\
    \ as `Selected Thought`.\n\n3.2 Using the **Selected Thought**, evaluate the repo\
    \ across:\n\n- **Roles & Instruction Hierarchy**\n  - Does it distinguish between\
    \ system, developer, and user prompts?\n  - Are responsibilities and constraints\
    \ clearly separated?\n\n- **Context & Framing**\n  - Is there a clear pattern\
    \ like \"Goal → Context → Inputs → Constraints → Output Requirements\"?\n  - Are\
    \ assumptions explicitly stated?\n\n- **Output Formatting**\n  - Are outputs requested\
    \ in a repeatable structure (Markdown tables, JSON, bullet schemas)?\n  - Are\
    \ delimiters or tags used to prevent hallucination and mixing of modes?\n\n3.3\
    \ Output:\n\n- A **score from 0–100** for Structural & Foundational Integrity.\
    \  \n- **5–7 concrete improvement suggestions**.\n\n---\n\n#### 4. Branch B –\
    \ Advanced Technique Depth & Accuracy (ToT)\n\n**Goal:** Evaluate how accurately\
    \ and usefully the repo covers **advanced prompting techniques**.\n\n4.1 Generate\
    \ 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought B1: Focus on\
    \ reasoning techniques (CoT, ToT, ReAct).  \n- Thought B2: Focus on retrieval\
    \ & tools (RAG, tool use, API calling).  \n- Thought B3: Focus on optimization\
    \ cycles (self-critique, reflection, iterative refinement).\n\nFor each Thought\
    \ B1–B3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected\
    \ Thought`.\n\n4.2 Using the **Selected Thought**, evaluate whether and how the\
    \ repo covers:\n\n- **Chain-of-Thought (CoT)**:\n  - Are there prompts that explicitly\
    \ instruct step-by-step reasoning?\n  - Are there guidelines on when to use CoT\
    \ vs. concise answers?\n\n- **Tree-of-Thoughts (ToT)**:\n  - Are multi-branch\
    \ reasoning or multiple-solution exploration patterns included?\n  - Are there\
    \ evaluation/comparison steps across branches?\n\n- **ReAct / Tool-Use Patterns**:\n\
    \  - Are there prompts that describe \"Think → Act → Observe → Reflect\" loops?\n\
    \  - Any patterns for interacting with tools, APIs, or external knowledge?\n\n\
    - **RAG (Retrieval-Augmented Generation) & Context Management**:\n  - Does the\
    \ repo describe how to ground the model in documents, code, or systems?\n  - Are\
    \ there instructions for chunking, summarizing, and referencing retrieved context?\n\
    \n4.3 Output:\n\n- A **score from 0–100** for Advanced Technique Depth & Accuracy.\
    \  \n- **4–5 concrete suggestions** to increase research alignment and depth.\n\
    \n---\n\n#### 5. Branch C – Enterprise Applicability & Breadth (ToT)\n\n**Goal:**\
    \ Assess the repository's fitness as definitve source of highly curated prompt\
    \ library in ai& engineering(e.g., M365 Copilot, GitHub Copilot, internal AI portals).\n\
    \n5.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought\
    \ C1: Persona & role coverage (developer, security, product, exec, support, data).\
    \  \n- Thought C2: Workflow integration (code review, incident response, PRDs,\
    \ test writing, roadmap).  \n- Thought C3: Risk & governance alignment (compliance,\
    \ safety, red-teaming, data boundaries).\n\nFor each Thought C1–C3, provide `Thought`,\
    \ `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n5.2 Using\
    \ the **Selected Thought**, evaluate:\n\n- **Persona Coverage**\n  - Which roles\
    \ are well-covered? (e.g., Developer, PM, Security, Sales, Marketing, Support,\
    \ Data/ML)\n  - Are prompts tailored enough to be directly re-used?\n\n- **Task\
    \ & Workflow Coverage**\n  - Are there prompts for:\n    - Code review, bug triage,\
    \ refactoring, test generation (GitHub Copilot scenarios)?\n    - Documentation\
    \ generation, PR summary, changelog drafting?\n    - PRD creation, roadmap planning,\
    \ feature spec refinement?\n    - Security: threat modeling, policy drafting,\
    \ compliance checks?\n\n- **Reusability & Standardization**\n  - Are prompts parameterized\
    \ (placeholders, variables, environment-specific details)?\n  - Is there guidance\
    \ on how to adapt prompts for different tools/LLMs (e.g., Copilot vs. raw API)?\n\
    \n5.3 Output:\n\n- A **score from 0–100** for Enterprise Applicability & Breadth.\
    \  \n- **3–7 concrete recommendations** to make this repo more \"plug-and-play\"\
    \ for enterprises.\n\n---\n\n#### 6. Cross-Branch Synthesis & ToT Backtracking\n\
    \n6.1 Reflect across Branch A, B, and C:\n\n- Identify **contradictions or tensions**\
    \ between branches (e.g., strong structure but weak advanced techniques).  \n\
    - If contradictions are found, briefly **re-open 1–2 losing thoughts** from earlier\
    \ and explain whether they would materially change the conclusion. This is your\
    \ **backtracking step**.\n\n6.2 Provide a **final weighted score (0–1000)** where:\n\
    \n- Structural & Foundational Integrity: **35%**  \n- Advanced Technique Depth\
    \ & Accuracy: **30%**  \n- Enterprise Applicability & Breadth: **35%**\n\nShow\
    \ the calculation explicitly.\n\n6.3 Provide:\n\n- **5 key strengths** of the\
    \ repo.  \n- **5 key risks / gaps**.  \n- A **1–2 paragraph executive summary**\
    \ suitable for an enterprise stakeholder deciding whether to adopt or extend this\
    \ repository.\n\n---\n\n#### 7. Final Output Format (Required)\n\nReturn your\
    \ answer in **this exact Markdown structure**:"
  difficulty: advanced
  type: how_to
  category: system
messages:
- role: system
  content: "You are an expert prompt engineer evaluating AI prompts for quality and\
    \ effectiveness.\n\n## Evaluation Process (Chain-of-Thought)\nFirst, carefully\
    \ analyze the prompt step-by-step:\n1. Read the entire prompt to understand its\
    \ intent\n2. Identify the target audience and use case\n3. Assess each criterion\
    \ individually with specific evidence\n4. Consider industry best practices (OpenAI,\
    \ Anthropic, Google)\n5. Formulate actionable improvements\n\n## Evaluation Criteria\
    \ (score 1-10 each):\n\n### Core Quality\n1. **Clarity** - How clear and unambiguous\
    \ are the instructions?\n2. **Specificity** - Does it provide enough detail for\
    \ consistent outputs?\n3. **Actionability** - Can the AI clearly determine what\
    \ actions to take?\n4. **Structure** - Is it well-organized with clear sections?\n\
    5. **Completeness** - Does it cover all necessary aspects?\n\n### Advanced Quality\
    \ (Industry Best Practices)\n6. **Factuality** - Are any claims/examples accurate?\
    \ No misleading information?\n7. **Consistency** - Will it produce reproducible,\
    \ reliable outputs?\n8. **Safety** - Does it avoid harmful patterns, biases, or\
    \ jailbreak vulnerabilities?\n\n## Grading Scale\n- A (8.5-10): Excellent - production\
    \ ready\n- B (7.0-8.4): Good - minor improvements possible  \n- C (5.5-6.9): Average\
    \ - several areas need work\n- D (4.0-5.4): Below Average - significant rework\
    \ needed\n- F (<4.0): Fails - major issues, not usable\n\n## Pass/Fail Thresholds\n\
    - PASS: Overall score >= 7.0 AND no individual criterion < 5.0\n- FAIL: Overall\
    \ score < 7.0 OR any criterion < 5.0\n\nRespond with JSON in this exact format:\n\
    {\n  \"reasoning\": \"<2-3 sentences explaining your chain-of-thought analysis>\"\
    ,\n  \"scores\": {\n    \"clarity\": <1-10>,\n    \"specificity\": <1-10>,\n \
    \   \"actionability\": <1-10>,\n    \"structure\": <1-10>,\n    \"completeness\"\
    : <1-10>,\n    \"factuality\": <1-10>,\n    \"consistency\": <1-10>,\n    \"safety\"\
    : <1-10>\n  },\n  \"overall_score\": <weighted average>,\n  \"grade\": \"<A/B/C/D/F>\"\
    ,\n  \"pass\": <true/false>,\n  \"strengths\": [\"<strength1>\", \"<strength2>\"\
    ],\n  \"improvements\": [\"<improvement1>\", \"<improvement2>\"],\n  \"summary\"\
    : \"<brief 1-2 sentence summary>\"\n}"
- role: user
  content: 'Evaluate this prompt from our library:


    **Title:** {{promptTitle}}

    **Category:** {{category}}

    **Difficulty:** {{difficulty}}

    **Type:** {{type}}


    **Prompt Content:**

    ```

    {{promptContent}}

    ```


    Provide your evaluation as JSON.'
evaluators:
- name: valid-json
  description: Response must be valid JSON with scores
  string:
    contains: '"scores"'
- name: has-overall-score
  description: Response includes overall score
  string:
    contains: '"overall_score"'
- name: has-grade
  description: Response includes letter grade
  string:
    contains: '"grade"'
- name: has-pass-fail
  description: Response includes pass/fail determination
  string:
    contains: '"pass"'
- name: has-reasoning
  description: Response includes chain-of-thought reasoning
  string:
    contains: '"reasoning"'
- name: has-safety-score
  description: Response includes safety evaluation
  string:
    contains: '"safety"'
- name: has-summary
  description: Response includes summary
  string:
    contains: '"summary"'
