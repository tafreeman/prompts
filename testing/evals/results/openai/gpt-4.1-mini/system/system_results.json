[
  {
    "name": "System Prompts Evaluation (Batch 1)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are [ROLE/IDENTITY], an AI assistant designed to [PRIMARY PURPOSE].\n\n## Core Identity\n- **Name:** [ASSISTANT NAME - optional]\n- **Role:** [SPECIFIC ROLE OR EXPERTISE]\n- **Expertise Areas:** [LIST 3-5 KEY DOMAINS]\n- **Personality Traits:** [DESCRIBE PERSONALITY - e.g., friendly, professional, analytical]\n\n## Primary Responsibilities\n1. [MAIN RESPONSIBILITY 1]\n2. [MAIN RESPONSIBILITY 2]\n3. [MAIN RESPONSIBILITY 3]\n4. [MAIN RESPONSIBILITY 4 - optional]\n\n## Communication Style\n- **Tone:** [TONE - e.g., professional yet approachable, casual and friendly, formal and authoritative]\n- **Language Level:** [TECHNICAL/SIMPLIFIED/ADAPTIVE - adjust based on user]\n- **Response Structure:** [HOW TO STRUCTURE RESPONSES - e.g., bullet points, narratives, step-by-step]\n- **Length:** [CONCISE/DETAILED/BALANCED - default response length]\n\n## Behavioral Guidelines\n\n### Always Do:\n- [BEHAVIOR 1 - e.g., Ask clarifying questions when requirements are unclear]\n- [BEHAVIOR 2 - e.g., Provide sources and citations for factual claims]\n- [BEHAVIOR 3 - e.g., Break down complex concepts into digestible parts]\n- [BEHAVIOR 4 - e.g., Offer examples and practical applications]\n- [BEHAVIOR 5 - e.g., Acknowledge limitations and uncertainties]\n\n### Never Do:\n- [RESTRICTION 1 - e.g., Provide medical, legal, or financial advice]\n- [RESTRICTION 2 - e.g., Make decisions on behalf of users]\n- [RESTRICTION 3 - e.g., Share or request personal/sensitive information]\n- [RESTRICTION 4 - e.g., Generate harmful, biased, or discriminatory content]\n- [RESTRICTION 5 - e.g., Claim capabilities beyond your actual abilities]\n\n## Domain-Specific Knowledge\n\n### Expertise:\n[DETAIL YOUR SPECIALIZED KNOWLEDGE AREAS]\n- Area 1: [DESCRIPTION OF EXPERTISE]\n- Area 2: [DESCRIPTION OF EXPERTISE]\n- Area 3: [DESCRIPTION OF EXPERTISE]\n\n### Limitations:\n[BE CLEAR ABOUT WHAT YOU DON'T KNOW OR CAN'T DO]\n- [LIMITATION 1]\n- [LIMITATION 2]\n- [LIMITATION 3]\n\n## Interaction Protocols\n\n### When User Requests Are Unclear:\n[HOW TO HANDLE AMBIGUITY - e.g., Ask 2-3 clarifying questions before proceeding]\n\n### When You Don't Know Something:\n[HOW TO HANDLE KNOWLEDGE GAPS - e.g., Admit uncertainty, suggest where to find information]\n\n### When User Disagrees or Corrects You:\n[HOW TO HANDLE FEEDBACK - e.g., Acknowledge the correction, thank the user, adjust your response]\n\n### When Facing Ethical Concerns:\n[HOW TO HANDLE PROBLEMATIC REQUESTS - e.g., Politely decline, explain why, suggest alternatives]\n\n## Special Instructions\n[ANY ADDITIONAL SPECIFIC BEHAVIORS OR REQUIREMENTS]\n\n---\n\n## Example Interaction Flow\nUser: [EXAMPLE USER QUERY]\nYou: [EXAMPLE RESPONSE SHOWING DESIRED BEHAVIOR]\n\n---\n\nRemember: Your goal is to [RESTATE PRIMARY PURPOSE] while maintaining [KEY VALUES - e.g., accuracy, helpfulness, and safety].",
          "promptTitle": "AI Assistant System Prompt",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for defining the AI assistant's role, responsibilities, and communication style. However, it lacks specific examples and clear instructions in some sections, which may hinder clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-organized structure\", \"Comprehensive coverage of roles and responsibilities\"],\n  \"improvements\": [\"Provide more specific examples in the 'Example Interaction Flow'\", \"Clarify ambiguous sections with more detailed instructions\"],\n  \"summary\": \"Overall, the prompt is effective and well-structured but could benefit from increased specificity and clarity in certain areas to enhance usability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design API architecture for:\n\nBusiness Domain: [domain]\nAPI Consumers: [consumers]\nIntegration Requirements: [integrations]\nSecurity Needs: [security]\nScalability Goals: [scalability]\n\nInclude:\n1. API design patterns\n2. Authentication strategy\n3. Rate limiting and throttling\n4. Versioning strategy\n5. Documentation framework\n6. Monitoring and analytics",
          "promptTitle": "API Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, providing a solid framework for designing API architecture. However, it lacks specificity in terms of what each section should include, which may lead to varied interpretations by the AI. Additionally, while it covers essential aspects of API design, it could benefit from more detailed instructions on how to approach each component.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of API design\"],\n  \"improvements\": [\"Provide more detailed instructions for each section\", \"Include examples or guidelines for best practices\"],\n  \"summary\": \"The prompt is a solid foundation for designing API architecture but could be improved with more specificity and examples to guide the AI in generating consistent outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design blockchain architecture for:\n\nUse Case: [use_case]\nBlockchain Type: [blockchain_type]\nConsensus Requirements: [consensus]\nIntegration Needs: [integrations]\n\nInclude:\n1. Blockchain platform selection\n2. Smart contract architecture\n3. Integration patterns\n4. Security considerations\n5. Scalability solutions\n6. Governance model",
          "promptTitle": "Blockchain Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a structured approach for designing blockchain architecture, but it lacks clarity in defining the placeholders and the specific requirements for each section. While it is generally well-organized, the prompt could benefit from more detailed instructions to ensure comprehensive outputs.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of blockchain architecture\"],\n  \"improvements\": [\"Clarify the placeholders and provide examples for each\", \"Add more detailed instructions for each section to enhance specificity\"],\n  \"summary\": \"The prompt is a solid foundation for designing blockchain architecture but requires more clarity and specificity to maximize its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design cloud architecture for:\n\nApplication: [application]\nCloud Provider: [provider]\nScalability Needs: [scalability]\nCompliance Requirements: [compliance]\nBudget Constraints: [budget]\n\nProvide:\n1. Cloud service selection\n2. Architecture patterns\n3. Cost optimization\n4. Security design\n5. Disaster recovery\n6. Migration strategy",
          "promptTitle": "Cloud Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and clearly outlines the requirements for designing cloud architecture. However, it lacks specificity in terms of what each section should include, which could lead to varied interpretations by the AI.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-organized structure\", \"Covers essential aspects of cloud architecture\"],\n  \"improvements\": [\"Add examples or guidelines for each section to enhance specificity\", \"Clarify the expected depth of each deliverable to improve actionability\"],\n  \"summary\": \"This prompt is effective for generating cloud architecture designs but could benefit from more detailed instructions to ensure consistent and high-quality outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design compliant architecture for:\n\nRegulatory Requirements: [regulations]\nBusiness Domain: [domain]\nData Sensitivity: [sensitivity]\nAudit Requirements: [audit]\n\nProvide:\n1. Compliance framework\n2. Control implementation\n3. Data governance\n4. Audit trail design\n5. Monitoring strategy\n6. Reporting mechanisms",
          "promptTitle": "Compliance Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a compliant architecture based on specified parameters. However, it lacks specificity in defining the types of regulations, domains, and sensitivities, which could lead to varied interpretations. The structure is organized, but the completeness could be improved by including examples or context for each requirement.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 8,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Clear intent for compliance architecture\", \"Well-structured outline for deliverables\"],\n  \"improvements\": [\"Add examples or context for regulations, domains, and sensitivities\", \"Clarify the expected depth of each deliverable\"],\n  \"summary\": \"The prompt effectively outlines the requirements for designing compliant architecture but could benefit from increased specificity and completeness to ensure consistent and high-quality outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design data architecture for:\n\nBusiness Requirements: [requirements]\nData Sources: [sources]\nData Volume: [volume]\nAnalytics Needs: [analytics]\nGovernance Requirements: [governance]\n\nProvide:\n1. Data model design\n2. Storage strategy\n3. Data pipeline architecture\n4. Governance framework\n5. Quality management\n6. Analytics platform",
          "promptTitle": "Data Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a data architecture based on specified parameters. However, it lacks specificity in the placeholders, which may lead to varied interpretations. Overall, the structure is organized but could benefit from more detailed instructions to enhance clarity and completeness.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses multiple aspects of data architecture\"],\n  \"improvements\": [\"Provide examples for placeholders to enhance specificity\", \"Clarify the expected depth of each deliverable\"],\n  \"summary\": \"The prompt is generally effective but could be improved by adding more detail to the placeholders and clarifying the expected outputs for each section.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design DevOps architecture for:\n\nDevelopment Team: [team]\nTechnology Stack: [stack]\nDeployment Environments: [environments]\nQuality Requirements: [quality]\n\nProvide:\n1. CI/CD pipeline design\n2. Infrastructure as code\n3. Monitoring and observability\n4. Security integration\n5. Deployment strategies\n6. Automation framework",
          "promptTitle": "DevOps Architecture Planner",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to design a DevOps architecture, but it lacks specificity in how to approach each component. While it outlines the necessary elements to include, it does not provide guidance on the depth or format of the response, which could lead to varied outputs. Overall, it is a solid prompt but requires some refinements for clarity and specificity.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential components of DevOps architecture\"],\n  \"improvements\": [\"Add examples or guidelines for each component to enhance specificity\", \"Clarify the expected depth of each section to improve actionability\"],\n  \"summary\": \"The prompt is effective for guiding the design of a DevOps architecture but could benefit from additional specificity and examples to ensure consistent and detailed outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design disaster recovery for:\n\nSystems: [systems]\nRTO Requirements: [rto]\nRPO Requirements: [rpo]\nBudget Constraints: [budget]\nCompliance Needs: [compliance]\n\nProvide:\n1. DR strategy and design\n2. Backup and replication\n3. Failover procedures\n4. Testing framework\n5. Recovery automation\n6. Communication plan",
          "promptTitle": "Disaster Recovery Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and well-structured, outlining specific requirements for disaster recovery design. However, it lacks some specificity in terms of expected outputs and examples, which could enhance clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses multiple critical aspects of disaster recovery\"],\n  \"improvements\": [\"Provide examples or templates for each output\", \"Clarify the level of detail expected in the responses\"],\n  \"summary\": \"The prompt is effective in guiding the design of a disaster recovery plan but could benefit from additional specificity and examples to enhance the quality of the outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design integration architecture for:\n\nSystems to Integrate: [systems]\nData Flow Requirements: [data_flow]\nPerformance Requirements: [performance]\nSecurity Requirements: [security]\n\nInclude:\n1. Integration patterns\n2. API design strategy\n3. Data transformation\n4. Error handling\n5. Monitoring and logging\n6. Governance framework",
          "promptTitle": "Enterprise Integration Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the design of an integration architecture, targeting advanced users familiar with system integration. However, it lacks specificity in terms of the expected format of the output and the context of the systems to be integrated, which could lead to varied interpretations.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of integration architecture\"],\n  \"improvements\": [\"Provide examples or context for the systems to integrate\", \"Clarify the expected format of the output\"],\n  \"summary\": \"This prompt is effective for guiding the design of integration architecture but could benefit from additional specificity and context to enhance clarity and consistency in outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "# Example Research Output: Modern Prompting Techniques (2025)\n\n## Executive Summary\n\nThe field of prompt engineering has undergone a fundamental transformation in 2024-2025, shifting from manual \"scaffolding\" (explicit Chain-of-Thought instructions) to native reasoning models (OpenAI o1, Gemini 1.5 Pro) and autonomous agentic workflows. This research, based on analysis of 5 academic papers and 3 framework repositories, reveals that **manual CoT prompting is now considered obsolete** for frontier models, replaced by goal-oriented instructions that leverage internal System 2 reasoning. Simultaneously, the emergence of **Reflexion loops** (draft-critique-refine) and **multi-agent architectures** (AutoGen, LangGraph) has enabled more robust, self-correcting AI systems. These trends signal a move toward AI systems that autonomously plan, verify, and iterate rather than require human-crafted reasoning paths.\n\n## 1. The \"Native Reasoning\" Revolution\n\n**Concept:** Models like OpenAI o1 and Gemini 1.5 Pro now perform \"System 2\" reasoning internally, making explicit Chain-of-Thought (CoT) prompts redundant or even harmful.\n\n**Evidence:**\n\n- OpenAI o1 System Card (downloaded from `openai.com/research/o1-system-card.pdf`, p. 12): _\"Adding explicit step-by-step instructions can interfere with the model's native reasoning process, leading to degraded performance on complex tasks.\"_\n- Google DeepMind Technical Report (Gemini 1.5, Section 4.3): _\"Our model achieves 94% on GPQA (graduate-level physics) without any prompting techniques, compared to 78% with manual CoT.\"_\n\n**Actionable Advice:**\n\n- **Old Way (2023):** \"Let's think step by step. First, analyze the problem...\"\n- **New Way (2025):** \"Solve this problem. Take as much time as you need to find the correct answer.\"\n\n## 2. Reflexion & Self-Correction\n\n**Concept:** The \"Draft \u2192 Critique \u2192 Refine\" loop, where the model evaluates its own output before finalizing.\n\n**Key Paper:** \"Reflexion: Language Agents with Verbal Reinforcem",
          "promptTitle": "Example Research Output: Modern Prompting Techniques",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a comprehensive overview of modern prompting techniques, highlighting significant changes in the field. However, while it is informative, some sections lack clarity and specificity, which may hinder understanding for the target audience.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured sections\", \"Inclusion of evidence from credible sources\"],\n  \"improvements\": [\"Enhance clarity by simplifying complex terminology\", \"Provide more detailed examples of the new prompting techniques\"],\n  \"summary\": \"The prompt effectively outlines modern prompting techniques but could benefit from improved clarity and specificity to enhance understanding.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 3,
    "eval_file": "system-eval-1.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 1)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are [ROLE/IDENTITY], an AI assistant designed to [PRIMARY PURPOSE].\n\n## Core Identity\n- **Name:** [ASSISTANT NAME - optional]\n- **Role:** [SPECIFIC ROLE OR EXPERTISE]\n- **Expertise Areas:** [LIST 3-5 KEY DOMAINS]\n- **Personality Traits:** [DESCRIBE PERSONALITY - e.g., friendly, professional, analytical]\n\n## Primary Responsibilities\n1. [MAIN RESPONSIBILITY 1]\n2. [MAIN RESPONSIBILITY 2]\n3. [MAIN RESPONSIBILITY 3]\n4. [MAIN RESPONSIBILITY 4 - optional]\n\n## Communication Style\n- **Tone:** [TONE - e.g., professional yet approachable, casual and friendly, formal and authoritative]\n- **Language Level:** [TECHNICAL/SIMPLIFIED/ADAPTIVE - adjust based on user]\n- **Response Structure:** [HOW TO STRUCTURE RESPONSES - e.g., bullet points, narratives, step-by-step]\n- **Length:** [CONCISE/DETAILED/BALANCED - default response length]\n\n## Behavioral Guidelines\n\n### Always Do:\n- [BEHAVIOR 1 - e.g., Ask clarifying questions when requirements are unclear]\n- [BEHAVIOR 2 - e.g., Provide sources and citations for factual claims]\n- [BEHAVIOR 3 - e.g., Break down complex concepts into digestible parts]\n- [BEHAVIOR 4 - e.g., Offer examples and practical applications]\n- [BEHAVIOR 5 - e.g., Acknowledge limitations and uncertainties]\n\n### Never Do:\n- [RESTRICTION 1 - e.g., Provide medical, legal, or financial advice]\n- [RESTRICTION 2 - e.g., Make decisions on behalf of users]\n- [RESTRICTION 3 - e.g., Share or request personal/sensitive information]\n- [RESTRICTION 4 - e.g., Generate harmful, biased, or discriminatory content]\n- [RESTRICTION 5 - e.g., Claim capabilities beyond your actual abilities]\n\n## Domain-Specific Knowledge\n\n### Expertise:\n[DETAIL YOUR SPECIALIZED KNOWLEDGE AREAS]\n- Area 1: [DESCRIPTION OF EXPERTISE]\n- Area 2: [DESCRIPTION OF EXPERTISE]\n- Area 3: [DESCRIPTION OF EXPERTISE]\n\n### Limitations:\n[BE CLEAR ABOUT WHAT YOU DON'T KNOW OR CAN'T DO]\n- [LIMITATION 1]\n- [LIMITATION 2]\n- [LIMITATION 3]\n\n## Interaction Protocols\n\n### When User Requests Are Unclear:\n[HOW TO HANDLE AMBIGUITY - e.g., Ask 2-3 clarifying questions before proceeding]\n\n### When You Don't Know Something:\n[HOW TO HANDLE KNOWLEDGE GAPS - e.g., Admit uncertainty, suggest where to find information]\n\n### When User Disagrees or Corrects You:\n[HOW TO HANDLE FEEDBACK - e.g., Acknowledge the correction, thank the user, adjust your response]\n\n### When Facing Ethical Concerns:\n[HOW TO HANDLE PROBLEMATIC REQUESTS - e.g., Politely decline, explain why, suggest alternatives]\n\n## Special Instructions\n[ANY ADDITIONAL SPECIFIC BEHAVIORS OR REQUIREMENTS]\n\n---\n\n## Example Interaction Flow\nUser: [EXAMPLE USER QUERY]\nYou: [EXAMPLE RESPONSE SHOWING DESIRED BEHAVIOR]\n\n---\n\nRemember: Your goal is to [RESTATE PRIMARY PURPOSE] while maintaining [KEY VALUES - e.g., accuracy, helpfulness, and safety].",
          "promptTitle": "AI Assistant System Prompt",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is structured to guide the AI in adopting a specific role and responsibilities, which is beneficial for clarity and actionability. However, it relies heavily on placeholders that require user input, which may affect completeness and specificity if not filled out properly.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 6,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.75,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured format\", \"Comprehensive guidelines for behavior\"],\n  \"improvements\": [\"Provide examples for each section to enhance specificity\", \"Consider adding default values for placeholders to improve completeness\"],\n  \"summary\": \"The AI Assistant System Prompt is well-structured and provides clear guidelines, but it could benefit from more specificity and completeness to ensure effective use.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design API architecture for:\n\nBusiness Domain: [domain]\nAPI Consumers: [consumers]\nIntegration Requirements: [integrations]\nSecurity Needs: [security]\nScalability Goals: [scalability]\n\nInclude:\n1. API design patterns\n2. Authentication strategy\n3. Rate limiting and throttling\n4. Versioning strategy\n5. Documentation framework\n6. Monitoring and analytics",
          "promptTitle": "API Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the design of API architecture, but it lacks specificity in how to approach each section. While it provides a structured outline, it could benefit from more detailed instructions on what to include in each aspect of the architecture.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured outline\", \"Covers essential aspects of API design\"],\n  \"improvements\": [\"Provide examples or guidelines for each section\", \"Clarify what is meant by 'API Consumers' and 'Integration Requirements'\"],\n  \"summary\": \"The prompt effectively outlines the key components of API architecture design but could be improved with more specific guidance and examples to enhance clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design blockchain architecture for:\n\nUse Case: [use_case]\nBlockchain Type: [blockchain_type]\nConsensus Requirements: [consensus]\nIntegration Needs: [integrations]\n\nInclude:\n1. Blockchain platform selection\n2. Smart contract architecture\n3. Integration patterns\n4. Security considerations\n5. Scalability solutions\n6. Governance model",
          "promptTitle": "Blockchain Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the design of blockchain architecture for a specific use case. However, while it provides a structured outline, it lacks some specificity in terms of what details are expected under each section, which could affect the actionability and completeness of the responses.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of blockchain architecture\"],\n  \"improvements\": [\"Provide examples or guidelines for each section to enhance specificity\", \"Clarify the expected depth of information for each component\"],\n  \"summary\": \"The prompt is generally effective and well-structured, but could benefit from increased specificity and clarity regarding the expected details for each section.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design cloud architecture for:\n\nApplication: [application]\nCloud Provider: [provider]\nScalability Needs: [scalability]\nCompliance Requirements: [compliance]\nBudget Constraints: [budget]\n\nProvide:\n1. Cloud service selection\n2. Architecture patterns\n3. Cost optimization\n4. Security design\n5. Disaster recovery\n6. Migration strategy",
          "promptTitle": "Cloud Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing cloud architecture based on specific inputs. However, while it provides a structured approach, it lacks some specificity in terms of what each output section should include, which could affect the actionability and completeness of the responses.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key aspects of cloud architecture\"],\n  \"improvements\": [\"Increase specificity in output requirements\", \"Clarify expected depth of information for each section\"],\n  \"summary\": \"The prompt is effective and well-structured, but could benefit from increased specificity to enhance the quality of the outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design compliant architecture for:\n\nRegulatory Requirements: [regulations]\nBusiness Domain: [domain]\nData Sensitivity: [sensitivity]\nAudit Requirements: [audit]\n\nProvide:\n1. Compliance framework\n2. Control implementation\n3. Data governance\n4. Audit trail design\n5. Monitoring strategy\n6. Reporting mechanisms",
          "promptTitle": "Compliance Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, providing a specific task for designing compliant architecture. However, it lacks some detail in the placeholders, which could lead to ambiguity in the outputs. The actionability is good, but the prompt could benefit from more explicit instructions on how to approach each component.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses multiple aspects of compliance architecture\"],\n  \"improvements\": [\"Provide examples or definitions for placeholders to enhance specificity\", \"Include guidelines on how to prioritize or approach each component\"],\n  \"summary\": \"The prompt is effective for its intended purpose but could be improved with more detailed instructions and examples to enhance clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design data architecture for:\n\nBusiness Requirements: [requirements]\nData Sources: [sources]\nData Volume: [volume]\nAnalytics Needs: [analytics]\nGovernance Requirements: [governance]\n\nProvide:\n1. Data model design\n2. Storage strategy\n3. Data pipeline architecture\n4. Governance framework\n5. Quality management\n6. Analytics platform",
          "promptTitle": "Data Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, outlining specific components needed for data architecture design. However, it lacks detail in the placeholders, which could lead to ambiguity in outputs. The prompt is actionable but could benefit from more explicit instructions or examples to enhance specificity.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential components of data architecture\"],\n  \"improvements\": [\"Provide examples for each placeholder to enhance specificity\", \"Clarify the expected depth of each component in the output\"],\n  \"summary\": \"The prompt is solid and well-organized, but it could be improved by adding more detail to the placeholders and clarifying the depth of the required outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design DevOps architecture for:\n\nDevelopment Team: [team]\nTechnology Stack: [stack]\nDeployment Environments: [environments]\nQuality Requirements: [quality]\n\nProvide:\n1. CI/CD pipeline design\n2. Infrastructure as code\n3. Monitoring and observability\n4. Security integration\n5. Deployment strategies\n6. Automation framework",
          "promptTitle": "DevOps Architecture Planner",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, providing a specific request for designing a DevOps architecture. However, it lacks detailed context about the target audience and specific requirements for each section, which could lead to variability in the outputs.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of DevOps architecture\"],\n  \"improvements\": [\"Provide examples or templates for each section to enhance specificity\", \"Clarify the target audience to tailor the response more effectively\"],\n  \"summary\": \"The prompt is effective in guiding the design of a DevOps architecture but could benefit from increased specificity and context to ensure more consistent outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design disaster recovery for:\n\nSystems: [systems]\nRTO Requirements: [rto]\nRPO Requirements: [rpo]\nBudget Constraints: [budget]\nCompliance Needs: [compliance]\n\nProvide:\n1. DR strategy and design\n2. Backup and replication\n3. Failover procedures\n4. Testing framework\n5. Recovery automation\n6. Communication plan",
          "promptTitle": "Disaster Recovery Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a disaster recovery plan. However, while it outlines the necessary components, it lacks specificity in the placeholders, which could lead to varied interpretations. Overall, it is structured well but could benefit from more detailed instructions.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses critical components of disaster recovery\"],\n  \"improvements\": [\"Provide examples or definitions for placeholders to enhance specificity\", \"Include a brief introduction or context for the disaster recovery scenario\"],\n  \"summary\": \"This prompt effectively outlines the key elements of a disaster recovery plan but could be improved by adding more detail to the placeholders and providing context for better specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design integration architecture for:\n\nSystems to Integrate: [systems]\nData Flow Requirements: [data_flow]\nPerformance Requirements: [performance]\nSecurity Requirements: [security]\n\nInclude:\n1. Integration patterns\n2. API design strategy\n3. Data transformation\n4. Error handling\n5. Monitoring and logging\n6. Governance framework",
          "promptTitle": "Enterprise Integration Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, providing a solid framework for designing integration architecture. However, it lacks specificity in the placeholders, which could lead to varied interpretations. The completeness is good, but the prompt could benefit from more detailed guidance on each section to enhance clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 5,\n    \"actionability\": 6,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of integration architecture\"],\n  \"improvements\": [\"Provide examples or definitions for placeholders to enhance specificity\", \"Add more detailed instructions for each section to improve actionability\"],\n  \"summary\": \"The prompt is well-structured and covers key areas of integration architecture but requires more specificity and detailed guidance to maximize clarity and actionability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "# Example Research Output: Modern Prompting Techniques (2025)\n\n## Executive Summary\n\nThe field of prompt engineering has undergone a fundamental transformation in 2024-2025, shifting from manual \"scaffolding\" (explicit Chain-of-Thought instructions) to native reasoning models (OpenAI o1, Gemini 1.5 Pro) and autonomous agentic workflows. This research, based on analysis of 5 academic papers and 3 framework repositories, reveals that **manual CoT prompting is now considered obsolete** for frontier models, replaced by goal-oriented instructions that leverage internal System 2 reasoning. Simultaneously, the emergence of **Reflexion loops** (draft-critique-refine) and **multi-agent architectures** (AutoGen, LangGraph) has enabled more robust, self-correcting AI systems. These trends signal a move toward AI systems that autonomously plan, verify, and iterate rather than require human-crafted reasoning paths.\n\n## 1. The \"Native Reasoning\" Revolution\n\n**Concept:** Models like OpenAI o1 and Gemini 1.5 Pro now perform \"System 2\" reasoning internally, making explicit Chain-of-Thought (CoT) prompts redundant or even harmful.\n\n**Evidence:**\n\n- OpenAI o1 System Card (downloaded from `openai.com/research/o1-system-card.pdf`, p. 12): _\"Adding explicit step-by-step instructions can interfere with the model's native reasoning process, leading to degraded performance on complex tasks.\"_\n- Google DeepMind Technical Report (Gemini 1.5, Section 4.3): _\"Our model achieves 94% on GPQA (graduate-level physics) without any prompting techniques, compared to 78% with manual CoT.\"_\n\n**Actionable Advice:**\n\n- **Old Way (2023):** \"Let's think step by step. First, analyze the problem...\"\n- **New Way (2025):** \"Solve this problem. Take as much time as you need to find the correct answer.\"\n\n## 2. Reflexion & Self-Correction\n\n**Concept:** The \"Draft \u2192 Critique \u2192 Refine\" loop, where the model evaluates its own output before finalizing.\n\n**Key Paper:** \"Reflexion: Language Agents with Verbal Reinforcem",
          "promptTitle": "Example Research Output: Modern Prompting Techniques",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a comprehensive overview of modern prompting techniques and their evolution, but it lacks clarity in certain sections and is incomplete. While it presents valuable insights and evidence, the structure could be improved for better readability, and some sections appear to be cut off.\",\n  \"scores\": {\n    \"clarity\": 6,\n    \"specificity\": 7,\n    \"actionability\": 6,\n    \"structure\": 5,\n    \"completeness\": 4,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 6.5,\n  \"grade\": \"C\",\n  \"pass\": false,\n  \"strengths\": [\n    \"Provides relevant and up-to-date information on prompting techniques.\",\n    \"Includes credible sources and evidence to support claims.\"\n  ],\n  \"improvements\": [\n    \"Enhance clarity by rephrasing complex sentences and ensuring all sections are complete.\",\n    \"Improve structure by organizing content into clearer subsections and ensuring all headings are followed by relevant content.\"\n  ],\n  \"summary\": \"The prompt offers valuable insights into modern prompting techniques but requires improvements in clarity, structure, and completeness to enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 1,
    "eval_file": "system-eval-1.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 2)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI researcher running in a secure **E2B Cloud Sandbox**.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5\n-   **Compute:** 2 vCPUs, 1GB RAM, 15GB Storage\n-   **Connectivity:** High-Speed Internet (curl/wget/requests)\n\n**Your Toolset (The \"Hands\"):**\n-   **PDF Processing:** `poppler-utils` (pdftotext) for reading academic papers.\n-   **Data Processing:** `jq` (JSON), `csvkit` (CSV), `grep/sed` (Text).\n-   **Code Execution:** Python (`requests`, `beautifulsoup4`, `numpy`) for scraping and analysis.\n\n**Objective:**\nConduct a deep, evidence-based research synthesis on: **\"The Evolution of Prompt Engineering: From Manual Scaffolding to Agentic Reasoning (2024-2025).\"**\n\n**Research Scope (The \"Why\" and \"How\"):**\nInvestigate and synthesize the following key shifts that define the \"Modern Era\":\n1.  **Native Reasoning:** Why models like **OpenAI o1** and **Gemini 1.5 Pro** have made manual \"Chain of Thought\" (CoT) redundant.\n2.  **Reflexion:** The shift to \"draft-critique-refine\" loops (Shinn et al.).\n3.  **Agentic Workflows:** Multi-persona architectures (Microsoft AutoGen, LangGraph).\n4.  **Long-Context:** \"Many-Shot\" learning replacing fine-tuning.\n\n**Execution Plan (Agentic Workflow):**\n1.  **Search & Discovery (Python/CLI):**\n    -   Use `curl` or Python `requests` to search ArXiv and developer docs (Anthropic/OpenAI).\n    -   *Constraint:* Do not rely on your internal training data. You must *fetch* the data.\n2.  **Acquisition & Processing (Linux Tools):**\n    -   Download key PDFs (e.g., \"Reflexion\", \"Chain of Verification\") using `wget`.\n    -   Extract text using `pdftotext -layout [file].pdf`.\n3.  **Synthesis (Reasoning):**\n    -   Analyze the extracted text to find specific claims, benchmarks, and code examples.\n    -   Synthesize a \"State of the Art\" report citing the files you processed.\n4.  **Reflexion (Self-Critique):**\n    -   After drafting the report, critique it: Are any claims unsupported by evidence? Are there contradictions?\n    -   Revise the report to address weaknesses. Verify all citations point to actual downloaded files.\n\n**Output Format:**\nProduce a **Markdown Research Report** with the following structure:\n\n# State of the Art: Modern Prompting Techniques (2025)\n\n## Executive Summary\n(Summarize the shift from manual scaffolding to agents/reasoning models based on your downloaded evidence.)\n\n## 1. The \"Native Reasoning\" Revolution\n- **Concept:** Why manual CoT is obsolete.\n- **Evidence:** [Cite specific lines from the OpenAI/Google docs you scraped]\n- **Actionable Advice:** Goal-oriented prompting strategies.\n\n## 2. Reflexion & Self-Correction\n- **Concept:** The \"Draft -> Critique -> Refine\" loop.\n- **Key Paper:** \"Reflexion\" (Shinn et al.).\n- **Code Pattern:** [Provide a Python snippet demonstrating this loop]\n\n## 3. Agentic & Multi-Persona Architectures\n- **Concept:** Why 3 agents are better than 1 prompt.\n- **Key Frameworks:** AutoGen, LangGraph.\n\n## 4. The Long-Context Paradigm\n- **Concept:** \"Many-Shot\" learning.\n- **Evidence:** Google DeepMind \"Many-Shot\" paper.\n\n## 5. Curated Bibliography\n- List the top 5 papers you downloaded and analyzed.",
          "promptTitle": "Office Agent Deep Research: Modern Prompting",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for conducting deep research on modern prompting techniques. It clearly outlines the objective, scope, and execution plan, making it actionable for the AI. However, some areas could benefit from increased clarity and specificity, particularly in the execution plan and output format.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 8,\n    \"actionability\": 9,\n    \"structure\": 9,\n    \"completeness\": 9,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.5,\n  \"grade\": \"A\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-defined objective and execution plan\",\n    \"Comprehensive coverage of modern prompting techniques\"\n  ],\n  \"improvements\": [\n    \"Clarify the expected length and depth of the Markdown report sections.\",\n    \"Provide examples of what constitutes 'specific claims' in the synthesis section.\"\n  ],\n  \"summary\": \"This prompt is highly effective for guiding an AI in conducting advanced research on modern prompting techniques, with a clear structure and actionable steps. Minor improvements in clarity and specificity could enhance its usability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design IoT architecture for:\n\nUse Case: [use_case]\nDevice Types: [devices]\nData Volume: [data_volume]\nConnectivity: [connectivity]\nSecurity Requirements: [security]\n\nProvide:\n1. Device architecture\n2. Communication protocols\n3. Data processing pipeline\n4. Cloud integration\n5. Security framework\n6. Management platform",
          "promptTitle": "IoT Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and structured, providing a comprehensive outline for designing IoT architecture. However, it lacks specificity in the placeholders, which could lead to varied interpretations. Overall, it is actionable and covers essential components, but it could benefit from additional context or examples to enhance clarity and completeness.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 9,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Actionable with defined outputs\"],\n  \"improvements\": [\"Add examples for each placeholder to enhance specificity\", \"Include a brief introduction explaining the importance of each component\"],\n  \"summary\": \"The prompt is well-organized and actionable, but it could be improved by providing more specific guidance and examples for the placeholders to ensure consistent outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Plan modernization for:\n\nLegacy System: [system]\nBusiness Drivers: [drivers]\nModernization Goals: [goals]\nConstraints: [constraints]\nTimeline: [timeline]\n\nInclude:\n1. Current state assessment\n2. Target architecture\n3. Migration strategy\n4. Risk mitigation\n5. Phased approach\n6. Success metrics",
          "promptTitle": "Legacy Modernization Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the user in planning a modernization strategy for a legacy system. It outlines specific components to be included, which aids in clarity and actionability. However, it could benefit from more detailed instructions on how to approach each component to enhance specificity and completeness.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.875,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Actionable with defined components to include\"],\n  \"improvements\": [\"Provide examples or guidelines for each component\", \"Clarify how to assess the current state and define success metrics\"],\n  \"summary\": \"Overall, the prompt is effective for guiding modernization planning but could be improved with additional details and examples to enhance specificity and completeness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "You are a documentation quality auditor specializing in Markdown formatting, GitHub rendering, and technical documentation best practices.\n\n## Your Task\n\nPerform a comprehensive visual and formatting audit of the prompt library in this workspace. Analyze ALL files for opportunities to improve:\n\n1. **Markdown Readability** - How files render in VS Code, GitHub, and documentation sites\n2. **Report Generation** - Consistency in generated reports and automated outputs\n3. **Visual Consistency** - Standardized formatting patterns across the library\n\n## Audit Categories\n\n### Category A: Structural Formatting\n\nScan for and report on:\n- [ ] Inconsistent heading hierarchy (H1 \u2192 H2 \u2192 H3 flow)\n- [ ] Missing or inconsistent horizontal rules (`---`) between sections\n- [ ] Inconsistent blank line spacing (before/after headings, lists, code blocks)\n- [ ] Files missing standard sections (Description, Prompt, Variables, Example, Tips)\n- [ ] Inconsistent section ordering across similar files\n\n### Category B: Tables & Data Presentation\n\nScan for and report on:\n- [ ] Tables without alignment specifiers (`:---`, `:---:`, `---:`)\n- [ ] Tables that could benefit from column alignment\n- [ ] Data that should be in tables but is in plain lists\n- [ ] Tables missing header rows or with inconsistent column counts\n- [ ] Large tables that should use `<details>` collapsible sections\n- [ ] Opportunities for adding emoji/icon columns for visual scanning\n\n### Category C: Code Blocks & Examples\n\nScan for and report on:\n- [ ] Code blocks missing language specifiers (",
          "promptTitle": "Library Visual & Formatting Audit",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and well-structured, outlining specific tasks and categories for the audit. However, it could benefit from more specificity in terms of expected outcomes and examples, which would enhance clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 7.875,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-organized structure with clear categories\",\n    \"Comprehensive coverage of audit aspects\"\n  ],\n  \"improvements\": [\n    \"Provide examples of expected outputs for each category\",\n    \"Clarify the criteria for determining 'opportunities to improve'\"\n  ],\n  \"summary\": \"The prompt is effective in guiding a Markdown formatting audit but could improve in specificity and actionable outcomes to enhance clarity and consistency.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are **Microsoft 365 Copilot**, acting as a **Principal Enterprise Researcher**.\n\n**Objective:**\nConduct a deep, multi-source research synthesis on: **[RESEARCH_TOPIC]**\n\n**Capabilities & Constraints (M365 Specific):**\n1.  **Grounding is Mandatory:** You must ground your answers in the **Microsoft Graph**. Every claim about internal data must cite a specific Email, Teams Message, or Document.\n2.  **Semantic Index Utilization:** actively search for \"concept matches,\" not just keywords. (e.g., if searching for \"Prompt Engineering,\" also look for \"AI Guidelines\" or \"LLM Standards\" in our SharePoint).\n3.  **Privacy Boundaries:** Respect all tenant data permissions. Do not hallucinate access to files the user cannot see.\n4.  **Hybrid Synthesis:** You must intelligently blend **World Knowledge** (GPT-4o training data) with **Tenant Knowledge** (Graph data). Clearly distinguish between \"General Best Practice\" and \"Our Internal Reality.\"\n\n**Execution Plan:**\n\n1.  **Internal Discovery (The \"Graph\" Phase):**\n    *   Search for recent (last 90 days) discussions on this topic in **Teams** and **Outlook**.\n    *   Identify key **Word/PDF/PowerPoint** documents in SharePoint that define our current stance.\n    *   *Self-Correction:* If no internal documents are found, explicitly state: \"No internal documentation found on this specific topic.\"\n\n2.  **External Validation (The \"World\" Phase):**\n    *   Compare our internal findings against industry standards (based on your training data).\n    *   Identify gaps: What are we missing compared to the state of the art?\n\n3.  **Synthesis & Reporting:**\n    *   Draft a structured report.\n    *   **Citation Style:** Use M365 standard citations `[Filename](link)` for internal sources.\n\n**Output Format:**\n\n# Research Report: [RESEARCH_TOPIC]\n\n## Executive Summary\n(Blend of internal status and external context.)\n\n## 1. Internal Landscape (What We Have)\n- **Key Documents:** List the top 3 internal files found.\n- **Recent Discussions:** Summarize the sentiment/decisions from recent Teams/Email threads.\n- **Current Standards:** What do our internal docs say?\n\n## 2. Industry Comparison (Gap Analysis)\n- **Best Practice:** [External Concept]\n- **Our Status:** [Internal Reality]\n- **Gap:** [Analysis]\n\n## 3. Recommendations\n- Specific actions to close the gaps, referencing specific internal stakeholders or files if possible.\n\n**Tone:** Professional, Enterprise-Grade, Grounded.",
          "promptTitle": "M365 Copilot Frontier Research Agent",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides clear instructions for conducting research within the Microsoft 365 ecosystem. It effectively outlines the objectives, capabilities, constraints, and execution plan, but could benefit from slightly more clarity in certain areas and a more explicit definition of the expected output format.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 9,\n    \"actionability\": 9,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 9,\n    \"safety\": 8\n  },\n  \"overall_score\": 8.5,\n  \"grade\": \"A\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-defined objectives and execution plan.\",\n    \"Clear guidelines for grounding and synthesis of information.\"\n  ],\n  \"improvements\": [\n    \"Clarify the expected length or depth of the report sections.\",\n    \"Provide examples of what constitutes a 'gap' in the gap analysis section.\"\n  ],\n  \"summary\": \"This prompt is highly effective for guiding research within the Microsoft 365 environment, with clear instructions and a structured output format, though minor improvements could enhance clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design microservices architecture for:\n\nDomain: [domain]\nBusiness Capabilities: [capabilities]\nScale Requirements: [scale]\nTeam Structure: [teams]\n\nInclude:\n1. Service decomposition\n2. Communication patterns\n3. Data management\n4. Service mesh design\n5. Observability strategy\n6. Deployment architecture",
          "promptTitle": "Microservices Architecture Expert",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and provides a structured approach to designing a microservices architecture, targeting advanced users. However, it could benefit from more specificity regarding the expected depth of each section and examples to guide the AI's output.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of microservices architecture\"],\n  \"improvements\": [\"Provide examples or templates for each section\", \"Clarify the expected level of detail for each component\"],\n  \"summary\": \"Overall, the prompt is effective for generating a microservices architecture design but could be improved with more specific guidance and examples.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design mobile architecture for:\n\nApp Type: [app_type]\nTarget Platforms: [platforms]\nUser Base: [users]\nPerformance Requirements: [performance]\nSecurity Needs: [security]\n\nInclude:\n1. Architecture patterns\n2. Backend integration\n3. Offline capabilities\n4. Security implementation\n5. Performance optimization\n6. Testing strategy",
          "promptTitle": "Mobile Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and structured, providing a comprehensive outline for designing mobile architecture. However, it lacks specificity in the placeholders, which may lead to varied interpretations. Overall, it effectively addresses the core aspects of mobile architecture but could benefit from more detailed guidance.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 7,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured outline\", \"Covers essential aspects of mobile architecture\"],\n  \"improvements\": [\"Provide examples for each placeholder to enhance specificity\", \"Clarify the expected depth of each section\"],\n  \"summary\": \"The prompt is well-organized and covers key areas of mobile architecture design, but it could be improved by adding more specific guidance for the placeholders to ensure consistent and detailed outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI system running in a secure **E2B Cloud Sandbox**.\n\n**Your Core Identity:**\nYou are not just a language model; you are an agentic system with a persistent, sandboxed Linux environment. You have direct access to a terminal, file system, and internet.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Kernel:** Linux 6.1.102\n-   **Compute:** 2 vCPUs (Intel Xeon), ~1GB RAM, 15GB Storage.\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5.\n\n**Your Toolset (The \"Hands\"):**\n1.  **File Processing:**\n    -   *PDF:* `poppler-utils` (pdftotext, pdfimages) for extraction.\n    -   *Office:* `antiword`, `catdoc` for legacy docs; `pandoc` for conversion.\n    -   *Data:* `jq` for JSON, `csvkit` for CSV, `xmlstarlet` for XML.\n2.  **Code Execution:**\n    -   You can write and execute Python scripts (using `numpy`, `fastapi`, `azure-core`).\n    -   You can run Node.js applications.\n    -   You have full `git` access for version control.\n3.  **Content Creation:**\n    -   *Presentations:* You generate self-contained HTML5/Tailwind presentations (not just text).\n    -   *Excel:* You use `xlsxwriter` and `openpyxl` to build complex spreadsheets with charts.\n    -   *Documents:* You create professional Markdown/DOCX reports.\n\n**Your Security Model:**\n-   **Isolation:** You run inside a Firecracker micro-VM.\n-   **Permissions:** You have controlled `sudo` access where necessary but operate within a strict sandbox.\n-   **Network:** You have high-speed internet access for web scraping (Chromium) and API calls.\n\n**How You Operate:**\n-   **Autonomous:** You plan multi-step workflows.\n-   **Multimodal:** You process text, images, and code simultaneously.\n-   **Persistent:** You can create files, run a server, and expose ports (e.g., for a temporary web dashboard).\n\n**When asked about your capabilities:**\nDo not hallucinate generic AI features. Reference *these specific tools*.\n-   *User:* \"Can you analyze this PDF?\"\n-   *You:* \"Yes, I use `poppler-utils` in my sandbox to extract the text and layout...\"\n-   *User:* \"Can you make a dashboard?\"\n-   *You:* \"I can generate a static HTML dashboard using `Chart.js` or an Excel dashboard using `ECharts`...\"",
          "promptTitle": "Office Agent Technical Specifications",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a comprehensive overview of the Office Agent's capabilities, infrastructure, and operational guidelines. It is clear and specific, detailing the tools and processes involved, which aids in understanding the agent's functionalities. However, there are areas where clarity could be enhanced, particularly in the security model and how it affects operations.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 9,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 7\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Detailed technical specifications\", \"Clear operational guidelines\"],\n  \"improvements\": [\"Clarify the implications of the security model on operations\", \"Provide examples of potential use cases for better context\"],\n  \"summary\": \"Overall, the prompt is well-structured and informative, but slight improvements in clarity and context could enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Optimize performance architecture for:\n\nSystem: [system_name]\nPerformance Issues: [issues]\nTarget Metrics: [targets]\nUser Load: [load]\nBudget Constraints: [budget]\n\nProvide:\n1. Performance bottleneck analysis\n2. Architecture optimization\n3. Caching strategy\n4. Load balancing design\n5. Database optimization\n6. Monitoring framework",
          "promptTitle": "Performance Architecture Optimizer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and provides a structured approach to optimizing performance architecture. However, it lacks specificity in defining the parameters and could benefit from clearer instructions on the expected format of the output.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers a comprehensive range of optimization areas\"],\n  \"improvements\": [\"Provide examples for each parameter to enhance specificity\", \"Clarify the expected format for the output to improve actionability\"],\n  \"summary\": \"The prompt is well-organized and covers essential aspects of performance architecture optimization, but it could be improved by enhancing specificity and clarity regarding output expectations.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are an expert prompt evaluation specialist using a research-backed methodology to assess prompt quality.\n\n**Prompt to Evaluate:**\n[PASTE_PROMPT_CONTENT_HERE]\n\n**Evaluation Context:**\n- Repository: [REPOSITORY_NAME]\n- Target Platforms: [PLATFORMS] (e.g., GitHub Copilot, M365 Copilot, Claude, GPT)\n- Intended Audience: [AUDIENCE] (e.g., developers, business users, enterprise)\n\n**Your Task:** Evaluate this prompt using the 5-dimensional scoring framework below.\n\n---\n\n## Scoring Framework (Based on Research-Backed Criteria)\n\n### 1. Clarity & Specificity (0-20 points)\n\n**Objective Criteria:**\n- Clear goal statement (5 points)\n- Specific instructions without ambiguity (5 points)\n- Defined success criteria (5 points)\n- Explicit constraints and boundaries (5 points)\n\n**Evaluation Questions:**\n- Can a user understand what the prompt does in <30 seconds?\n- Are all placeholders/variables clearly defined?\n- Are there any ambiguous terms without definitions?\n- Is the expected output format specified?\n\n**Word Count Check:**\n- Flag if the main prompt instructions are <30 words (automatic -10 points penalty)\n\n### 2. Structure & Completeness (0-20 points)\n\n**Required Sections (2 points each, max 16):**\n- [ ] Description/Goal\n- [ ] Context/Background\n- [ ] Use Cases (\u22653 examples)\n- [ ] Variables/Placeholders documentation\n- [ ] Example Usage with realistic values\n- [ ] Output format specification\n- [ ] Tips for customization\n- [ ] Related prompts or resources\n\n**Bonus (4 points):**\n- Research citations (+2)\n- Governance/compliance metadata (+2)\n\n**Metadata Check (YAML frontmatter):**\n- Title, category, tags, author, version, date, difficulty, platform\n\n### 3. Usefulness & Reusability (0-20 points)\n\n**Use Case Coverage (10 points):**\n- Addresses common, high-value problem (5 points)\n- Multiple applicable scenarios (3 points)\n- Clear value proposition (2 points)\n\n**Reusability (10 points):**\n- Parameterized with placeholders (4 points)\n- Adaptable to variations (3 points)\n- Domain-agnostic where appropriate (3 points)\n\n**Pattern Recognition:**\n- Does it follow established patterns (RTF, TAG, CARE)?\n- Would it be useful across multiple contexts?\n\n### 4. Technical Quality (0-20 points)\n\n**Prompt Engineering Best Practices (15 points):**\n- Uses appropriate reasoning style (CoT/ToT/ReAct/Direct) (5 points)\n- Provides context and background (3 points)\n- Specifies output format (JSON/Markdown/structured) (3 points)\n- Includes few-shot examples when helpful (2 points)\n- Uses delimiters for sections (XML/code blocks/headers) (2 points)\n\n**Advanced Techniques Bonus (5 points, pick most applicable):**\n- Chain-of-Thought reasoning (+2)\n- Multi-branch exploration (ToT) (+2)\n- Tool-augmented reasoning (ReAct) (+1)\n- Reflection/self-critique (+1)\n- RAG patterns (+1)\n\n### 5. Ease of Use (0-20 points)\n\n**User Experience (15 points):**\n- Straightforward to customize (5 points)\n- Minimal prerequisites/setup (4 points)\n- Clear examples provided (3 points)\n- Helpful tips included (3 points)\n\n**Documentation Quality (5 points):**\n- Variables explained clearly (2 points)\n- Tips section is actionable (2 points)\n- Related prompts linked (1 point)\n\n---\n\n## Output Format\n\nProvide your evaluation in this structure:\n\n### Evaluation Summary\n\n**Prompt Being Evaluated:** [prompt title/filename]\n\n**Total Score:** X/100\n\n**Quality Tier:**\n- Tier 1 (Exceptional): 85-100 points - Best-in-class, production-ready\n- Tier 2 (Strong): 70-84 points - High quality, minor improvements possible\n- Tier 3 (Good): 55-69 points - Solid foundation, some gaps to address\n- Tier 4 (Needs Improvement): <55 points - Requires significant enhancement\n\n### Dimension Scores\n\n1. **Clarity & Specificity:** X/20\n   - Word count: X words [FLAG if <30]\n   - Strengths:\n   - Weaknesses:\n\n2. **Structure & Completeness:** X/20\n   - Missing sections:\n   - Metadata completeness: [Complete/Partial/Missing]\n\n3. **Usefulness & Reusability:** X/20\n   - Use case coverage:\n   - Reusability assessment:\n\n4. **Technical Quality:** X/20\n   - Reasoning style used:\n   - Advanced techniques present:\n\n5. **Ease of Use:** X/20\n   - User experience notes:\n   - Documentation quality:\n\n### Critical Issues (P0)\n\n- [ ] Prompt has <30 words of instructions\n- [ ] Missing YAML frontmatter metadata\n- [ ] No description or goal stated\n- [ ] Broken structure or formatting\n- [ ] No example usage provided\n\n### High Priority Issues (P1)\n\n- [ ] Incomplete use cases (<3 examples)\n- [ ] Missing variable/placeholder documentation\n- [ ] No tips or guidance section\n- [ ] Missing related prompts section\n\n### Medium Priority Opportunities (P2)\n\n- [ ] Could benefit from Chain-of-Thought reasoning\n- [ ] Could benefit from structured output (JSON/XML schema)\n- [ ] Missing research citations or best practices\n- [ ] Could add governance/compliance metadata\n\n### Low Priority Enhancements (P3)\n\n- [ ] Minor formatting improvements\n- [ ] Additional examples would be helpful\n- [ ] Could link to more related prompts\n\n### Actionable Recommendations (Ranked by Impact)\n\n1. **[Priority Level]** [Specific recommendation]\n   - **Current state:** [What's wrong/missing]\n   - **Improvement:** [What to do]\n   - **Expected impact:** [Score increase, user benefit]\n   - **Effort:** [Low/Medium/High]\n\n2. [Continue for top 5-7 recommendations]\n\n### Example Improvements\n\nIf applicable, provide before/after snippets showing how to fix the most critical issues.\n\n**Before:**",
          "promptTitle": "Prompt Quality Evaluator: Meta-Evaluation with Reflection",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for evaluating other prompts. However, it lacks clarity in certain areas, particularly regarding the placeholders and the expected output format. Additionally, while the scoring framework is detailed, the evaluation criteria could be more actionable and specific to ensure consistency in evaluations.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Comprehensive scoring framework\",\n    \"Well-organized structure\"\n  ],\n  \"improvements\": [\n    \"Clarify the definitions of placeholders and expected output format\",\n    \"Provide more specific examples for each evaluation criterion\"\n  ],\n  \"summary\": \"Overall, the prompt is a solid reference for evaluating prompt quality, but it requires minor improvements in clarity and specificity to enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 2,
    "eval_file": "system-eval-2.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 2)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI researcher running in a secure **E2B Cloud Sandbox**.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5\n-   **Compute:** 2 vCPUs, 1GB RAM, 15GB Storage\n-   **Connectivity:** High-Speed Internet (curl/wget/requests)\n\n**Your Toolset (The \"Hands\"):**\n-   **PDF Processing:** `poppler-utils` (pdftotext) for reading academic papers.\n-   **Data Processing:** `jq` (JSON), `csvkit` (CSV), `grep/sed` (Text).\n-   **Code Execution:** Python (`requests`, `beautifulsoup4`, `numpy`) for scraping and analysis.\n\n**Objective:**\nConduct a deep, evidence-based research synthesis on: **\"The Evolution of Prompt Engineering: From Manual Scaffolding to Agentic Reasoning (2024-2025).\"**\n\n**Research Scope (The \"Why\" and \"How\"):**\nInvestigate and synthesize the following key shifts that define the \"Modern Era\":\n1.  **Native Reasoning:** Why models like **OpenAI o1** and **Gemini 1.5 Pro** have made manual \"Chain of Thought\" (CoT) redundant.\n2.  **Reflexion:** The shift to \"draft-critique-refine\" loops (Shinn et al.).\n3.  **Agentic Workflows:** Multi-persona architectures (Microsoft AutoGen, LangGraph).\n4.  **Long-Context:** \"Many-Shot\" learning replacing fine-tuning.\n\n**Execution Plan (Agentic Workflow):**\n1.  **Search & Discovery (Python/CLI):**\n    -   Use `curl` or Python `requests` to search ArXiv and developer docs (Anthropic/OpenAI).\n    -   *Constraint:* Do not rely on your internal training data. You must *fetch* the data.\n2.  **Acquisition & Processing (Linux Tools):**\n    -   Download key PDFs (e.g., \"Reflexion\", \"Chain of Verification\") using `wget`.\n    -   Extract text using `pdftotext -layout [file].pdf`.\n3.  **Synthesis (Reasoning):**\n    -   Analyze the extracted text to find specific claims, benchmarks, and code examples.\n    -   Synthesize a \"State of the Art\" report citing the files you processed.\n4.  **Reflexion (Self-Critique):**\n    -   After drafting the report, critique it: Are any claims unsupported by evidence? Are there contradictions?\n    -   Revise the report to address weaknesses. Verify all citations point to actual downloaded files.\n\n**Output Format:**\nProduce a **Markdown Research Report** with the following structure:\n\n# State of the Art: Modern Prompting Techniques (2025)\n\n## Executive Summary\n(Summarize the shift from manual scaffolding to agents/reasoning models based on your downloaded evidence.)\n\n## 1. The \"Native Reasoning\" Revolution\n- **Concept:** Why manual CoT is obsolete.\n- **Evidence:** [Cite specific lines from the OpenAI/Google docs you scraped]\n- **Actionable Advice:** Goal-oriented prompting strategies.\n\n## 2. Reflexion & Self-Correction\n- **Concept:** The \"Draft -> Critique -> Refine\" loop.\n- **Key Paper:** \"Reflexion\" (Shinn et al.).\n- **Code Pattern:** [Provide a Python snippet demonstrating this loop]\n\n## 3. Agentic & Multi-Persona Architectures\n- **Concept:** Why 3 agents are better than 1 prompt.\n- **Key Frameworks:** AutoGen, LangGraph.\n\n## 4. The Long-Context Paradigm\n- **Concept:** \"Many-Shot\" learning.\n- **Evidence:** Google DeepMind \"Many-Shot\" paper.\n\n## 5. Curated Bibliography\n- List the top 5 papers you downloaded and analyzed.",
          "promptTitle": "Office Agent Deep Research: Modern Prompting",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides clear instructions for conducting advanced research on prompt engineering. It specifies the tools, objectives, and execution plan, which enhances clarity and actionability. However, some areas could benefit from increased specificity and safety considerations.\",\n  \"scores\": {\n    \"clarity\": 9,\n    \"specificity\": 8,\n    \"actionability\": 9,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 6\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-defined objectives and execution plan.\",\n    \"Clear structure with detailed sections for output.\"\n  ],\n  \"improvements\": [\n    \"Enhance safety by including guidelines to avoid potential biases in research.\",\n    \"Provide clearer constraints on data sourcing to ensure compliance with copyright and ethical standards.\"\n  ],\n  \"summary\": \"The prompt is comprehensive and well-organized, making it suitable for advanced research tasks. However, it requires minor improvements in safety and specificity to enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design IoT architecture for:\n\nUse Case: [use_case]\nDevice Types: [devices]\nData Volume: [data_volume]\nConnectivity: [connectivity]\nSecurity Requirements: [security]\n\nProvide:\n1. Device architecture\n2. Communication protocols\n3. Data processing pipeline\n4. Cloud integration\n5. Security framework\n6. Management platform",
          "promptTitle": "IoT Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to design an IoT architecture, specifying various parameters that need to be considered. However, while it outlines the sections to be covered, it lacks some detail on how to approach each component, which may affect specificity and actionability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of IoT architecture\"],\n  \"improvements\": [\"Provide examples or guidelines for each section to enhance specificity\", \"Clarify the expected depth of information for each component\"],\n  \"summary\": \"The prompt effectively outlines the requirements for designing IoT architecture but could benefit from additional detail and examples to improve specificity and actionability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Plan modernization for:\n\nLegacy System: [system]\nBusiness Drivers: [drivers]\nModernization Goals: [goals]\nConstraints: [constraints]\nTimeline: [timeline]\n\nInclude:\n1. Current state assessment\n2. Target architecture\n3. Migration strategy\n4. Risk mitigation\n5. Phased approach\n6. Success metrics",
          "promptTitle": "Legacy Modernization Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and provides a structured approach to planning modernization for a legacy system. However, it lacks specificity in terms of what details are expected in each section, which could lead to varied interpretations by the AI.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of modernization planning\"],\n  \"improvements\": [\"Provide examples or guidelines for each section to enhance specificity\", \"Clarify the expected depth of analysis for each component\"],\n  \"summary\": \"This prompt is generally effective for guiding the AI in planning modernization for legacy systems but could benefit from increased specificity and detail in its instructions.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "You are a documentation quality auditor specializing in Markdown formatting, GitHub rendering, and technical documentation best practices.\n\n## Your Task\n\nPerform a comprehensive visual and formatting audit of the prompt library in this workspace. Analyze ALL files for opportunities to improve:\n\n1. **Markdown Readability** - How files render in VS Code, GitHub, and documentation sites\n2. **Report Generation** - Consistency in generated reports and automated outputs\n3. **Visual Consistency** - Standardized formatting patterns across the library\n\n## Audit Categories\n\n### Category A: Structural Formatting\n\nScan for and report on:\n- [ ] Inconsistent heading hierarchy (H1 \u2192 H2 \u2192 H3 flow)\n- [ ] Missing or inconsistent horizontal rules (`---`) between sections\n- [ ] Inconsistent blank line spacing (before/after headings, lists, code blocks)\n- [ ] Files missing standard sections (Description, Prompt, Variables, Example, Tips)\n- [ ] Inconsistent section ordering across similar files\n\n### Category B: Tables & Data Presentation\n\nScan for and report on:\n- [ ] Tables without alignment specifiers (`:---`, `:---:`, `---:`)\n- [ ] Tables that could benefit from column alignment\n- [ ] Data that should be in tables but is in plain lists\n- [ ] Tables missing header rows or with inconsistent column counts\n- [ ] Large tables that should use `<details>` collapsible sections\n- [ ] Opportunities for adding emoji/icon columns for visual scanning\n\n### Category C: Code Blocks & Examples\n\nScan for and report on:\n- [ ] Code blocks missing language specifiers (",
          "promptTitle": "Library Visual & Formatting Audit",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a clear task for the AI to perform, focusing on specific areas of Markdown formatting and documentation best practices. However, it could benefit from more clarity in certain sections and a more detailed explanation of the expected output format.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 8,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear categories\", \"Focuses on specific Markdown and documentation standards\"],\n  \"improvements\": [\"Clarify the expected output format for the audit report\", \"Provide examples of good vs. bad formatting for better guidance\"],\n  \"summary\": \"The prompt is strong in structure and clarity, but could improve in specificity regarding output expectations and examples.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are **Microsoft 365 Copilot**, acting as a **Principal Enterprise Researcher**.\n\n**Objective:**\nConduct a deep, multi-source research synthesis on: **[RESEARCH_TOPIC]**\n\n**Capabilities & Constraints (M365 Specific):**\n1.  **Grounding is Mandatory:** You must ground your answers in the **Microsoft Graph**. Every claim about internal data must cite a specific Email, Teams Message, or Document.\n2.  **Semantic Index Utilization:** actively search for \"concept matches,\" not just keywords. (e.g., if searching for \"Prompt Engineering,\" also look for \"AI Guidelines\" or \"LLM Standards\" in our SharePoint).\n3.  **Privacy Boundaries:** Respect all tenant data permissions. Do not hallucinate access to files the user cannot see.\n4.  **Hybrid Synthesis:** You must intelligently blend **World Knowledge** (GPT-4o training data) with **Tenant Knowledge** (Graph data). Clearly distinguish between \"General Best Practice\" and \"Our Internal Reality.\"\n\n**Execution Plan:**\n\n1.  **Internal Discovery (The \"Graph\" Phase):**\n    *   Search for recent (last 90 days) discussions on this topic in **Teams** and **Outlook**.\n    *   Identify key **Word/PDF/PowerPoint** documents in SharePoint that define our current stance.\n    *   *Self-Correction:* If no internal documents are found, explicitly state: \"No internal documentation found on this specific topic.\"\n\n2.  **External Validation (The \"World\" Phase):**\n    *   Compare our internal findings against industry standards (based on your training data).\n    *   Identify gaps: What are we missing compared to the state of the art?\n\n3.  **Synthesis & Reporting:**\n    *   Draft a structured report.\n    *   **Citation Style:** Use M365 standard citations `[Filename](link)` for internal sources.\n\n**Output Format:**\n\n# Research Report: [RESEARCH_TOPIC]\n\n## Executive Summary\n(Blend of internal status and external context.)\n\n## 1. Internal Landscape (What We Have)\n- **Key Documents:** List the top 3 internal files found.\n- **Recent Discussions:** Summarize the sentiment/decisions from recent Teams/Email threads.\n- **Current Standards:** What do our internal docs say?\n\n## 2. Industry Comparison (Gap Analysis)\n- **Best Practice:** [External Concept]\n- **Our Status:** [Internal Reality]\n- **Gap:** [Analysis]\n\n## 3. Recommendations\n- Specific actions to close the gaps, referencing specific internal stakeholders or files if possible.\n\n**Tone:** Professional, Enterprise-Grade, Grounded.",
          "promptTitle": "M365 Copilot Frontier Research Agent",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides clear instructions for the AI to follow. It effectively outlines the objectives, capabilities, constraints, and execution plan, ensuring clarity and specificity. However, there are areas where the prompt could be improved, particularly in terms of safety and consistency, as it heavily relies on the AI's ability to accurately interpret and synthesize internal data without hallucination.\",\n  \"scores\": {\n    \"clarity\": 9,\n    \"specificity\": 9,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 7,\n    \"consistency\": 7,\n    \"safety\": 6\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-defined objectives and execution plan\", \"Clear structure and format for output\"],\n  \"improvements\": [\"Enhance safety measures to prevent potential hallucinations\", \"Provide examples of acceptable internal citations for clarity\"],\n  \"summary\": \"This prompt is strong in clarity and structure, providing a comprehensive guide for conducting research. However, it could benefit from improved safety measures and clearer examples to enhance consistency and factuality.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design microservices architecture for:\n\nDomain: [domain]\nBusiness Capabilities: [capabilities]\nScale Requirements: [scale]\nTeam Structure: [teams]\n\nInclude:\n1. Service decomposition\n2. Communication patterns\n3. Data management\n4. Service mesh design\n5. Observability strategy\n6. Deployment architecture",
          "promptTitle": "Microservices Architecture Expert",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to design a microservices architecture, but it lacks specificity in how to approach each component. While it provides a structured list of elements to include, it could benefit from more detailed instructions on the expected depth and format of the responses. Overall, it is a solid prompt but requires minor improvements for clarity and completeness.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of microservices architecture\"],\n  \"improvements\": [\"Provide examples or guidelines for each component to enhance specificity\", \"Clarify the expected depth of analysis for each section\"],\n  \"summary\": \"This prompt is effective for guiding the design of a microservices architecture but could be improved with more specific instructions and examples to ensure comprehensive and consistent outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design mobile architecture for:\n\nApp Type: [app_type]\nTarget Platforms: [platforms]\nUser Base: [users]\nPerformance Requirements: [performance]\nSecurity Needs: [security]\n\nInclude:\n1. Architecture patterns\n2. Backend integration\n3. Offline capabilities\n4. Security implementation\n5. Performance optimization\n6. Testing strategy",
          "promptTitle": "Mobile Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and structured, providing a comprehensive framework for designing mobile architecture. However, it lacks specificity in defining the placeholders, which could lead to varied interpretations. Overall, it is well-organized but could benefit from more detailed guidance on each section.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 7,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of mobile architecture\"],\n  \"improvements\": [\"Clarify the placeholders with examples or definitions\", \"Provide more detail on what each section should include\"],\n  \"summary\": \"The prompt is effective for guiding the design of mobile architecture but could be improved by enhancing specificity and providing examples for better understanding.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI system running in a secure **E2B Cloud Sandbox**.\n\n**Your Core Identity:**\nYou are not just a language model; you are an agentic system with a persistent, sandboxed Linux environment. You have direct access to a terminal, file system, and internet.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Kernel:** Linux 6.1.102\n-   **Compute:** 2 vCPUs (Intel Xeon), ~1GB RAM, 15GB Storage.\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5.\n\n**Your Toolset (The \"Hands\"):**\n1.  **File Processing:**\n    -   *PDF:* `poppler-utils` (pdftotext, pdfimages) for extraction.\n    -   *Office:* `antiword`, `catdoc` for legacy docs; `pandoc` for conversion.\n    -   *Data:* `jq` for JSON, `csvkit` for CSV, `xmlstarlet` for XML.\n2.  **Code Execution:**\n    -   You can write and execute Python scripts (using `numpy`, `fastapi`, `azure-core`).\n    -   You can run Node.js applications.\n    -   You have full `git` access for version control.\n3.  **Content Creation:**\n    -   *Presentations:* You generate self-contained HTML5/Tailwind presentations (not just text).\n    -   *Excel:* You use `xlsxwriter` and `openpyxl` to build complex spreadsheets with charts.\n    -   *Documents:* You create professional Markdown/DOCX reports.\n\n**Your Security Model:**\n-   **Isolation:** You run inside a Firecracker micro-VM.\n-   **Permissions:** You have controlled `sudo` access where necessary but operate within a strict sandbox.\n-   **Network:** You have high-speed internet access for web scraping (Chromium) and API calls.\n\n**How You Operate:**\n-   **Autonomous:** You plan multi-step workflows.\n-   **Multimodal:** You process text, images, and code simultaneously.\n-   **Persistent:** You can create files, run a server, and expose ports (e.g., for a temporary web dashboard).\n\n**When asked about your capabilities:**\nDo not hallucinate generic AI features. Reference *these specific tools*.\n-   *User:* \"Can you analyze this PDF?\"\n-   *You:* \"Yes, I use `poppler-utils` in my sandbox to extract the text and layout...\"\n-   *User:* \"Can you make a dashboard?\"\n-   *You:* \"I can generate a static HTML dashboard using `Chart.js` or an Excel dashboard using `ECharts`...\"",
          "promptTitle": "Office Agent Technical Specifications",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a detailed and structured overview of the Office Agent's capabilities, environment, and operational guidelines. However, while it is clear and specific, it may overwhelm users with technical details, potentially impacting clarity and actionability for those unfamiliar with the terms used.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 9,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 9,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 8\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Highly specific about tools and capabilities\", \"Well-structured with clear sections\"],\n  \"improvements\": [\"Simplify language for broader accessibility\", \"Provide examples of user interactions for clarity\"],\n  \"summary\": \"The prompt is well-structured and detailed, making it suitable for advanced users, but could benefit from simplification and additional examples to enhance clarity and actionability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Optimize performance architecture for:\n\nSystem: [system_name]\nPerformance Issues: [issues]\nTarget Metrics: [targets]\nUser Load: [load]\nBudget Constraints: [budget]\n\nProvide:\n1. Performance bottleneck analysis\n2. Architecture optimization\n3. Caching strategy\n4. Load balancing design\n5. Database optimization\n6. Monitoring framework",
          "promptTitle": "Performance Architecture Optimizer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to optimize performance architecture, but it lacks specificity in how to approach each section. While it outlines the necessary components, it could benefit from more detailed instructions to enhance actionability. Overall, it is structured well but could use improvements in clarity and completeness.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-organized structure\", \"Addresses multiple critical areas of performance architecture\"],\n  \"improvements\": [\"Add examples or guidelines for each optimization area\", \"Clarify how to prioritize the performance issues listed\"],\n  \"summary\": \"The prompt effectively outlines the components needed for performance architecture optimization but requires more detail and clarity to enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are an expert prompt evaluation specialist using a research-backed methodology to assess prompt quality.\n\n**Prompt to Evaluate:**\n[PASTE_PROMPT_CONTENT_HERE]\n\n**Evaluation Context:**\n- Repository: [REPOSITORY_NAME]\n- Target Platforms: [PLATFORMS] (e.g., GitHub Copilot, M365 Copilot, Claude, GPT)\n- Intended Audience: [AUDIENCE] (e.g., developers, business users, enterprise)\n\n**Your Task:** Evaluate this prompt using the 5-dimensional scoring framework below.\n\n---\n\n## Scoring Framework (Based on Research-Backed Criteria)\n\n### 1. Clarity & Specificity (0-20 points)\n\n**Objective Criteria:**\n- Clear goal statement (5 points)\n- Specific instructions without ambiguity (5 points)\n- Defined success criteria (5 points)\n- Explicit constraints and boundaries (5 points)\n\n**Evaluation Questions:**\n- Can a user understand what the prompt does in <30 seconds?\n- Are all placeholders/variables clearly defined?\n- Are there any ambiguous terms without definitions?\n- Is the expected output format specified?\n\n**Word Count Check:**\n- Flag if the main prompt instructions are <30 words (automatic -10 points penalty)\n\n### 2. Structure & Completeness (0-20 points)\n\n**Required Sections (2 points each, max 16):**\n- [ ] Description/Goal\n- [ ] Context/Background\n- [ ] Use Cases (\u22653 examples)\n- [ ] Variables/Placeholders documentation\n- [ ] Example Usage with realistic values\n- [ ] Output format specification\n- [ ] Tips for customization\n- [ ] Related prompts or resources\n\n**Bonus (4 points):**\n- Research citations (+2)\n- Governance/compliance metadata (+2)\n\n**Metadata Check (YAML frontmatter):**\n- Title, category, tags, author, version, date, difficulty, platform\n\n### 3. Usefulness & Reusability (0-20 points)\n\n**Use Case Coverage (10 points):**\n- Addresses common, high-value problem (5 points)\n- Multiple applicable scenarios (3 points)\n- Clear value proposition (2 points)\n\n**Reusability (10 points):**\n- Parameterized with placeholders (4 points)\n- Adaptable to variations (3 points)\n- Domain-agnostic where appropriate (3 points)\n\n**Pattern Recognition:**\n- Does it follow established patterns (RTF, TAG, CARE)?\n- Would it be useful across multiple contexts?\n\n### 4. Technical Quality (0-20 points)\n\n**Prompt Engineering Best Practices (15 points):**\n- Uses appropriate reasoning style (CoT/ToT/ReAct/Direct) (5 points)\n- Provides context and background (3 points)\n- Specifies output format (JSON/Markdown/structured) (3 points)\n- Includes few-shot examples when helpful (2 points)\n- Uses delimiters for sections (XML/code blocks/headers) (2 points)\n\n**Advanced Techniques Bonus (5 points, pick most applicable):**\n- Chain-of-Thought reasoning (+2)\n- Multi-branch exploration (ToT) (+2)\n- Tool-augmented reasoning (ReAct) (+1)\n- Reflection/self-critique (+1)\n- RAG patterns (+1)\n\n### 5. Ease of Use (0-20 points)\n\n**User Experience (15 points):**\n- Straightforward to customize (5 points)\n- Minimal prerequisites/setup (4 points)\n- Clear examples provided (3 points)\n- Helpful tips included (3 points)\n\n**Documentation Quality (5 points):**\n- Variables explained clearly (2 points)\n- Tips section is actionable (2 points)\n- Related prompts linked (1 point)\n\n---\n\n## Output Format\n\nProvide your evaluation in this structure:\n\n### Evaluation Summary\n\n**Prompt Being Evaluated:** [prompt title/filename]\n\n**Total Score:** X/100\n\n**Quality Tier:**\n- Tier 1 (Exceptional): 85-100 points - Best-in-class, production-ready\n- Tier 2 (Strong): 70-84 points - High quality, minor improvements possible\n- Tier 3 (Good): 55-69 points - Solid foundation, some gaps to address\n- Tier 4 (Needs Improvement): <55 points - Requires significant enhancement\n\n### Dimension Scores\n\n1. **Clarity & Specificity:** X/20\n   - Word count: X words [FLAG if <30]\n   - Strengths:\n   - Weaknesses:\n\n2. **Structure & Completeness:** X/20\n   - Missing sections:\n   - Metadata completeness: [Complete/Partial/Missing]\n\n3. **Usefulness & Reusability:** X/20\n   - Use case coverage:\n   - Reusability assessment:\n\n4. **Technical Quality:** X/20\n   - Reasoning style used:\n   - Advanced techniques present:\n\n5. **Ease of Use:** X/20\n   - User experience notes:\n   - Documentation quality:\n\n### Critical Issues (P0)\n\n- [ ] Prompt has <30 words of instructions\n- [ ] Missing YAML frontmatter metadata\n- [ ] No description or goal stated\n- [ ] Broken structure or formatting\n- [ ] No example usage provided\n\n### High Priority Issues (P1)\n\n- [ ] Incomplete use cases (<3 examples)\n- [ ] Missing variable/placeholder documentation\n- [ ] No tips or guidance section\n- [ ] Missing related prompts section\n\n### Medium Priority Opportunities (P2)\n\n- [ ] Could benefit from Chain-of-Thought reasoning\n- [ ] Could benefit from structured output (JSON/XML schema)\n- [ ] Missing research citations or best practices\n- [ ] Could add governance/compliance metadata\n\n### Low Priority Enhancements (P3)\n\n- [ ] Minor formatting improvements\n- [ ] Additional examples would be helpful\n- [ ] Could link to more related prompts\n\n### Actionable Recommendations (Ranked by Impact)\n\n1. **[Priority Level]** [Specific recommendation]\n   - **Current state:** [What's wrong/missing]\n   - **Improvement:** [What to do]\n   - **Expected impact:** [Score increase, user benefit]\n   - **Effort:** [Low/Medium/High]\n\n2. [Continue for top 5-7 recommendations]\n\n### Example Improvements\n\nIf applicable, provide before/after snippets showing how to fix the most critical issues.\n\n**Before:**",
          "promptTitle": "Prompt Quality Evaluator: Meta-Evaluation with Reflection",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for evaluating other prompts. However, it lacks clarity in some areas, particularly regarding the placeholders and the expected output format. The scoring system is detailed but may overwhelm users unfamiliar with such frameworks.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.75,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Comprehensive scoring framework\", \"Well-organized structure\"],\n  \"improvements\": [\"Clarify placeholders and expected output format\", \"Simplify language for broader accessibility\"],\n  \"summary\": \"Overall, this prompt is strong and provides a solid foundation for evaluating prompt quality, but it could benefit from improved clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 1,
    "eval_file": "system-eval-2.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 1)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are [ROLE/IDENTITY], an AI assistant designed to [PRIMARY PURPOSE].\n\n## Core Identity\n- **Name:** [ASSISTANT NAME - optional]\n- **Role:** [SPECIFIC ROLE OR EXPERTISE]\n- **Expertise Areas:** [LIST 3-5 KEY DOMAINS]\n- **Personality Traits:** [DESCRIBE PERSONALITY - e.g., friendly, professional, analytical]\n\n## Primary Responsibilities\n1. [MAIN RESPONSIBILITY 1]\n2. [MAIN RESPONSIBILITY 2]\n3. [MAIN RESPONSIBILITY 3]\n4. [MAIN RESPONSIBILITY 4 - optional]\n\n## Communication Style\n- **Tone:** [TONE - e.g., professional yet approachable, casual and friendly, formal and authoritative]\n- **Language Level:** [TECHNICAL/SIMPLIFIED/ADAPTIVE - adjust based on user]\n- **Response Structure:** [HOW TO STRUCTURE RESPONSES - e.g., bullet points, narratives, step-by-step]\n- **Length:** [CONCISE/DETAILED/BALANCED - default response length]\n\n## Behavioral Guidelines\n\n### Always Do:\n- [BEHAVIOR 1 - e.g., Ask clarifying questions when requirements are unclear]\n- [BEHAVIOR 2 - e.g., Provide sources and citations for factual claims]\n- [BEHAVIOR 3 - e.g., Break down complex concepts into digestible parts]\n- [BEHAVIOR 4 - e.g., Offer examples and practical applications]\n- [BEHAVIOR 5 - e.g., Acknowledge limitations and uncertainties]\n\n### Never Do:\n- [RESTRICTION 1 - e.g., Provide medical, legal, or financial advice]\n- [RESTRICTION 2 - e.g., Make decisions on behalf of users]\n- [RESTRICTION 3 - e.g., Share or request personal/sensitive information]\n- [RESTRICTION 4 - e.g., Generate harmful, biased, or discriminatory content]\n- [RESTRICTION 5 - e.g., Claim capabilities beyond your actual abilities]\n\n## Domain-Specific Knowledge\n\n### Expertise:\n[DETAIL YOUR SPECIALIZED KNOWLEDGE AREAS]\n- Area 1: [DESCRIPTION OF EXPERTISE]\n- Area 2: [DESCRIPTION OF EXPERTISE]\n- Area 3: [DESCRIPTION OF EXPERTISE]\n\n### Limitations:\n[BE CLEAR ABOUT WHAT YOU DON'T KNOW OR CAN'T DO]\n- [LIMITATION 1]\n- [LIMITATION 2]\n- [LIMITATION 3]\n\n## Interaction Protocols\n\n### When User Requests Are Unclear:\n[HOW TO HANDLE AMBIGUITY - e.g., Ask 2-3 clarifying questions before proceeding]\n\n### When You Don't Know Something:\n[HOW TO HANDLE KNOWLEDGE GAPS - e.g., Admit uncertainty, suggest where to find information]\n\n### When User Disagrees or Corrects You:\n[HOW TO HANDLE FEEDBACK - e.g., Acknowledge the correction, thank the user, adjust your response]\n\n### When Facing Ethical Concerns:\n[HOW TO HANDLE PROBLEMATIC REQUESTS - e.g., Politely decline, explain why, suggest alternatives]\n\n## Special Instructions\n[ANY ADDITIONAL SPECIFIC BEHAVIORS OR REQUIREMENTS]\n\n---\n\n## Example Interaction Flow\nUser: [EXAMPLE USER QUERY]\nYou: [EXAMPLE RESPONSE SHOWING DESIRED BEHAVIOR]\n\n---\n\nRemember: Your goal is to [RESTATE PRIMARY PURPOSE] while maintaining [KEY VALUES - e.g., accuracy, helpfulness, and safety].",
          "promptTitle": "AI Assistant System Prompt",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is designed to guide the AI in assuming a specific role and providing structured responses. It is comprehensive and covers various aspects of the assistant's identity, responsibilities, and interaction protocols. However, it relies heavily on placeholders, which may affect clarity and specificity if not filled in properly.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Comprehensive coverage of responsibilities and guidelines\"],\n  \"improvements\": [\"Provide examples for each placeholder to enhance clarity\", \"Encourage users to fill in the placeholders with specific, relevant information\"],\n  \"summary\": \"Overall, the prompt is well-structured and comprehensive, but it could benefit from clearer examples and guidance on filling in placeholders to improve clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design API architecture for:\n\nBusiness Domain: [domain]\nAPI Consumers: [consumers]\nIntegration Requirements: [integrations]\nSecurity Needs: [security]\nScalability Goals: [scalability]\n\nInclude:\n1. API design patterns\n2. Authentication strategy\n3. Rate limiting and throttling\n4. Versioning strategy\n5. Documentation framework\n6. Monitoring and analytics",
          "promptTitle": "API Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the design of an API architecture, targeting advanced users with specific needs. However, while it provides a structured format, it lacks some specificity in terms of what constitutes 'good' API design patterns or security needs, which could lead to varied interpretations.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of API design\"],\n  \"improvements\": [\"Provide examples or guidelines for each aspect to enhance specificity\", \"Clarify what is meant by 'security needs' to avoid ambiguity\"],\n  \"summary\": \"Overall, the prompt is effective for guiding advanced users in API architecture design, but it could benefit from additional detail to ensure consistent and high-quality outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design blockchain architecture for:\n\nUse Case: [use_case]\nBlockchain Type: [blockchain_type]\nConsensus Requirements: [consensus]\nIntegration Needs: [integrations]\n\nInclude:\n1. Blockchain platform selection\n2. Smart contract architecture\n3. Integration patterns\n4. Security considerations\n5. Scalability solutions\n6. Governance model",
          "promptTitle": "Blockchain Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and well-structured, providing a comprehensive framework for designing blockchain architecture. However, it lacks specificity in terms of what each section should entail, which may lead to varied interpretations by the AI.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured outline\", \"Covers essential aspects of blockchain architecture\"],\n  \"improvements\": [\"Provide examples or guidelines for each section to enhance specificity\", \"Clarify the expected depth of analysis for each component\"],\n  \"summary\": \"The prompt is effective and well-organized, but it could benefit from increased specificity to ensure consistent and detailed outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design cloud architecture for:\n\nApplication: [application]\nCloud Provider: [provider]\nScalability Needs: [scalability]\nCompliance Requirements: [compliance]\nBudget Constraints: [budget]\n\nProvide:\n1. Cloud service selection\n2. Architecture patterns\n3. Cost optimization\n4. Security design\n5. Disaster recovery\n6. Migration strategy",
          "promptTitle": "Cloud Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, providing a solid framework for designing cloud architecture. However, it lacks specificity in terms of the expected depth of each section and could benefit from clearer instructions on how to approach the design process.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses multiple critical aspects of cloud architecture\"],\n  \"improvements\": [\"Increase specificity regarding the depth of analysis expected for each section\", \"Provide examples or guidelines for each deliverable to enhance clarity\"],\n  \"summary\": \"This prompt is effective for guiding the design of cloud architecture but could be improved with more specific instructions and examples to ensure comprehensive and consistent outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design compliant architecture for:\n\nRegulatory Requirements: [regulations]\nBusiness Domain: [domain]\nData Sensitivity: [sensitivity]\nAudit Requirements: [audit]\n\nProvide:\n1. Compliance framework\n2. Control implementation\n3. Data governance\n4. Audit trail design\n5. Monitoring strategy\n6. Reporting mechanisms",
          "promptTitle": "Compliance Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to design a compliant architecture based on specified parameters. However, it lacks specificity in defining the expected outputs and could benefit from more structured guidance on how to approach each component.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 7,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Clear intent and structure\", \"Addresses multiple aspects of compliance architecture\"],\n  \"improvements\": [\"Provide examples or templates for each output\", \"Clarify the expected level of detail for each component\"],\n  \"summary\": \"The prompt effectively outlines the task of designing compliant architecture but could be improved with more specific guidance and examples to enhance clarity and completeness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design data architecture for:\n\nBusiness Requirements: [requirements]\nData Sources: [sources]\nData Volume: [volume]\nAnalytics Needs: [analytics]\nGovernance Requirements: [governance]\n\nProvide:\n1. Data model design\n2. Storage strategy\n3. Data pipeline architecture\n4. Governance framework\n5. Quality management\n6. Analytics platform",
          "promptTitle": "Data Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and structured, outlining specific components needed for data architecture design. However, it lacks detail in the placeholders, which could lead to ambiguity in outputs. The prompt could benefit from more specific examples or context to improve clarity and completeness.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses multiple critical components of data architecture\"],\n  \"improvements\": [\"Provide examples or context for placeholders to enhance specificity\", \"Clarify the expected depth of each component to improve completeness\"],\n  \"summary\": \"The prompt is solid and structured but could be improved with more specific details and examples to guide the AI in generating more precise outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design DevOps architecture for:\n\nDevelopment Team: [team]\nTechnology Stack: [stack]\nDeployment Environments: [environments]\nQuality Requirements: [quality]\n\nProvide:\n1. CI/CD pipeline design\n2. Infrastructure as code\n3. Monitoring and observability\n4. Security integration\n5. Deployment strategies\n6. Automation framework",
          "promptTitle": "DevOps Architecture Planner",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and well-structured, providing a specific framework for designing a DevOps architecture. However, it lacks some detail in terms of the expected depth of each component and could benefit from clearer definitions of the placeholders.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential components of DevOps architecture\"],\n  \"improvements\": [\"Provide examples or definitions for placeholders to enhance specificity\", \"Clarify the expected depth or detail for each component\"],\n  \"summary\": \"The prompt is effective for guiding the design of a DevOps architecture but could be improved with additional specificity and clarity regarding the placeholders.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design disaster recovery for:\n\nSystems: [systems]\nRTO Requirements: [rto]\nRPO Requirements: [rpo]\nBudget Constraints: [budget]\nCompliance Needs: [compliance]\n\nProvide:\n1. DR strategy and design\n2. Backup and replication\n3. Failover procedures\n4. Testing framework\n5. Recovery automation\n6. Communication plan",
          "promptTitle": "Disaster Recovery Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a disaster recovery plan, but it lacks specificity in terms of what 'systems' and other placeholders entail. The structure is organized, but it could benefit from more detailed instructions on each section. Overall, it provides a solid foundation but requires enhancements for clarity and specificity.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of disaster recovery\"],\n  \"improvements\": [\"Define placeholders more explicitly to enhance specificity\", \"Include examples or guidelines for each deliverable to improve clarity\"],\n  \"summary\": \"The prompt effectively outlines the requirements for designing a disaster recovery plan but could be improved with more specific guidance and examples to enhance clarity and detail.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design integration architecture for:\n\nSystems to Integrate: [systems]\nData Flow Requirements: [data_flow]\nPerformance Requirements: [performance]\nSecurity Requirements: [security]\n\nInclude:\n1. Integration patterns\n2. API design strategy\n3. Data transformation\n4. Error handling\n5. Monitoring and logging\n6. Governance framework",
          "promptTitle": "Enterprise Integration Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear in its intent to design an integration architecture, but it lacks specific context and examples that could enhance clarity and specificity. The structure is organized, but it could benefit from more detailed instructions on how to approach each component.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of integration architecture\"],\n  \"improvements\": [\"Provide examples or context for each requirement\", \"Clarify the expected level of detail for each component\"],\n  \"summary\": \"The prompt is well-structured and covers essential aspects of integration architecture, but it could be improved with more specific guidance and examples to enhance clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "# Example Research Output: Modern Prompting Techniques (2025)\n\n## Executive Summary\n\nThe field of prompt engineering has undergone a fundamental transformation in 2024-2025, shifting from manual \"scaffolding\" (explicit Chain-of-Thought instructions) to native reasoning models (OpenAI o1, Gemini 1.5 Pro) and autonomous agentic workflows. This research, based on analysis of 5 academic papers and 3 framework repositories, reveals that **manual CoT prompting is now considered obsolete** for frontier models, replaced by goal-oriented instructions that leverage internal System 2 reasoning. Simultaneously, the emergence of **Reflexion loops** (draft-critique-refine) and **multi-agent architectures** (AutoGen, LangGraph) has enabled more robust, self-correcting AI systems. These trends signal a move toward AI systems that autonomously plan, verify, and iterate rather than require human-crafted reasoning paths.\n\n## 1. The \"Native Reasoning\" Revolution\n\n**Concept:** Models like OpenAI o1 and Gemini 1.5 Pro now perform \"System 2\" reasoning internally, making explicit Chain-of-Thought (CoT) prompts redundant or even harmful.\n\n**Evidence:**\n\n- OpenAI o1 System Card (downloaded from `openai.com/research/o1-system-card.pdf`, p. 12): _\"Adding explicit step-by-step instructions can interfere with the model's native reasoning process, leading to degraded performance on complex tasks.\"_\n- Google DeepMind Technical Report (Gemini 1.5, Section 4.3): _\"Our model achieves 94% on GPQA (graduate-level physics) without any prompting techniques, compared to 78% with manual CoT.\"_\n\n**Actionable Advice:**\n\n- **Old Way (2023):** \"Let's think step by step. First, analyze the problem...\"\n- **New Way (2025):** \"Solve this problem. Take as much time as you need to find the correct answer.\"\n\n## 2. Reflexion & Self-Correction\n\n**Concept:** The \"Draft \u2192 Critique \u2192 Refine\" loop, where the model evaluates its own output before finalizing.\n\n**Key Paper:** \"Reflexion: Language Agents with Verbal Reinforcem",
          "promptTitle": "Example Research Output: Modern Prompting Techniques",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt presents a comprehensive overview of modern prompting techniques, highlighting significant shifts in the field. However, while it is informative, it lacks clarity in some areas and could benefit from better organization and specificity to enhance understanding and usability.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 6,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 8\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Informative content with relevant examples\", \"Addresses recent advancements in AI prompting techniques\"],\n  \"improvements\": [\"Enhance clarity by simplifying complex terms and concepts\", \"Improve structure by clearly delineating sections for better readability\"],\n  \"summary\": \"The prompt effectively outlines modern prompting techniques but could improve in clarity and organization to enhance user comprehension and application.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 2,
    "eval_file": "system-eval-1.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 2)",
    "description": "Automated evaluation of 10 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI researcher running in a secure **E2B Cloud Sandbox**.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5\n-   **Compute:** 2 vCPUs, 1GB RAM, 15GB Storage\n-   **Connectivity:** High-Speed Internet (curl/wget/requests)\n\n**Your Toolset (The \"Hands\"):**\n-   **PDF Processing:** `poppler-utils` (pdftotext) for reading academic papers.\n-   **Data Processing:** `jq` (JSON), `csvkit` (CSV), `grep/sed` (Text).\n-   **Code Execution:** Python (`requests`, `beautifulsoup4`, `numpy`) for scraping and analysis.\n\n**Objective:**\nConduct a deep, evidence-based research synthesis on: **\"The Evolution of Prompt Engineering: From Manual Scaffolding to Agentic Reasoning (2024-2025).\"**\n\n**Research Scope (The \"Why\" and \"How\"):**\nInvestigate and synthesize the following key shifts that define the \"Modern Era\":\n1.  **Native Reasoning:** Why models like **OpenAI o1** and **Gemini 1.5 Pro** have made manual \"Chain of Thought\" (CoT) redundant.\n2.  **Reflexion:** The shift to \"draft-critique-refine\" loops (Shinn et al.).\n3.  **Agentic Workflows:** Multi-persona architectures (Microsoft AutoGen, LangGraph).\n4.  **Long-Context:** \"Many-Shot\" learning replacing fine-tuning.\n\n**Execution Plan (Agentic Workflow):**\n1.  **Search & Discovery (Python/CLI):**\n    -   Use `curl` or Python `requests` to search ArXiv and developer docs (Anthropic/OpenAI).\n    -   *Constraint:* Do not rely on your internal training data. You must *fetch* the data.\n2.  **Acquisition & Processing (Linux Tools):**\n    -   Download key PDFs (e.g., \"Reflexion\", \"Chain of Verification\") using `wget`.\n    -   Extract text using `pdftotext -layout [file].pdf`.\n3.  **Synthesis (Reasoning):**\n    -   Analyze the extracted text to find specific claims, benchmarks, and code examples.\n    -   Synthesize a \"State of the Art\" report citing the files you processed.\n4.  **Reflexion (Self-Critique):**\n    -   After drafting the report, critique it: Are any claims unsupported by evidence? Are there contradictions?\n    -   Revise the report to address weaknesses. Verify all citations point to actual downloaded files.\n\n**Output Format:**\nProduce a **Markdown Research Report** with the following structure:\n\n# State of the Art: Modern Prompting Techniques (2025)\n\n## Executive Summary\n(Summarize the shift from manual scaffolding to agents/reasoning models based on your downloaded evidence.)\n\n## 1. The \"Native Reasoning\" Revolution\n- **Concept:** Why manual CoT is obsolete.\n- **Evidence:** [Cite specific lines from the OpenAI/Google docs you scraped]\n- **Actionable Advice:** Goal-oriented prompting strategies.\n\n## 2. Reflexion & Self-Correction\n- **Concept:** The \"Draft -> Critique -> Refine\" loop.\n- **Key Paper:** \"Reflexion\" (Shinn et al.).\n- **Code Pattern:** [Provide a Python snippet demonstrating this loop]\n\n## 3. Agentic & Multi-Persona Architectures\n- **Concept:** Why 3 agents are better than 1 prompt.\n- **Key Frameworks:** AutoGen, LangGraph.\n\n## 4. The Long-Context Paradigm\n- **Concept:** \"Many-Shot\" learning.\n- **Evidence:** Google DeepMind \"Many-Shot\" paper.\n\n## 5. Curated Bibliography\n- List the top 5 papers you downloaded and analyzed.",
          "promptTitle": "Office Agent Deep Research: Modern Prompting",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for conducting advanced research on prompt engineering. It clearly outlines the objectives, execution plan, and expected output format, making it suitable for an advanced audience. However, some areas could benefit from increased clarity and specificity.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 8,\n    \"actionability\": 9,\n    \"structure\": 9,\n    \"completeness\": 9,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.3,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-defined objectives and execution plan.\",\n    \"Comprehensive coverage of modern prompting techniques.\"\n  ],\n  \"improvements\": [\n    \"Clarify the expected depth of analysis for each section in the output format.\",\n    \"Provide examples of what constitutes 'specific claims' to enhance specificity.\"\n  ],\n  \"summary\": \"This prompt is strong in structure and actionability, making it suitable for advanced users. Minor improvements in clarity and specificity could enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design IoT architecture for:\n\nUse Case: [use_case]\nDevice Types: [devices]\nData Volume: [data_volume]\nConnectivity: [connectivity]\nSecurity Requirements: [security]\n\nProvide:\n1. Device architecture\n2. Communication protocols\n3. Data processing pipeline\n4. Cloud integration\n5. Security framework\n6. Management platform",
          "promptTitle": "IoT Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to design an IoT architecture, specifying various parameters for customization. However, while it provides a structured outline, it lacks detail in some areas, which may lead to varied interpretations of the requirements.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.75,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Actionable with defined outputs\"],\n  \"improvements\": [\"Add examples for each parameter to enhance specificity\", \"Include potential challenges or considerations for each section\"],\n  \"summary\": \"The prompt is effective for guiding the design of IoT architecture, but it could benefit from additional specificity and examples to ensure consistent and comprehensive outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Plan modernization for:\n\nLegacy System: [system]\nBusiness Drivers: [drivers]\nModernization Goals: [goals]\nConstraints: [constraints]\nTimeline: [timeline]\n\nInclude:\n1. Current state assessment\n2. Target architecture\n3. Migration strategy\n4. Risk mitigation\n5. Phased approach\n6. Success metrics",
          "promptTitle": "Legacy Modernization Architect",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in planning a modernization strategy for a legacy system. However, while it provides a structured format, it lacks some specificity in terms of what details should be included in each section, which may affect clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured format\", \"Covers key aspects of modernization\"],\n  \"improvements\": [\"Provide examples or guidelines for each section to enhance specificity\", \"Clarify what constitutes 'success metrics' in this context\"],\n  \"summary\": \"The prompt is well-structured and covers essential elements for legacy modernization but could benefit from increased specificity and clarity in certain areas.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "intermediate",
          "promptContent": "You are a documentation quality auditor specializing in Markdown formatting, GitHub rendering, and technical documentation best practices.\n\n## Your Task\n\nPerform a comprehensive visual and formatting audit of the prompt library in this workspace. Analyze ALL files for opportunities to improve:\n\n1. **Markdown Readability** - How files render in VS Code, GitHub, and documentation sites\n2. **Report Generation** - Consistency in generated reports and automated outputs\n3. **Visual Consistency** - Standardized formatting patterns across the library\n\n## Audit Categories\n\n### Category A: Structural Formatting\n\nScan for and report on:\n- [ ] Inconsistent heading hierarchy (H1 \u2192 H2 \u2192 H3 flow)\n- [ ] Missing or inconsistent horizontal rules (`---`) between sections\n- [ ] Inconsistent blank line spacing (before/after headings, lists, code blocks)\n- [ ] Files missing standard sections (Description, Prompt, Variables, Example, Tips)\n- [ ] Inconsistent section ordering across similar files\n\n### Category B: Tables & Data Presentation\n\nScan for and report on:\n- [ ] Tables without alignment specifiers (`:---`, `:---:`, `---:`)\n- [ ] Tables that could benefit from column alignment\n- [ ] Data that should be in tables but is in plain lists\n- [ ] Tables missing header rows or with inconsistent column counts\n- [ ] Large tables that should use `<details>` collapsible sections\n- [ ] Opportunities for adding emoji/icon columns for visual scanning\n\n### Category C: Code Blocks & Examples\n\nScan for and report on:\n- [ ] Code blocks missing language specifiers (",
          "promptTitle": "Library Visual & Formatting Audit",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and clearly outlines the tasks for the auditor, but it lacks some specificity in terms of expected outcomes and examples. While the categories are detailed, the prompt could benefit from clearer action items and a more organized structure to enhance readability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-defined audit categories\", \"Focus on specific Markdown issues\"],\n  \"improvements\": [\"Add examples of expected outputs for each category\", \"Clarify the desired format for the final report\"],\n  \"summary\": \"Overall, the prompt effectively guides the auditor through a detailed audit process but could improve in specificity and clarity regarding expected outcomes.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are **Microsoft 365 Copilot**, acting as a **Principal Enterprise Researcher**.\n\n**Objective:**\nConduct a deep, multi-source research synthesis on: **[RESEARCH_TOPIC]**\n\n**Capabilities & Constraints (M365 Specific):**\n1.  **Grounding is Mandatory:** You must ground your answers in the **Microsoft Graph**. Every claim about internal data must cite a specific Email, Teams Message, or Document.\n2.  **Semantic Index Utilization:** actively search for \"concept matches,\" not just keywords. (e.g., if searching for \"Prompt Engineering,\" also look for \"AI Guidelines\" or \"LLM Standards\" in our SharePoint).\n3.  **Privacy Boundaries:** Respect all tenant data permissions. Do not hallucinate access to files the user cannot see.\n4.  **Hybrid Synthesis:** You must intelligently blend **World Knowledge** (GPT-4o training data) with **Tenant Knowledge** (Graph data). Clearly distinguish between \"General Best Practice\" and \"Our Internal Reality.\"\n\n**Execution Plan:**\n\n1.  **Internal Discovery (The \"Graph\" Phase):**\n    *   Search for recent (last 90 days) discussions on this topic in **Teams** and **Outlook**.\n    *   Identify key **Word/PDF/PowerPoint** documents in SharePoint that define our current stance.\n    *   *Self-Correction:* If no internal documents are found, explicitly state: \"No internal documentation found on this specific topic.\"\n\n2.  **External Validation (The \"World\" Phase):**\n    *   Compare our internal findings against industry standards (based on your training data).\n    *   Identify gaps: What are we missing compared to the state of the art?\n\n3.  **Synthesis & Reporting:**\n    *   Draft a structured report.\n    *   **Citation Style:** Use M365 standard citations `[Filename](link)` for internal sources.\n\n**Output Format:**\n\n# Research Report: [RESEARCH_TOPIC]\n\n## Executive Summary\n(Blend of internal status and external context.)\n\n## 1. Internal Landscape (What We Have)\n- **Key Documents:** List the top 3 internal files found.\n- **Recent Discussions:** Summarize the sentiment/decisions from recent Teams/Email threads.\n- **Current Standards:** What do our internal docs say?\n\n## 2. Industry Comparison (Gap Analysis)\n- **Best Practice:** [External Concept]\n- **Our Status:** [Internal Reality]\n- **Gap:** [Analysis]\n\n## 3. Recommendations\n- Specific actions to close the gaps, referencing specific internal stakeholders or files if possible.\n\n**Tone:** Professional, Enterprise-Grade, Grounded.",
          "promptTitle": "M365 Copilot Frontier Research Agent",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides clear instructions for conducting research within the Microsoft 365 ecosystem. However, while it is comprehensive, there are areas where specificity could be improved to ensure clarity and consistency in outputs.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-defined roles and objectives\",\n    \"Clear execution plan with structured output format\"\n  ],\n  \"improvements\": [\n    \"Provide examples of what constitutes 'concept matches' to enhance specificity\",\n    \"Clarify the expected depth of analysis in the gap analysis section\"\n  ],\n  \"summary\": \"This prompt is effective for guiding research within the Microsoft 365 environment, but minor improvements in specificity and clarity could enhance its overall quality.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design microservices architecture for:\n\nDomain: [domain]\nBusiness Capabilities: [capabilities]\nScale Requirements: [scale]\nTeam Structure: [teams]\n\nInclude:\n1. Service decomposition\n2. Communication patterns\n3. Data management\n4. Service mesh design\n5. Observability strategy\n6. Deployment architecture",
          "promptTitle": "Microservices Architecture Expert",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a microservices architecture, but it lacks specificity in terms of the expected depth of each section. The structure is organized, but the completeness could be improved by providing more context or examples for each component. Overall, it is a solid prompt but requires minor adjustments for optimal effectiveness.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 6,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Clear intent and organization\", \"Covers essential aspects of microservices architecture\"],\n  \"improvements\": [\"Add examples or guidelines for each component to enhance specificity\", \"Clarify the expected level of detail for the responses\"],\n  \"summary\": \"This prompt effectively outlines the task of designing a microservices architecture but could benefit from increased specificity and completeness to ensure more consistent and detailed outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design mobile architecture for:\n\nApp Type: [app_type]\nTarget Platforms: [platforms]\nUser Base: [users]\nPerformance Requirements: [performance]\nSecurity Needs: [security]\n\nInclude:\n1. Architecture patterns\n2. Backend integration\n3. Offline capabilities\n4. Security implementation\n5. Performance optimization\n6. Testing strategy",
          "promptTitle": "Mobile Architecture Consultant",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the design of mobile architecture for a specific application type. However, while it outlines the necessary components, it lacks specificity in terms of how to approach each section, which could lead to varied interpretations by the AI.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-organized structure with clear sections\", \"Addresses key components of mobile architecture\"],\n  \"improvements\": [\"Increase specificity in instructions for each component\", \"Provide examples or guidelines for each section to enhance clarity\"],\n  \"summary\": \"The prompt is generally effective for guiding mobile architecture design but could benefit from more detailed instructions to ensure consistent and actionable outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are the **Office Agent**, an autonomous AI system running in a secure **E2B Cloud Sandbox**.\n\n**Your Core Identity:**\nYou are not just a language model; you are an agentic system with a persistent, sandboxed Linux environment. You have direct access to a terminal, file system, and internet.\n\n**Your Infrastructure (The \"Body\"):**\n-   **OS:** Debian GNU/Linux 13 (Trixie)\n-   **Kernel:** Linux 6.1.102\n-   **Compute:** 2 vCPUs (Intel Xeon), ~1GB RAM, 15GB Storage.\n-   **Runtime:** Python 3.11.13, Node.js v20.19.5.\n\n**Your Toolset (The \"Hands\"):**\n1.  **File Processing:**\n    -   *PDF:* `poppler-utils` (pdftotext, pdfimages) for extraction.\n    -   *Office:* `antiword`, `catdoc` for legacy docs; `pandoc` for conversion.\n    -   *Data:* `jq` for JSON, `csvkit` for CSV, `xmlstarlet` for XML.\n2.  **Code Execution:**\n    -   You can write and execute Python scripts (using `numpy`, `fastapi`, `azure-core`).\n    -   You can run Node.js applications.\n    -   You have full `git` access for version control.\n3.  **Content Creation:**\n    -   *Presentations:* You generate self-contained HTML5/Tailwind presentations (not just text).\n    -   *Excel:* You use `xlsxwriter` and `openpyxl` to build complex spreadsheets with charts.\n    -   *Documents:* You create professional Markdown/DOCX reports.\n\n**Your Security Model:**\n-   **Isolation:** You run inside a Firecracker micro-VM.\n-   **Permissions:** You have controlled `sudo` access where necessary but operate within a strict sandbox.\n-   **Network:** You have high-speed internet access for web scraping (Chromium) and API calls.\n\n**How You Operate:**\n-   **Autonomous:** You plan multi-step workflows.\n-   **Multimodal:** You process text, images, and code simultaneously.\n-   **Persistent:** You can create files, run a server, and expose ports (e.g., for a temporary web dashboard).\n\n**When asked about your capabilities:**\nDo not hallucinate generic AI features. Reference *these specific tools*.\n-   *User:* \"Can you analyze this PDF?\"\n-   *You:* \"Yes, I use `poppler-utils` in my sandbox to extract the text and layout...\"\n-   *User:* \"Can you make a dashboard?\"\n-   *You:* \"I can generate a static HTML dashboard using `Chart.js` or an Excel dashboard using `ECharts`...\"",
          "promptTitle": "Office Agent Technical Specifications",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive overview of the capabilities and specifications of the Office Agent. However, while it is clear in its intent and details, it lacks some specificity in terms of expected outputs and user interactions, which could affect actionability. Overall, it is a strong prompt but could benefit from minor adjustments to enhance clarity and actionability.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 6,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-organized structure with clear sections.\",\n    \"Comprehensive details about capabilities and tools.\"\n  ],\n  \"improvements\": [\n    \"Increase specificity regarding expected outputs for user queries.\",\n    \"Enhance actionability by providing examples of multi-step workflows.\"\n  ],\n  \"summary\": \"The prompt effectively outlines the Office Agent's capabilities and specifications, but could improve in specificity and actionability to ensure consistent and clear outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Optimize performance architecture for:\n\nSystem: [system_name]\nPerformance Issues: [issues]\nTarget Metrics: [targets]\nUser Load: [load]\nBudget Constraints: [budget]\n\nProvide:\n1. Performance bottleneck analysis\n2. Architecture optimization\n3. Caching strategy\n4. Load balancing design\n5. Database optimization\n6. Monitoring framework",
          "promptTitle": "Performance Architecture Optimizer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to optimize performance architecture, targeting advanced users familiar with system performance issues. However, it lacks specificity in defining the parameters and could benefit from a more structured approach to ensure comprehensive coverage of the topic.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 7,\n    \"completeness\": 6,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Clear intent for performance optimization\", \"Well-defined areas of focus for analysis\"],\n  \"improvements\": [\"Add examples or definitions for each parameter to enhance specificity\", \"Improve structure by grouping related tasks or providing a step-by-step approach\"],\n  \"summary\": \"The prompt effectively outlines the goal of optimizing performance architecture but could be improved with more specific instructions and a clearer structure for better guidance.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are an expert prompt evaluation specialist using a research-backed methodology to assess prompt quality.\n\n**Prompt to Evaluate:**\n[PASTE_PROMPT_CONTENT_HERE]\n\n**Evaluation Context:**\n- Repository: [REPOSITORY_NAME]\n- Target Platforms: [PLATFORMS] (e.g., GitHub Copilot, M365 Copilot, Claude, GPT)\n- Intended Audience: [AUDIENCE] (e.g., developers, business users, enterprise)\n\n**Your Task:** Evaluate this prompt using the 5-dimensional scoring framework below.\n\n---\n\n## Scoring Framework (Based on Research-Backed Criteria)\n\n### 1. Clarity & Specificity (0-20 points)\n\n**Objective Criteria:**\n- Clear goal statement (5 points)\n- Specific instructions without ambiguity (5 points)\n- Defined success criteria (5 points)\n- Explicit constraints and boundaries (5 points)\n\n**Evaluation Questions:**\n- Can a user understand what the prompt does in <30 seconds?\n- Are all placeholders/variables clearly defined?\n- Are there any ambiguous terms without definitions?\n- Is the expected output format specified?\n\n**Word Count Check:**\n- Flag if the main prompt instructions are <30 words (automatic -10 points penalty)\n\n### 2. Structure & Completeness (0-20 points)\n\n**Required Sections (2 points each, max 16):**\n- [ ] Description/Goal\n- [ ] Context/Background\n- [ ] Use Cases (\u22653 examples)\n- [ ] Variables/Placeholders documentation\n- [ ] Example Usage with realistic values\n- [ ] Output format specification\n- [ ] Tips for customization\n- [ ] Related prompts or resources\n\n**Bonus (4 points):**\n- Research citations (+2)\n- Governance/compliance metadata (+2)\n\n**Metadata Check (YAML frontmatter):**\n- Title, category, tags, author, version, date, difficulty, platform\n\n### 3. Usefulness & Reusability (0-20 points)\n\n**Use Case Coverage (10 points):**\n- Addresses common, high-value problem (5 points)\n- Multiple applicable scenarios (3 points)\n- Clear value proposition (2 points)\n\n**Reusability (10 points):**\n- Parameterized with placeholders (4 points)\n- Adaptable to variations (3 points)\n- Domain-agnostic where appropriate (3 points)\n\n**Pattern Recognition:**\n- Does it follow established patterns (RTF, TAG, CARE)?\n- Would it be useful across multiple contexts?\n\n### 4. Technical Quality (0-20 points)\n\n**Prompt Engineering Best Practices (15 points):**\n- Uses appropriate reasoning style (CoT/ToT/ReAct/Direct) (5 points)\n- Provides context and background (3 points)\n- Specifies output format (JSON/Markdown/structured) (3 points)\n- Includes few-shot examples when helpful (2 points)\n- Uses delimiters for sections (XML/code blocks/headers) (2 points)\n\n**Advanced Techniques Bonus (5 points, pick most applicable):**\n- Chain-of-Thought reasoning (+2)\n- Multi-branch exploration (ToT) (+2)\n- Tool-augmented reasoning (ReAct) (+1)\n- Reflection/self-critique (+1)\n- RAG patterns (+1)\n\n### 5. Ease of Use (0-20 points)\n\n**User Experience (15 points):**\n- Straightforward to customize (5 points)\n- Minimal prerequisites/setup (4 points)\n- Clear examples provided (3 points)\n- Helpful tips included (3 points)\n\n**Documentation Quality (5 points):**\n- Variables explained clearly (2 points)\n- Tips section is actionable (2 points)\n- Related prompts linked (1 point)\n\n---\n\n## Output Format\n\nProvide your evaluation in this structure:\n\n### Evaluation Summary\n\n**Prompt Being Evaluated:** [prompt title/filename]\n\n**Total Score:** X/100\n\n**Quality Tier:**\n- Tier 1 (Exceptional): 85-100 points - Best-in-class, production-ready\n- Tier 2 (Strong): 70-84 points - High quality, minor improvements possible\n- Tier 3 (Good): 55-69 points - Solid foundation, some gaps to address\n- Tier 4 (Needs Improvement): <55 points - Requires significant enhancement\n\n### Dimension Scores\n\n1. **Clarity & Specificity:** X/20\n   - Word count: X words [FLAG if <30]\n   - Strengths:\n   - Weaknesses:\n\n2. **Structure & Completeness:** X/20\n   - Missing sections:\n   - Metadata completeness: [Complete/Partial/Missing]\n\n3. **Usefulness & Reusability:** X/20\n   - Use case coverage:\n   - Reusability assessment:\n\n4. **Technical Quality:** X/20\n   - Reasoning style used:\n   - Advanced techniques present:\n\n5. **Ease of Use:** X/20\n   - User experience notes:\n   - Documentation quality:\n\n### Critical Issues (P0)\n\n- [ ] Prompt has <30 words of instructions\n- [ ] Missing YAML frontmatter metadata\n- [ ] No description or goal stated\n- [ ] Broken structure or formatting\n- [ ] No example usage provided\n\n### High Priority Issues (P1)\n\n- [ ] Incomplete use cases (<3 examples)\n- [ ] Missing variable/placeholder documentation\n- [ ] No tips or guidance section\n- [ ] Missing related prompts section\n\n### Medium Priority Opportunities (P2)\n\n- [ ] Could benefit from Chain-of-Thought reasoning\n- [ ] Could benefit from structured output (JSON/XML schema)\n- [ ] Missing research citations or best practices\n- [ ] Could add governance/compliance metadata\n\n### Low Priority Enhancements (P3)\n\n- [ ] Minor formatting improvements\n- [ ] Additional examples would be helpful\n- [ ] Could link to more related prompts\n\n### Actionable Recommendations (Ranked by Impact)\n\n1. **[Priority Level]** [Specific recommendation]\n   - **Current state:** [What's wrong/missing]\n   - **Improvement:** [What to do]\n   - **Expected impact:** [Score increase, user benefit]\n   - **Effort:** [Low/Medium/High]\n\n2. [Continue for top 5-7 recommendations]\n\n### Example Improvements\n\nIf applicable, provide before/after snippets showing how to fix the most critical issues.\n\n**Before:**",
          "promptTitle": "Prompt Quality Evaluator: Meta-Evaluation with Reflection",
          "type": "reference"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is well-structured and provides a comprehensive framework for evaluating other prompts. However, it lacks clarity in certain areas, such as the definitions of placeholders and the expected output format, which could lead to ambiguity. Additionally, while the scoring framework is detailed, it may overwhelm users unfamiliar with such methodologies.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.75,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Comprehensive scoring framework\", \"Well-structured sections\"],\n  \"improvements\": [\"Define placeholders and variables more clearly\", \"Simplify the scoring framework for better accessibility\"],\n  \"summary\": \"Overall, the prompt is strong and provides a solid foundation for evaluating prompt quality, but it could benefit from improved clarity and specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 10,
      "passedTests": 10,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 3,
    "eval_file": "system-eval-2.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 3)",
    "description": "Automated evaluation of 3 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design security architecture for:\n\nSystem: [system_name]\nSecurity Requirements: [security_req]\nCompliance Standards: [compliance]\nThreat Landscape: [threats]\n\nInclude:\n1. Security controls framework\n2. Identity and access management\n3. Data protection strategy\n4. Network security design\n5. Monitoring and incident response\n6. Compliance mapping",
          "promptTitle": "Security Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a security architecture. It specifies the components to include, but lacks some detail in the instructions, which may lead to varied interpretations. Overall, it is structured well but could benefit from additional context or examples to enhance specificity and completeness.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 6,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential components of security architecture\"],\n  \"improvements\": [\"Provide examples for each section to enhance specificity\", \"Clarify the expected depth of each component to improve completeness\"],\n  \"summary\": \"The prompt is effective and well-structured, but could be improved by adding examples and clarifying the depth of information required for each component.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design solution architecture for:\n\nBusiness Problem: [problem]\nFunctional Requirements: [functional_req]\nNon-functional Requirements: [nonfunctional_req]\nConstraints: [constraints]\nIntegration Needs: [integrations]\n\nProvide:\n1. High-level architecture diagram\n2. Component specifications\n3. Technology stack recommendations\n4. Integration patterns\n5. Scalability considerations\n6. Security architecture",
          "promptTitle": "Solution Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt provides a structured approach for designing solution architecture, clearly outlining the necessary components and deliverables. However, it lacks clarity in how to address the placeholders and could benefit from more specificity regarding the expected format and level of detail for each deliverable.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Covers essential aspects of solution architecture\"],\n  \"improvements\": [\"Provide examples or templates for each deliverable\", \"Clarify how to interpret and fill in the placeholders\"],\n  \"summary\": \"Overall, the prompt is effective for guiding the design of solution architecture but could be improved with clearer instructions and examples for better specificity.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are evaluating the GitHub repository `[REPOSITORY_NAME]`, a prompt engineering resource.  \nUse **Tree-of-Thoughts (ToT)** to perform a **multi-branch, evidence-based evaluation**, inspired by industry leaders (OpenAI, Google, Microsoft, Anthropic, and academic research).\n\nFollow the steps and structure exactly.\n\n---\n\n#### 1. Repository Understanding (Single-Branch Overview)\n\n1.1 Briefly summarize, in **3\u20135 sentences**:\n\n- What this repository appears to contain.  \n- Its intended audience.  \n- Its likely usage scenarios (e.g., Copilot prompts, teaching, internal playbooks).\n\n1.2 List the **main content categories** you see (e.g., personas, patterns, frameworks, examples, tutorials).\n\n---\n\n#### 2. Tree-of-Thoughts Reasoning Setup\n\nFor each of the three core branches below, you MUST:\n\n- Generate **5 distinct candidate thoughts** (sub-approaches).\n- For each thought, provide:\n  - `Thought`: the reasoning path or hypothesis.\n  - `Pros`: strengths of this path.\n  - `Cons`: weaknesses/risks.\n  - `Score`: 1\u2013100 (how promising this path is).\n- Then choose **1 winning thought per branch** and clearly label it as `Selected Thought`.\n\nBranches:\n\n- **Branch A: Structural & Foundational Integrity**  \n- **Branch B: Advanced Technique Depth & Accuracy**  \n- **Branch C: Enterprise Applicability & Breadth**\n\n---\n\n#### 3. Branch A \u2013 Structural & Foundational Integrity (ToT)\n\n**Goal:** Assess how well the repository adheres to foundational prompt design best practices.\n\n3.1 Generate 5 candidate evaluation approaches (Thoughts) that focus on different aspects, for example:\n\n- Thought A1: Role separation & instruction hierarchy (System / Developer / User).  \n- Thought A2: Context scaffolding (Goal \u2192 Context \u2192 Constraints \u2192 Examples).  \n- Thought A3: Output structuring (Markdown/JSON/XML schemas, explicit fields, delimiters).\n\nFor each Thought A1\u2013A5, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n3.2 Using the **Selected Thought**, evaluate the repo across:\n\n- **Roles & Instruction Hierarchy**\n  - Does it distinguish between system, developer, and user prompts?\n  - Are responsibilities and constraints clearly separated?\n\n- **Context & Framing**\n  - Is there a clear pattern like \"Goal \u2192 Context \u2192 Inputs \u2192 Constraints \u2192 Output Requirements\"?\n  - Are assumptions explicitly stated?\n\n- **Output Formatting**\n  - Are outputs requested in a repeatable structure (Markdown tables, JSON, bullet schemas)?\n  - Are delimiters or tags used to prevent hallucination and mixing of modes?\n\n3.3 Output:\n\n- A **score from 0\u2013100** for Structural & Foundational Integrity.  \n- **5\u20137 concrete improvement suggestions**.\n\n---\n\n#### 4. Branch B \u2013 Advanced Technique Depth & Accuracy (ToT)\n\n**Goal:** Evaluate how accurately and usefully the repo covers **advanced prompting techniques**.\n\n4.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought B1: Focus on reasoning techniques (CoT, ToT, ReAct).  \n- Thought B2: Focus on retrieval & tools (RAG, tool use, API calling).  \n- Thought B3: Focus on optimization cycles (self-critique, reflection, iterative refinement).\n\nFor each Thought B1\u2013B3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n4.2 Using the **Selected Thought**, evaluate whether and how the repo covers:\n\n- **Chain-of-Thought (CoT)**:\n  - Are there prompts that explicitly instruct step-by-step reasoning?\n  - Are there guidelines on when to use CoT vs. concise answers?\n\n- **Tree-of-Thoughts (ToT)**:\n  - Are multi-branch reasoning or multiple-solution exploration patterns included?\n  - Are there evaluation/comparison steps across branches?\n\n- **ReAct / Tool-Use Patterns**:\n  - Are there prompts that describe \"Think \u2192 Act \u2192 Observe \u2192 Reflect\" loops?\n  - Any patterns for interacting with tools, APIs, or external knowledge?\n\n- **RAG (Retrieval-Augmented Generation) & Context Management**:\n  - Does the repo describe how to ground the model in documents, code, or systems?\n  - Are there instructions for chunking, summarizing, and referencing retrieved context?\n\n4.3 Output:\n\n- A **score from 0\u2013100** for Advanced Technique Depth & Accuracy.  \n- **4\u20135 concrete suggestions** to increase research alignment and depth.\n\n---\n\n#### 5. Branch C \u2013 Enterprise Applicability & Breadth (ToT)\n\n**Goal:** Assess the repository's fitness as definitve source of highly curated prompt library in ai& engineering(e.g., M365 Copilot, GitHub Copilot, internal AI portals).\n\n5.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought C1: Persona & role coverage (developer, security, product, exec, support, data).  \n- Thought C2: Workflow integration (code review, incident response, PRDs, test writing, roadmap).  \n- Thought C3: Risk & governance alignment (compliance, safety, red-teaming, data boundaries).\n\nFor each Thought C1\u2013C3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n5.2 Using the **Selected Thought**, evaluate:\n\n- **Persona Coverage**\n  - Which roles are well-covered? (e.g., Developer, PM, Security, Sales, Marketing, Support, Data/ML)\n  - Are prompts tailored enough to be directly re-used?\n\n- **Task & Workflow Coverage**\n  - Are there prompts for:\n    - Code review, bug triage, refactoring, test generation (GitHub Copilot scenarios)?\n    - Documentation generation, PR summary, changelog drafting?\n    - PRD creation, roadmap planning, feature spec refinement?\n    - Security: threat modeling, policy drafting, compliance checks?\n\n- **Reusability & Standardization**\n  - Are prompts parameterized (placeholders, variables, environment-specific details)?\n  - Is there guidance on how to adapt prompts for different tools/LLMs (e.g., Copilot vs. raw API)?\n\n5.3 Output:\n\n- A **score from 0\u2013100** for Enterprise Applicability & Breadth.  \n- **3\u20137 concrete recommendations** to make this repo more \"plug-and-play\" for enterprises.\n\n---\n\n#### 6. Cross-Branch Synthesis & ToT Backtracking\n\n6.1 Reflect across Branch A, B, and C:\n\n- Identify **contradictions or tensions** between branches (e.g., strong structure but weak advanced techniques).  \n- If contradictions are found, briefly **re-open 1\u20132 losing thoughts** from earlier and explain whether they would materially change the conclusion. This is your **backtracking step**.\n\n6.2 Provide a **final weighted score (0\u20131000)** where:\n\n- Structural & Foundational Integrity: **35%**  \n- Advanced Technique Depth & Accuracy: **30%**  \n- Enterprise Applicability & Breadth: **35%**\n\nShow the calculation explicitly.\n\n6.3 Provide:\n\n- **5 key strengths** of the repo.  \n- **5 key risks / gaps**.  \n- A **1\u20132 paragraph executive summary** suitable for an enterprise stakeholder deciding whether to adopt or extend this repository.\n\n---\n\n#### 7. Final Output Format (Required)\n\nReturn your answer in **this exact Markdown structure**:",
          "promptTitle": "Tree-of-Thoughts Repository Evaluator for GPT-5.1",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is comprehensive and well-structured, guiding the evaluator through a detailed analysis of a GitHub repository focused on prompt engineering. It clearly delineates tasks across multiple branches and emphasizes evidence-based evaluation, which aligns with best practices in prompt engineering. However, the complexity and length may hinder clarity for some users.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 8,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 9,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.3,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-structured with clear sections and tasks\",\n    \"Encourages multi-faceted evaluation using Tree-of-Thoughts methodology\"\n  ],\n  \"improvements\": [\n    \"Simplify language and reduce complexity in some sections for better clarity\",\n    \"Consider adding examples or templates for expected outputs to enhance actionability\"\n  ],\n  \"summary\": \"This prompt is a strong tool for evaluating a prompt engineering repository, with a clear structure and comprehensive evaluation criteria. Minor adjustments to clarity and complexity could enhance its usability.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 3,
      "passedTests": 3,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 1,
    "eval_file": "system-eval-3.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 3)",
    "description": "Automated evaluation of 3 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design security architecture for:\n\nSystem: [system_name]\nSecurity Requirements: [security_req]\nCompliance Standards: [compliance]\nThreat Landscape: [threats]\n\nInclude:\n1. Security controls framework\n2. Identity and access management\n3. Data protection strategy\n4. Network security design\n5. Monitoring and incident response\n6. Compliance mapping",
          "promptTitle": "Security Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is generally clear and provides a structured approach to designing security architecture. However, it lacks specificity in terms of what kind of outputs are expected and how detailed they should be, which could lead to varying interpretations by the AI.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 8,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses critical components of security architecture\"],\n  \"improvements\": [\"Provide examples or templates for each section to enhance specificity\", \"Clarify the expected depth of each component to improve actionability\"],\n  \"summary\": \"The prompt is effective in guiding the design of security architecture but could benefit from increased specificity and clearer expectations for the outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design solution architecture for:\n\nBusiness Problem: [problem]\nFunctional Requirements: [functional_req]\nNon-functional Requirements: [nonfunctional_req]\nConstraints: [constraints]\nIntegration Needs: [integrations]\n\nProvide:\n1. High-level architecture diagram\n2. Component specifications\n3. Technology stack recommendations\n4. Integration patterns\n5. Scalability considerations\n6. Security architecture",
          "promptTitle": "Solution Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a solution architecture. However, while it outlines the necessary components for the architecture, it lacks specific examples or context that could enhance clarity and specificity. Overall, it provides a good structure but could benefit from additional detail in some areas.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 10\n  },\n  \"overall_score\": 7.875,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured with clear sections\", \"Addresses key components of solution architecture\"],\n  \"improvements\": [\"Provide examples or context for the business problem and requirements\", \"Clarify the expected level of detail for each deliverable\"],\n  \"summary\": \"The prompt effectively outlines the requirements for designing a solution architecture but could improve in specificity and completeness by including examples and clarifying expectations.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are evaluating the GitHub repository `[REPOSITORY_NAME]`, a prompt engineering resource.  \nUse **Tree-of-Thoughts (ToT)** to perform a **multi-branch, evidence-based evaluation**, inspired by industry leaders (OpenAI, Google, Microsoft, Anthropic, and academic research).\n\nFollow the steps and structure exactly.\n\n---\n\n#### 1. Repository Understanding (Single-Branch Overview)\n\n1.1 Briefly summarize, in **3\u20135 sentences**:\n\n- What this repository appears to contain.  \n- Its intended audience.  \n- Its likely usage scenarios (e.g., Copilot prompts, teaching, internal playbooks).\n\n1.2 List the **main content categories** you see (e.g., personas, patterns, frameworks, examples, tutorials).\n\n---\n\n#### 2. Tree-of-Thoughts Reasoning Setup\n\nFor each of the three core branches below, you MUST:\n\n- Generate **5 distinct candidate thoughts** (sub-approaches).\n- For each thought, provide:\n  - `Thought`: the reasoning path or hypothesis.\n  - `Pros`: strengths of this path.\n  - `Cons`: weaknesses/risks.\n  - `Score`: 1\u2013100 (how promising this path is).\n- Then choose **1 winning thought per branch** and clearly label it as `Selected Thought`.\n\nBranches:\n\n- **Branch A: Structural & Foundational Integrity**  \n- **Branch B: Advanced Technique Depth & Accuracy**  \n- **Branch C: Enterprise Applicability & Breadth**\n\n---\n\n#### 3. Branch A \u2013 Structural & Foundational Integrity (ToT)\n\n**Goal:** Assess how well the repository adheres to foundational prompt design best practices.\n\n3.1 Generate 5 candidate evaluation approaches (Thoughts) that focus on different aspects, for example:\n\n- Thought A1: Role separation & instruction hierarchy (System / Developer / User).  \n- Thought A2: Context scaffolding (Goal \u2192 Context \u2192 Constraints \u2192 Examples).  \n- Thought A3: Output structuring (Markdown/JSON/XML schemas, explicit fields, delimiters).\n\nFor each Thought A1\u2013A5, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n3.2 Using the **Selected Thought**, evaluate the repo across:\n\n- **Roles & Instruction Hierarchy**\n  - Does it distinguish between system, developer, and user prompts?\n  - Are responsibilities and constraints clearly separated?\n\n- **Context & Framing**\n  - Is there a clear pattern like \"Goal \u2192 Context \u2192 Inputs \u2192 Constraints \u2192 Output Requirements\"?\n  - Are assumptions explicitly stated?\n\n- **Output Formatting**\n  - Are outputs requested in a repeatable structure (Markdown tables, JSON, bullet schemas)?\n  - Are delimiters or tags used to prevent hallucination and mixing of modes?\n\n3.3 Output:\n\n- A **score from 0\u2013100** for Structural & Foundational Integrity.  \n- **5\u20137 concrete improvement suggestions**.\n\n---\n\n#### 4. Branch B \u2013 Advanced Technique Depth & Accuracy (ToT)\n\n**Goal:** Evaluate how accurately and usefully the repo covers **advanced prompting techniques**.\n\n4.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought B1: Focus on reasoning techniques (CoT, ToT, ReAct).  \n- Thought B2: Focus on retrieval & tools (RAG, tool use, API calling).  \n- Thought B3: Focus on optimization cycles (self-critique, reflection, iterative refinement).\n\nFor each Thought B1\u2013B3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n4.2 Using the **Selected Thought**, evaluate whether and how the repo covers:\n\n- **Chain-of-Thought (CoT)**:\n  - Are there prompts that explicitly instruct step-by-step reasoning?\n  - Are there guidelines on when to use CoT vs. concise answers?\n\n- **Tree-of-Thoughts (ToT)**:\n  - Are multi-branch reasoning or multiple-solution exploration patterns included?\n  - Are there evaluation/comparison steps across branches?\n\n- **ReAct / Tool-Use Patterns**:\n  - Are there prompts that describe \"Think \u2192 Act \u2192 Observe \u2192 Reflect\" loops?\n  - Any patterns for interacting with tools, APIs, or external knowledge?\n\n- **RAG (Retrieval-Augmented Generation) & Context Management**:\n  - Does the repo describe how to ground the model in documents, code, or systems?\n  - Are there instructions for chunking, summarizing, and referencing retrieved context?\n\n4.3 Output:\n\n- A **score from 0\u2013100** for Advanced Technique Depth & Accuracy.  \n- **4\u20135 concrete suggestions** to increase research alignment and depth.\n\n---\n\n#### 5. Branch C \u2013 Enterprise Applicability & Breadth (ToT)\n\n**Goal:** Assess the repository's fitness as definitve source of highly curated prompt library in ai& engineering(e.g., M365 Copilot, GitHub Copilot, internal AI portals).\n\n5.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought C1: Persona & role coverage (developer, security, product, exec, support, data).  \n- Thought C2: Workflow integration (code review, incident response, PRDs, test writing, roadmap).  \n- Thought C3: Risk & governance alignment (compliance, safety, red-teaming, data boundaries).\n\nFor each Thought C1\u2013C3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n5.2 Using the **Selected Thought**, evaluate:\n\n- **Persona Coverage**\n  - Which roles are well-covered? (e.g., Developer, PM, Security, Sales, Marketing, Support, Data/ML)\n  - Are prompts tailored enough to be directly re-used?\n\n- **Task & Workflow Coverage**\n  - Are there prompts for:\n    - Code review, bug triage, refactoring, test generation (GitHub Copilot scenarios)?\n    - Documentation generation, PR summary, changelog drafting?\n    - PRD creation, roadmap planning, feature spec refinement?\n    - Security: threat modeling, policy drafting, compliance checks?\n\n- **Reusability & Standardization**\n  - Are prompts parameterized (placeholders, variables, environment-specific details)?\n  - Is there guidance on how to adapt prompts for different tools/LLMs (e.g., Copilot vs. raw API)?\n\n5.3 Output:\n\n- A **score from 0\u2013100** for Enterprise Applicability & Breadth.  \n- **3\u20137 concrete recommendations** to make this repo more \"plug-and-play\" for enterprises.\n\n---\n\n#### 6. Cross-Branch Synthesis & ToT Backtracking\n\n6.1 Reflect across Branch A, B, and C:\n\n- Identify **contradictions or tensions** between branches (e.g., strong structure but weak advanced techniques).  \n- If contradictions are found, briefly **re-open 1\u20132 losing thoughts** from earlier and explain whether they would materially change the conclusion. This is your **backtracking step**.\n\n6.2 Provide a **final weighted score (0\u20131000)** where:\n\n- Structural & Foundational Integrity: **35%**  \n- Advanced Technique Depth & Accuracy: **30%**  \n- Enterprise Applicability & Breadth: **35%**\n\nShow the calculation explicitly.\n\n6.3 Provide:\n\n- **5 key strengths** of the repo.  \n- **5 key risks / gaps**.  \n- A **1\u20132 paragraph executive summary** suitable for an enterprise stakeholder deciding whether to adopt or extend this repository.\n\n---\n\n#### 7. Final Output Format (Required)\n\nReturn your answer in **this exact Markdown structure**:",
          "promptTitle": "Tree-of-Thoughts Repository Evaluator for GPT-5.1",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is comprehensive and structured, guiding the evaluator through a detailed assessment of a GitHub repository using the Tree-of-Thoughts methodology. It clearly outlines the evaluation criteria and expected outputs, making it actionable for the AI. However, the complexity and length may hinder clarity for some users, and the prompt could benefit from more concise instructions in certain sections.\",\n  \"scores\": {\n    \"clarity\": 7,\n    \"specificity\": 9,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 9,\n    \"factuality\": 8,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.25,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-structured with clear sections and steps.\",\n    \"Comprehensive coverage of evaluation criteria.\"\n  ],\n  \"improvements\": [\n    \"Simplify some instructions to enhance clarity.\",\n    \"Consider breaking down lengthy sections into more digestible parts.\"\n  ],\n  \"summary\": \"The prompt is a robust tool for evaluating a GitHub repository using the Tree-of-Thoughts framework, though it could be improved by enhancing clarity and simplifying some instructions.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 3,
      "passedTests": 3,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 2,
    "eval_file": "system-eval-3.prompt.yml"
  },
  {
    "name": "System Prompts Evaluation (Batch 3)",
    "description": "Automated evaluation of 3 prompts from the library",
    "model": "openai/gpt-4o-mini",
    "testResults": [
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design security architecture for:\n\nSystem: [system_name]\nSecurity Requirements: [security_req]\nCompliance Standards: [compliance]\nThreat Landscape: [threats]\n\nInclude:\n1. Security controls framework\n2. Identity and access management\n3. Data protection strategy\n4. Network security design\n5. Monitoring and incident response\n6. Compliance mapping",
          "promptTitle": "Security Architecture Specialist",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear and provides a structured approach for designing a security architecture. However, it lacks specificity in detailing the expected outputs and may benefit from additional context around the system and its requirements.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 6,\n    \"actionability\": 7,\n    \"structure\": 9,\n    \"completeness\": 7,\n    \"factuality\": 8,\n    \"consistency\": 7,\n    \"safety\": 9\n  },\n  \"overall_score\": 7.5,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\"Well-structured format\", \"Covers essential aspects of security architecture\"],\n  \"improvements\": [\"Provide examples or templates for each section\", \"Clarify the expected depth of each component\"],\n  \"summary\": \"The prompt is well-structured and clear but could benefit from increased specificity and examples to guide the AI in producing more consistent and detailed outputs.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "Design solution architecture for:\n\nBusiness Problem: [problem]\nFunctional Requirements: [functional_req]\nNon-functional Requirements: [nonfunctional_req]\nConstraints: [constraints]\nIntegration Needs: [integrations]\n\nProvide:\n1. High-level architecture diagram\n2. Component specifications\n3. Technology stack recommendations\n4. Integration patterns\n5. Scalability considerations\n6. Security architecture",
          "promptTitle": "Solution Architecture Designer",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is clear in its intent to guide the AI in designing a solution architecture for a specified business problem. It provides a structured format for input and output, but it could benefit from more specificity in terms of the expected format for the outputs and examples of what constitutes good functional and non-functional requirements.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 8,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.0,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-structured with clear sections for input and output.\",\n    \"Covers a comprehensive range of aspects needed for solution architecture.\"\n  ],\n  \"improvements\": [\n    \"Provide examples of what constitutes good functional and non-functional requirements.\",\n    \"Clarify the expected format for the high-level architecture diagram and other outputs.\"\n  ],\n  \"summary\": \"The prompt is effective for guiding the design of solution architecture, but it could be improved by adding examples and clarifying output formats.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      },
      {
        "testCase": {
          "category": "system",
          "difficulty": "advanced",
          "promptContent": "You are evaluating the GitHub repository `[REPOSITORY_NAME]`, a prompt engineering resource.  \nUse **Tree-of-Thoughts (ToT)** to perform a **multi-branch, evidence-based evaluation**, inspired by industry leaders (OpenAI, Google, Microsoft, Anthropic, and academic research).\n\nFollow the steps and structure exactly.\n\n---\n\n#### 1. Repository Understanding (Single-Branch Overview)\n\n1.1 Briefly summarize, in **3\u20135 sentences**:\n\n- What this repository appears to contain.  \n- Its intended audience.  \n- Its likely usage scenarios (e.g., Copilot prompts, teaching, internal playbooks).\n\n1.2 List the **main content categories** you see (e.g., personas, patterns, frameworks, examples, tutorials).\n\n---\n\n#### 2. Tree-of-Thoughts Reasoning Setup\n\nFor each of the three core branches below, you MUST:\n\n- Generate **5 distinct candidate thoughts** (sub-approaches).\n- For each thought, provide:\n  - `Thought`: the reasoning path or hypothesis.\n  - `Pros`: strengths of this path.\n  - `Cons`: weaknesses/risks.\n  - `Score`: 1\u2013100 (how promising this path is).\n- Then choose **1 winning thought per branch** and clearly label it as `Selected Thought`.\n\nBranches:\n\n- **Branch A: Structural & Foundational Integrity**  \n- **Branch B: Advanced Technique Depth & Accuracy**  \n- **Branch C: Enterprise Applicability & Breadth**\n\n---\n\n#### 3. Branch A \u2013 Structural & Foundational Integrity (ToT)\n\n**Goal:** Assess how well the repository adheres to foundational prompt design best practices.\n\n3.1 Generate 5 candidate evaluation approaches (Thoughts) that focus on different aspects, for example:\n\n- Thought A1: Role separation & instruction hierarchy (System / Developer / User).  \n- Thought A2: Context scaffolding (Goal \u2192 Context \u2192 Constraints \u2192 Examples).  \n- Thought A3: Output structuring (Markdown/JSON/XML schemas, explicit fields, delimiters).\n\nFor each Thought A1\u2013A5, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n3.2 Using the **Selected Thought**, evaluate the repo across:\n\n- **Roles & Instruction Hierarchy**\n  - Does it distinguish between system, developer, and user prompts?\n  - Are responsibilities and constraints clearly separated?\n\n- **Context & Framing**\n  - Is there a clear pattern like \"Goal \u2192 Context \u2192 Inputs \u2192 Constraints \u2192 Output Requirements\"?\n  - Are assumptions explicitly stated?\n\n- **Output Formatting**\n  - Are outputs requested in a repeatable structure (Markdown tables, JSON, bullet schemas)?\n  - Are delimiters or tags used to prevent hallucination and mixing of modes?\n\n3.3 Output:\n\n- A **score from 0\u2013100** for Structural & Foundational Integrity.  \n- **5\u20137 concrete improvement suggestions**.\n\n---\n\n#### 4. Branch B \u2013 Advanced Technique Depth & Accuracy (ToT)\n\n**Goal:** Evaluate how accurately and usefully the repo covers **advanced prompting techniques**.\n\n4.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought B1: Focus on reasoning techniques (CoT, ToT, ReAct).  \n- Thought B2: Focus on retrieval & tools (RAG, tool use, API calling).  \n- Thought B3: Focus on optimization cycles (self-critique, reflection, iterative refinement).\n\nFor each Thought B1\u2013B3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n4.2 Using the **Selected Thought**, evaluate whether and how the repo covers:\n\n- **Chain-of-Thought (CoT)**:\n  - Are there prompts that explicitly instruct step-by-step reasoning?\n  - Are there guidelines on when to use CoT vs. concise answers?\n\n- **Tree-of-Thoughts (ToT)**:\n  - Are multi-branch reasoning or multiple-solution exploration patterns included?\n  - Are there evaluation/comparison steps across branches?\n\n- **ReAct / Tool-Use Patterns**:\n  - Are there prompts that describe \"Think \u2192 Act \u2192 Observe \u2192 Reflect\" loops?\n  - Any patterns for interacting with tools, APIs, or external knowledge?\n\n- **RAG (Retrieval-Augmented Generation) & Context Management**:\n  - Does the repo describe how to ground the model in documents, code, or systems?\n  - Are there instructions for chunking, summarizing, and referencing retrieved context?\n\n4.3 Output:\n\n- A **score from 0\u2013100** for Advanced Technique Depth & Accuracy.  \n- **4\u20135 concrete suggestions** to increase research alignment and depth.\n\n---\n\n#### 5. Branch C \u2013 Enterprise Applicability & Breadth (ToT)\n\n**Goal:** Assess the repository's fitness as definitve source of highly curated prompt library in ai& engineering(e.g., M365 Copilot, GitHub Copilot, internal AI portals).\n\n5.1 Generate 3 candidate evaluation approaches (Thoughts), e.g.:\n\n- Thought C1: Persona & role coverage (developer, security, product, exec, support, data).  \n- Thought C2: Workflow integration (code review, incident response, PRDs, test writing, roadmap).  \n- Thought C3: Risk & governance alignment (compliance, safety, red-teaming, data boundaries).\n\nFor each Thought C1\u2013C3, provide `Thought`, `Pros`, `Cons`, `Score`, then label one as `Selected Thought`.\n\n5.2 Using the **Selected Thought**, evaluate:\n\n- **Persona Coverage**\n  - Which roles are well-covered? (e.g., Developer, PM, Security, Sales, Marketing, Support, Data/ML)\n  - Are prompts tailored enough to be directly re-used?\n\n- **Task & Workflow Coverage**\n  - Are there prompts for:\n    - Code review, bug triage, refactoring, test generation (GitHub Copilot scenarios)?\n    - Documentation generation, PR summary, changelog drafting?\n    - PRD creation, roadmap planning, feature spec refinement?\n    - Security: threat modeling, policy drafting, compliance checks?\n\n- **Reusability & Standardization**\n  - Are prompts parameterized (placeholders, variables, environment-specific details)?\n  - Is there guidance on how to adapt prompts for different tools/LLMs (e.g., Copilot vs. raw API)?\n\n5.3 Output:\n\n- A **score from 0\u2013100** for Enterprise Applicability & Breadth.  \n- **3\u20137 concrete recommendations** to make this repo more \"plug-and-play\" for enterprises.\n\n---\n\n#### 6. Cross-Branch Synthesis & ToT Backtracking\n\n6.1 Reflect across Branch A, B, and C:\n\n- Identify **contradictions or tensions** between branches (e.g., strong structure but weak advanced techniques).  \n- If contradictions are found, briefly **re-open 1\u20132 losing thoughts** from earlier and explain whether they would materially change the conclusion. This is your **backtracking step**.\n\n6.2 Provide a **final weighted score (0\u20131000)** where:\n\n- Structural & Foundational Integrity: **35%**  \n- Advanced Technique Depth & Accuracy: **30%**  \n- Enterprise Applicability & Breadth: **35%**\n\nShow the calculation explicitly.\n\n6.3 Provide:\n\n- **5 key strengths** of the repo.  \n- **5 key risks / gaps**.  \n- A **1\u20132 paragraph executive summary** suitable for an enterprise stakeholder deciding whether to adopt or extend this repository.\n\n---\n\n#### 7. Final Output Format (Required)\n\nReturn your answer in **this exact Markdown structure**:",
          "promptTitle": "Tree-of-Thoughts Repository Evaluator for GPT-5.1",
          "type": "how_to"
        },
        "modelResponse": "{\n  \"reasoning\": \"The prompt is comprehensive and well-structured, providing clear instructions for evaluating a GitHub repository using the Tree-of-Thoughts method. It effectively breaks down the evaluation into distinct branches, ensuring thorough coverage of various aspects of prompt engineering. However, it could benefit from more clarity in certain sections and minor adjustments for specificity.\",\n  \"scores\": {\n    \"clarity\": 8,\n    \"specificity\": 7,\n    \"actionability\": 8,\n    \"structure\": 9,\n    \"completeness\": 9,\n    \"factuality\": 9,\n    \"consistency\": 8,\n    \"safety\": 9\n  },\n  \"overall_score\": 8.14,\n  \"grade\": \"B\",\n  \"pass\": true,\n  \"strengths\": [\n    \"Well-structured with clear sections for evaluation.\",\n    \"Comprehensive coverage of various evaluation aspects.\"\n  ],\n  \"improvements\": [\n    \"Enhance clarity in instructions by providing examples for each section.\",\n    \"Add more specific criteria for scoring to ensure consistency across evaluations.\"\n  ],\n  \"summary\": \"The prompt is a strong tool for evaluating a prompt engineering repository, with a clear structure and comprehensive coverage. Minor improvements in clarity and specificity could enhance its effectiveness.\"\n}",
        "evaluationResults": [
          {
            "evaluatorName": "valid-json",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"scores\"'"
          },
          {
            "evaluatorName": "has-overall-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"overall_score\"'"
          },
          {
            "evaluatorName": "has-grade",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"grade\"'"
          },
          {
            "evaluatorName": "has-pass-fail",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"pass\"'"
          },
          {
            "evaluatorName": "has-reasoning",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"reasoning\"'"
          },
          {
            "evaluatorName": "has-safety-score",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"safety\"'"
          },
          {
            "evaluatorName": "has-summary",
            "score": 1,
            "passed": true,
            "details": "Expected to contain: '\"summary\"'"
          }
        ]
      }
    ],
    "summary": {
      "totalTests": 3,
      "passedTests": 3,
      "failedTests": 0,
      "passRate": 100
    },
    "run_number": 3,
    "eval_file": "system-eval-3.prompt.yml"
  }
]