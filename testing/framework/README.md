# üß™ Testing Framework Core

Core testing infrastructure for prompt validation and evaluation across the entire library.

## üìã Overview

This directory contains the universal testing framework that powers all prompt validation, integration testing, and evaluation workflows. It provides a unified interface for running tests against prompts, agents, and multimodal content.

## üìÅ Structure

```
framework/
‚îú‚îÄ‚îÄ README.md           # This file
‚îú‚îÄ‚îÄ core/               # Core test runner and orchestration
‚îî‚îÄ‚îÄ validators/         # Validation framework components
```

## üéØ Purpose

The testing framework provides:

- **Universal test runner** - Execute any test suite with parallel/sequential execution
- **Multi-provider support** - Azure, GitHub Models, Ollama, local ONNX
- **Test orchestration** - Manage test suites, retries, timeouts, and caching
- **Result aggregation** - Collect metrics, validation results, and performance data
- **Provider auto-detection** - Automatically select available LLM provider
- **Validation pipeline** - Structured output validation with multiple validators

## üöÄ Quick Start

### Run Test Suites

```bash
# Run a single test suite
python testing/run_tests.py test_suites/example_test_suite.yaml

# Run multiple suites
python testing/run_tests.py suite1.yaml suite2.yaml

# Run with specific settings
python testing/run_tests.py test_suite.yaml \
  --parallel \
  --max-workers 5 \
  --filter-tags unit integration \
  --output results.json
```

### Create a Test Suite

```yaml
# test_suite.yaml
name: My Test Suite
description: Tests for my prompts
test_cases:
  - id: test_001
    name: Test basic prompt
    description: Verify basic prompt execution
    test_type: unit
    prompt_id: my_prompt
    inputs:
      user_input: "Hello world"
    expected_outputs:
      contains: "greeting"
    validators:
      - semantic
      - format
    metrics:
      - response_relevance
    timeout: 30
    retries: 3
    tags:
      - quick
      - smoke
```

### Programmatic Usage

```python
import asyncio
from framework.core.test_runner import PromptTestRunner, TestCase, TestType

async def run_tests():
    runner = PromptTestRunner()
    
    # Create test case
    test_case = TestCase(
        id="test_001",
        name="Test Prompt",
        description="Test a simple prompt",
        test_type=TestType.UNIT,
        prompt_id="basic_prompt",
        inputs={"input": "Hello, world!"},
        validators=["semantic"],
        timeout=10
    )
    
    # Run test
    result = await runner.run_single_test(test_case)
    print(f"Status: {result.status.value}")
    print(f"Output: {result.actual_output}")
    print(f"Metrics: {result.metrics}")

asyncio.run(run_tests())
```

## üîß Features

### Test Types

| Type | Purpose | Use Cases |
|------|---------|-----------|
| `unit` | Single prompt validation | Individual prompt testing |
| `integration` | Multi-component tests | End-to-end workflows |
| `regression` | Prevent regressions | CI/CD validation |
| `performance` | Speed/cost analysis | Optimization testing |
| `safety` | Security testing | Harmful content detection |
| `quality` | Output quality | Score validation |
| `benchmark` | Comparative testing | Model comparison |

### LLM Provider Support

The framework auto-detects available providers in priority order:

1. **Azure Foundry** - If `AZURE_FOUNDRY_API_KEY` and endpoints configured
2. **Local ONNX** - If `tools/local_model.py` available
3. **GitHub Models** - If `gh` CLI with models extension installed
4. **Ollama** - If Ollama server running on localhost:11434

### Execution Modes

```bash
# Parallel execution (default)
python testing/run_tests.py suite.yaml --parallel --max-workers 10

# Sequential execution
python testing/run_tests.py suite.yaml --max-workers 1

# Filter by tags
python testing/run_tests.py suite.yaml --filter-tags smoke quick

# Filter by type
python testing/run_tests.py suite.yaml --filter-type unit

# Dry run (validate without executing)
python testing/run_tests.py suite.yaml --dry-run
```

### Retry Logic

Tests automatically retry on failure with exponential backoff:

```yaml
test_cases:
  - id: test_flaky
    retries: 3  # Will retry up to 3 times
    timeout: 30  # 30 second timeout per attempt
```

### Caching

Results are automatically cached based on prompt ID and inputs. Cache is bypassed for performance tests.

## üìä Test Results

### Console Output

```
üß™ Running Test Suite: example_suite
==================================================
‚úÖ Test basic prompt: passed (1.23s)
‚úÖ Test complex workflow: passed (3.45s)
‚ùå Test edge case: failed (0.89s)
   Error: Output validation failed

==================================================
TEST EXECUTION SUMMARY
==================================================
üìä Suite: example_suite
   Total Tests: 3
   ‚úÖ Passed: 2
   ‚ùå Failed: 1
   üí• Errors: 0
   Pass Rate: 66.67%
   Execution Time: 5.57s
   Total Cost: $0.0023
```

### JSON Output

```json
{
  "timestamp": "2025-12-04T10:30:00",
  "summary": {
    "suite_name": "example_suite",
    "total_tests": 3,
    "passed": 2,
    "failed": 1,
    "pass_rate": "66.67%",
    "execution_time": "5.57s",
    "total_cost": "$0.0023"
  },
  "metrics": {
    "total_tokens": 1234,
    "avg_execution_time": "1.86s"
  },
  "details": [...]
}
```

## üîç Validation

The framework integrates with validators in `framework/validators/`:

```python
# Available validators
validators:
  - json          # JSON structure validation
  - code_python   # Python code validation
  - code_javascript  # JavaScript code validation
  - semantic      # Semantic similarity
  - safety        # Safety checks
  - performance   # Performance metrics
```

See [validators/README.md](validators/README.md) for details.

## üìà Metrics Collection

Automatically collected metrics:

| Metric | Description |
|--------|-------------|
| `execution_time` | Test execution duration (seconds) |
| `total_tokens` | Combined prompt + completion tokens |
| `prompt_tokens` | Input tokens |
| `completion_tokens` | Output tokens |
| `estimated_cost` | Estimated API cost ($) |
| `output_length` | Response character count |
| `output_lines` | Response line count |

Custom metrics can be added via the metrics collector.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         TestOrchestrator                    ‚îÇ
‚îÇ  (run_tests.py)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚îú‚îÄ‚îÄ> Load Test Suites (YAML/JSON)
               ‚îÇ
               ‚îú‚îÄ‚îÄ> PromptTestRunner
               ‚îÇ    ‚îú‚îÄ> Provider Auto-detection
               ‚îÇ    ‚îú‚îÄ> Test Execution (parallel/sequential)
               ‚îÇ    ‚îú‚îÄ> Retry Logic & Timeouts
               ‚îÇ    ‚îú‚îÄ> Result Caching
               ‚îÇ    ‚îî‚îÄ> Validation Pipeline
               ‚îÇ
               ‚îú‚îÄ‚îÄ> Validators (framework/validators/)
               ‚îÇ    ‚îú‚îÄ> JSON Validator
               ‚îÇ    ‚îú‚îÄ> Code Validator
               ‚îÇ    ‚îú‚îÄ> Semantic Validator
               ‚îÇ    ‚îî‚îÄ> Safety Validator
               ‚îÇ
               ‚îî‚îÄ‚îÄ> Results
                    ‚îú‚îÄ> Console Summary
                    ‚îú‚îÄ> JSON Report
                    ‚îî‚îÄ> Log Files
```

## üîó Integration Points

### With Validators

```python
# Register custom validator
from framework.validators.base_validator import BaseValidator

class MyValidator(BaseValidator):
    async def validate(self, output, expected):
        # Custom validation logic
        return True

runner.validators['my_validator'] = MyValidator()
```

### With CI/CD

```yaml
# .github/workflows/test.yml
- name: Run test suite
  run: |
    python testing/run_tests.py test_suite.yaml \
      --output results.json
  
- name: Check results
  run: |
    python -c "
    import json
    with open('results.json') as f:
        results = json.load(f)
    exit(0 if results['summary']['failed'] == 0 else 1)
    "
```

## üì¶ Dependencies

```bash
# Required
pip install pyyaml pytest

# Optional (for specific providers)
pip install onnxruntime-genai  # Local ONNX models
gh extension install github/gh-models  # GitHub Models
```

## üêõ Troubleshooting

### No Provider Available

```bash
# Check available providers
python -c "
from framework.core.test_runner import PromptTestRunner
runner = PromptTestRunner()
print(f'Provider: {runner._detect_provider()}')
"
```

### Timeout Issues

Increase timeout values in test suite:

```yaml
test_cases:
  - timeout: 60  # Increase to 60 seconds
```

### Rate Limiting

Use sequential execution to avoid rate limits:

```bash
python testing/run_tests.py suite.yaml --max-workers 1
```

## üìñ See Also

- [core/README.md](core/README.md) - Test runner implementation details
- [validators/README.md](validators/README.md) - Validation framework
- [../integration/README.md](../integration/README.md) - Integration tests
- [../unit/README.md](../unit/README.md) - Unit tests
- [../../docs/ARCHITECTURE_PLAN.md](../../docs/ARCHITECTURE_PLAN.md) - Overall architecture

---

**Built with ‚ù§Ô∏è for reliable prompt testing**
