<![CDATA[<div align="center">

# Agentic Workflows

**A production-grade multi-agent AI workflow engine with DAG-based execution, tiered model routing, YAML-defined workflows, and rubric-based LLM evaluation.**

[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Linting: ruff](https://img.shields.io/badge/linting-ruff-261230.svg)](https://github.com/astral-sh/ruff)
[![Type checked: mypy](https://img.shields.io/badge/type%20checked-mypy-blue.svg)](http://mypy-lang.org/)

</div>

---

## Overview

Agentic Workflows is a framework for orchestrating multi-agent AI pipelines where each agent occupies a specific role (planner, researcher, coder, reviewer) and operates at a defined capability tier. Workflows are authored as declarative YAML files and compiled into executable DAGs with automatic parallel scheduling, conditional branching, iterative loops, and failure cascade propagation.

The system solves three core problems in production AI orchestration:

1. **Scheduling complexity** â€” Real-world agent pipelines have diamond dependencies, conditional execution, and iterative refinement loops. A linear pipeline can't express "run two analysts in parallel, then merge results only if a confidence threshold is met." A DAG with Kahn's algorithm can.

2. **Model reliability** â€” Any single LLM endpoint can rate-limit, timeout, or fail. The tiered routing system maps each workflow step to a capability tier (not a specific model), then resolves to the best available model at runtime with health-weighted selection, adaptive cooldowns, and circuit breaker patterns.

3. **Evaluation rigor** â€” "Does the output look good?" isn't a scoring methodology. The evaluation framework uses YAML-defined rubrics with weighted criteria, multidimensional scoring across coverage/quality/agreement/verification/recency dimensions, and LLM-as-judge integration for subjective quality assessment.

---

## Architecture

```mermaid
graph TD
    A["ğŸ“„ YAML Workflow DSL<br/><i>deep_research Â· code_review Â· tdd_codegen</i>"] --> B["ğŸ” Workflow Loader & Validator<br/><i>Expression resolution Â· Input typing</i>"]

    B --> C["âš¡ DAG Executor<br/><i>Native Runtime</i>"]
    B --> D["ğŸ”— LangGraph Compiler<br/><i>LangChain Engine</i>"]

    C --> E["ğŸ”€ Tiered Model Router"]
    D --> E

    E --> F["â˜ï¸ Provider Backends"]

    subgraph DAG ["DAG Executor"]
        C
        C1["Kahn's algorithm scheduling"]
        C2["asyncio.wait FIRST_COMPLETED"]
        C3["Cascade failure propagation"]
        C4["Max concurrency control"]
    end

    subgraph LG ["LangGraph Compiler"]
        D
        D1["StateGraph node generation"]
        D2["Conditional + loop edges"]
        D3["Checkpointing support"]
    end

    subgraph Router ["Model Router"]
        E
        E1["Tier 0 â†’ Deterministic Â· no LLM"]
        E2["Tier 1 â†’ Small Â· flash-lite, 4o-mini"]
        E3["Tier 2 â†’ Medium Â· flash, haiku"]
        E4["Tier 3 â†’ Large Â· 2.5-flash, gpt-4o"]
        E5["Tier 4 â†’ Premium Â· 2.5-pro, claude-sonnet"]
        E6["Tier 5 â†’ Frontier Â· claude-opus"]
        E7["Health-weighted selection Â· Adaptive cooldowns<br/>Circuit breakers Â· Cost-aware routing"]
    end

    subgraph Providers ["Providers"]
        F
        F1["OpenAI"]
        F2["Anthropic"]
        F3["Google Gemini"]
        F4["Ollama / LM Studio"]
        F5["Azure OpenAI"]
        F6["GitHub Models"]
    end

    style A fill:#4a90d9,stroke:#2c5f8a,color:#fff
    style B fill:#6c5ce7,stroke:#4834a8,color:#fff
    style E fill:#e17055,stroke:#b34530,color:#fff
    style F fill:#00b894,stroke:#008060,color:#fff
    style C fill:#fdcb6e,stroke:#c8a034,color:#333
    style D fill:#fdcb6e,stroke:#c8a034,color:#333
```

---

## Key Design Decisions

### Why DAG over Pipeline?

Multi-agent workflows rarely execute linearly. Consider the **deep research** workflow: after an initial planning step, _two specialist analysts_ (AI-focused and SWE-focused) run in parallel over the same evidence bundle. Their outputs merge into a Chain-of-Verification (CoVe) step, which conditionally triggers another research round based on confidence scoring. A pipeline would serialize the analysts unnecessarily; a DAG with `asyncio.wait(FIRST_COMPLETED)` maximizes throughput.

### Why YAML Workflow Definitions?

Workflow structure should be auditable and version-controlled separately from execution logic. Each YAML file declares steps, their agent assignments, input/output mappings with expression syntax (`${steps.intake_scope.outputs.scoped_goal}`), conditional gates (`when:`), and loop termination conditions (`loop_until:`). The runtime compiles these into executable graphs â€” the YAML is the single source of truth for workflow topology.

### Why Tiered Model Routing?

Mapping workflow steps to models by name creates brittleness: model names change, endpoints go down, pricing shifts. Instead, each agent is assigned a _capability tier_ (e.g., `tier3_analyst`). The router resolves this to the best available model at runtime, with fallback chains like:

```
Tier 3: gemini-2.5-flash â†’ gh:gpt-4o â†’ openai:gpt-4o â†’ anthropic:claude-sonnet
```

The `SmartModelRouter` extends this with health-weighted selection, adaptive cooldowns (exponential backoff on failures), and circuit breaker patterns â€” if a model fails repeatedly, it's temporarily excluded from selection.

### Why Rubric-Based Scoring?

LLM outputs resist binary pass/fail evaluation. The scoring system uses YAML-defined rubrics with weighted criteria, score normalization with clamping, and explicit handling of missing criteria. For complex evaluations, a multidimensional scoring engine classifies outputs across five orthogonal dimensions (coverage, source quality, agreement, verification, recency) into lettered tiers (S/A/B/C/D/F), with configurable pass/fail gates per dimension.

---

## Workflow Definitions

The engine ships with **10 production workflow definitions**, each solving a distinct orchestration pattern:

| Workflow | Steps | Pattern | Description |
|----------|-------|---------|-------------|
| `deep_research` | 28 | Iterative DAG with conditional rounds | ToT planning â†’ ReAct retrieval â†’ dual-specialist analysis â†’ CoVe verification â†’ confidence gating â†’ up to 4 iterative rounds â†’ final synthesis + RAG packaging |
| `code_review` | 5 | Fan-out / fan-in | Parse code â†’ parallel architecture + quality reviews â†’ synthesis |
| `bug_resolution` | 6 | Sequential with verification | Reproduce â†’ root cause analysis â†’ fix â†’ test â†’ verify |
| `tdd_codegen_e2e` | 27 | Multi-loop with bounded iteration | Spec â†’ test scaffolding â†’ implementation â†’ iterative test/fix cycles |
| `multi_agent_codegen_e2e` | 32 | Full pipeline with review loops | Plan â†’ architect â†’ implement â†’ review â†’ iterate until quality gates pass |
| `fullstack_generation` | 7 | Sequential with parallel sub-steps | Requirements â†’ API design â†’ frontend + backend in parallel â†’ integration |
| `plan_implementation` | 24 | Complex DAG with checkpoints | Breakdown â†’ dependency analysis â†’ phased implementation â†’ validation |
| `test_deterministic` | 1 | Tier-0 only | Deterministic step for testing the engine without LLM calls |

### Example: Deep Research Workflow

The `deep_research.yaml` workflow demonstrates the full power of the DSL â€” iterative research with bounded rounds, conditional execution, parallel specialist analysis, and confidence-gated termination:

```yaml
steps:
  - name: intake_scope
    agent: tier3_planner
    description: Clarify user objective, constraints, and success criteria
    tools: [context_store]
    inputs:
      topic: ${inputs.topic}
      goal: ${coalesce(inputs.goal, inputs.topic)}
    outputs:
      scoped_goal: scoped_goal

  - name: analyst_ai_round1         # â† Runs in parallel
    agent: tier3_analyst             #   with analyst_swe_round1
    depends_on: [retrieval_react_round1]
    ...

  - name: analyst_swe_round1        # â† Runs in parallel
    agent: tier3_analyst             #   with analyst_ai_round1
    depends_on: [retrieval_react_round1]
    ...

  - name: hypothesis_tree_tot_round2
    agent: tier3_reasoner
    depends_on: [coverage_confidence_audit_round1]
    when: ${inputs.max_rounds} >= 2 and not ${steps...gate_passed}
    # â†‘ Conditional: only runs if confidence gate hasn't passed
```

#### Execution Flow (Single Research Round)

```mermaid
graph LR
    A["ğŸ¯ intake_scope<br/><i>tier3_planner</i>"] --> B["ğŸ“‹ source_policy<br/><i>tier2_researcher</i>"]
    B --> C["ğŸŒ³ hypothesis_tree<br/><i>tier3_reasoner</i>"]
    C --> D["ğŸ” retrieval_react<br/><i>tier2_researcher</i>"]

    D --> E["ğŸ¤– analyst_ai<br/><i>tier3_analyst</i>"]
    D --> F["ğŸ’» analyst_swe<br/><i>tier3_analyst</i>"]

    E --> G["âœ… cove_verify<br/><i>tier3_reviewer</i>"]
    F --> G

    G --> H{"ğŸ“Š confidence<br/>gate passed?"}

    H -- "No" --> C2["ğŸŒ³ next round<br/><i>refine hypotheses</i>"]
    H -- "Yes" --> I["ğŸ“ final_synthesis<br/><i>tier4_writer</i>"]
    I --> J["ğŸ“¦ rag_package<br/><i>tier2_assembler</i>"]

    style A fill:#4a90d9,stroke:#2c5f8a,color:#fff
    style E fill:#e17055,stroke:#b34530,color:#fff
    style F fill:#e17055,stroke:#b34530,color:#fff
    style G fill:#6c5ce7,stroke:#4834a8,color:#fff
    style H fill:#fdcb6e,stroke:#c8a034,color:#333
    style I fill:#00b894,stroke:#008060,color:#fff
    style C2 fill:#fd79a8,stroke:#c0392b,color:#fff
```

---

## Project Structure

```
agentic-workflows/
â”œâ”€â”€ agentic-workflows-v2/          # Core runtime package
â”‚   â”œâ”€â”€ agentic_v2/
â”‚   â”‚   â”œâ”€â”€ engine/                # DAG executor, step runner, expression engine
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_executor.py    # Kahn's algorithm with asyncio scheduling
â”‚   â”‚   â”‚   â”œâ”€â”€ dag.py             # DAG data structure with cycle detection
â”‚   â”‚   â”‚   â”œâ”€â”€ expressions.py     # ${...} expression resolver
â”‚   â”‚   â”‚   â”œâ”€â”€ runtime.py         # Subprocess & Docker isolated execution
â”‚   â”‚   â”‚   â””â”€â”€ step.py            # Step execution with should_run() gates
â”‚   â”‚   â”œâ”€â”€ models/                # Model routing and provider abstraction
â”‚   â”‚   â”‚   â”œâ”€â”€ router.py          # Tier-based routing with DSL chain builder
â”‚   â”‚   â”‚   â”œâ”€â”€ smart_router.py    # Health-weighted selection, circuit breakers
â”‚   â”‚   â”‚   â”œâ”€â”€ backends.py        # Provider adapters (OpenAI, Anthropic, Gemini, etc.)
â”‚   â”‚   â”‚   â””â”€â”€ model_stats.py     # Performance tracking and persistence
â”‚   â”‚   â”œâ”€â”€ agents/                # Agent implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ orchestrator.py    # Capability-based agent matching + DAG scheduling
â”‚   â”‚   â”‚   â”œâ”€â”€ architect.py       # System design agent
â”‚   â”‚   â”‚   â”œâ”€â”€ coder.py           # Code generation agent
â”‚   â”‚   â”‚   â””â”€â”€ reviewer.py        # Code review agent
â”‚   â”‚   â”œâ”€â”€ workflows/             # Workflow runtime
â”‚   â”‚   â”‚   â”œâ”€â”€ definitions/       # 10 YAML workflow definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py          # YAML â†’ WorkflowConfig parser
â”‚   â”‚   â”‚   â””â”€â”€ runner.py          # Workflow execution orchestrator
â”‚   â”‚   â”œâ”€â”€ langchain/             # LangGraph integration
â”‚   â”‚   â”‚   â””â”€â”€ graph.py           # YAML â†’ LangGraph StateGraph compiler
â”‚   â”‚   â”œâ”€â”€ server/                # FastAPI backend
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py             # Application factory
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation.py      # Evaluation endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ judge.py           # LLM-as-judge integration
â”‚   â”‚   â”‚   â”œâ”€â”€ websocket.py       # Live SSE/WebSocket streaming
â”‚   â”‚   â”‚   â””â”€â”€ scoring_profiles.py
â”‚   â”‚   â”œâ”€â”€ contracts/             # Typed data contracts (Pydantic + dataclasses)
â”‚   â”‚   â”œâ”€â”€ prompts/               # Agent system prompts (Markdown)
â”‚   â”‚   â”œâ”€â”€ tools/                 # Built-in agent tools
â”‚   â”‚   â””â”€â”€ cli/                   # `agentic` CLI entry point
â”‚   â”œâ”€â”€ ui/                        # React dashboard (Vite + TypeScript)
â”‚   â”œâ”€â”€ tests/                     # 36 test files
â”‚   â””â”€â”€ pyproject.toml             # hatchling build, optional dep groups
â”‚
â”œâ”€â”€ agentic-v2-eval/               # Evaluation framework package
â”‚   â”œâ”€â”€ src/agentic_v2_eval/
â”‚   â”‚   â”œâ”€â”€ datasets/              # Dataset adapters (SWE-bench, etc.)
â”‚   â”‚   â”œâ”€â”€ evaluators/            # Rubric-based evaluators
â”‚   â”‚   â”œâ”€â”€ reporters/             # Result reporting
â”‚   â”‚   â””â”€â”€ scoring/               # Scoring utilities
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ tools/                         # Shared utilities
â”‚   â”œâ”€â”€ llm/                       # Provider adapters, probes, ranking
â”‚   â””â”€â”€ core/                      # Config, errors, caching, helpers
â”‚
â””â”€â”€ .github/                       # CI/CD and GitHub configuration
```

---

## Technical Highlights

### DAG Executor â€” Concurrent Scheduling Engine

The heart of the runtime is `dag_executor.py`, implementing Kahn's algorithm with `asyncio.wait(FIRST_COMPLETED)` for true parallel scheduling:

- **Dynamic scheduling**: Steps are dispatched as soon as all dependencies resolve â€” no artificial batching
- **Max concurrency control**: Configurable limit prevents resource exhaustion on large DAGs
- **Cascade failure propagation**: When a step fails, all transitively dependent steps are automatically skipped via BFS traversal
- **Deadlock detection**: Unreachable steps (unmet dependencies after all runnable steps complete) are detected and force-skipped
- **Observable execution**: Every step transition emits structured events via an `on_update` callback for SSE/WebSocket streaming to the React UI

### Smart Model Router â€” Resilient Model Selection

The `SmartModelRouter` goes beyond simple fallback chains:

- **Health-weighted selection**: Models with better track records are preferred, scored by `success_rate Ã— (1 / avg_latency)`
- **Adaptive cooldowns**: Cooldown duration scales with consecutive failures via exponential backoff (`base Ã— multiplier^failures`)
- **Circuit breaker pattern**: Models in `OPEN` state are excluded from selection until a configurable number of probe successes
- **Cost-aware routing**: Optional `max_cost` filter excludes models above a cost-per-token threshold
- **Stats persistence**: Performance data survives process restarts via atomic JSON serialization

### LangGraph Compiler â€” YAML to Executable Graph

`graph.py` compiles YAML workflow configs into LangGraph `StateGraph` objects:

- One node per step, with tier-appropriate agent construction
- Conditional edges from `when:` expressions (steps self-skip when conditions aren't met, preserving graph connectivity)
- Loop edges from `loop_until:` expressions with iteration counting and max bounds
- Multi-model failover: each LLM node tries candidates in order, catching retryable errors and advancing to the next model
- Full token tracking via provider-specific metadata extraction (OpenAI, Anthropic, Gemini)

### Evaluation Framework â€” Rubric-Based Scoring

The evaluation system supports multiple scoring profiles:

- **Weighted criteria scoring**: Each rubric criterion has a weight; final scores are normalized with clamping and missing-criteria awareness
- **Multidimensional scoring**: DORA-style classification across 5 dimensions with S/A/B/C/D/F tiers and configurable gate logic
- **LLM-as-judge**: Integrates an LLM evaluator for subjective quality assessment with structured rubric prompts
- **Scoring profiles**: Switchable configurations (Profile Aâ€“E) for different evaluation contexts

---

## Quick Start

### Prerequisites

- Python 3.11+
- At least one LLM provider configured (see `.env.example` for supported providers)

### Installation

```bash
# Clone the repository
git clone https://github.com/tafreeman/prompts.git
cd prompts

# Install the core runtime with development + server + LangChain dependencies
cd agentic-workflows-v2
pip install -e ".[dev,server,langchain]"

# Configure your environment
cp ../.env.example ../.env
# Edit .env with your API keys
```

### Running a Workflow

```bash
# List available workflows
agentic list workflows

# Run the deep research workflow
agentic run deep_research --input topic="agentic AI architectures"

# Run with dry-run (validates DAG without executing)
agentic run code_review --dry-run
```

### Starting the Dashboard

```bash
# Start the FastAPI backend
cd agentic-workflows-v2
uvicorn agentic_v2.server.app:create_app --factory --reload --port 8000

# Start the React UI (in another terminal)
cd agentic-workflows-v2/ui
npm install && npm run dev
```

### Running Tests

```bash
# Runtime tests (36 test files)
cd agentic-workflows-v2
pytest tests/ -v --cov=agentic_v2

# Evaluation framework tests
cd agentic-v2-eval
pytest tests/ -v
```

---

## Development

### Code Quality Toolchain

This project enforces code quality through pre-commit hooks:

| Tool | Purpose |
|------|---------|
| **black** | Code formatting (88 char line length) |
| **isort** | Import sorting (black-compatible profile) |
| **ruff** | Fast linting with auto-fix |
| **mypy** | Static type checking |
| **docformatter** | Docstring formatting |
| **pydocstyle** | Google-style docstring enforcement |

```bash
# Install pre-commit hooks
pre-commit install

# Run all hooks manually
pre-commit run --all-files
```

### Code Style

- **Type annotations throughout** â€” `dict[str, float]`, `Optional[str]`, `list[dict[str, Any]]` â€” consistent Python 3.11+ generics
- **Dataclasses** â€” frozen where appropriate, field factories, clean defaults
- **Custom exception hierarchy** â€” `MissingDependencyError`, `CycleDetectedError` with structured attributes
- **Google-style docstrings** â€” Args/Returns/Raises sections across all public APIs
- **Async-first** â€” `pytest-asyncio` for async test infrastructure

### Optional Dependency Groups

```toml
[project.optional-dependencies]
dev       = ["pytest", "pytest-asyncio", "pytest-cov", "black", "ruff"]
server    = ["fastapi", "uvicorn", "python-multipart"]
langchain = ["langchain", "langchain-openai", "langchain-anthropic", "langchain-google-genai"]
tracing   = ["opentelemetry-sdk", "opentelemetry-exporter-otlp-proto-grpc"]
claude    = ["anthropic", "claude-agent-sdk"]
```

---

## Contributing

See [CONTRIBUTING.md](agentic-workflows-v2/CONTRIBUTING.md) for development guidelines, code review process, and pull request standards.

## Security

See [SECURITY.md](agentic-workflows-v2/SECURITY.md) for vulnerability reporting procedures.

## License

This project is licensed under the MIT License â€” see [LICENSE](agentic-workflows-v2/LICENSE) for details.
]]>
