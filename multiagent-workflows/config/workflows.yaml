# Workflow Definitions for Multi-Agent System
# Each workflow uses core agents from agents.yaml with stage-specific configuration
version: "1.0.0"

workflows:
  # ==========================================================================
  # Workflow 1: Full-Stack Application Generation
  # ==========================================================================
  fullstack_generation:
    name: Full-Stack Application Generation
    description: Convert business requirements and UI mockups into a complete full-stack application
    
    inputs:
      - name: requirements
        type: text
        required: true
        description: Business requirements document
      - name: mockup_path
        type: file
        required: false
        description: Path to UI mockup image(s)
      - name: tech_stack
        type: object
        required: false
        description: Optional tech stack preferences
        
    outputs:
      - name: application
        type: directory
        description: Complete application codebase
      - name: documentation
        type: directory
        description: Generated documentation
      - name: tests
        type: directory
        description: Test suites
        
    steps:
      - id: vision_analysis
        name: Analyze UI Mockups
        agent: vision
        model_preference: vision
        condition: "inputs.mockup_path is not None"
        stage_prompt: |
          Analyze the UI mockup and extract:
          1. All interactive elements (buttons, forms, inputs, links)
          2. Layout structure (header, sidebar, main content, footer)
          3. Color palette (primary, secondary, accent colors)
          4. Component hierarchy and relationships
        outputs: ["ui_components", "layout_structure", "color_palette"]
        
      - id: requirements_parsing
        name: Parse Business Requirements
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Parse the business requirements and extract:
          1. User Stories with acceptance criteria (Given/When/Then)
          2. Data entities and relationships
          3. Business rules and constraints
          4. Non-functional requirements
        inputs: ["inputs.requirements"]
        outputs: ["user_stories", "acceptance_criteria", "data_entities"]
        
      - id: architecture_design
        name: Design System Architecture
        agent: architect
        model_preference: reasoning_complex
        stage_prompt: |
          Design the system architecture including:
          1. Tech stack recommendation with justification
          2. Component diagram (high-level architecture)
          3. API strategy (REST/GraphQL, versioning)
          4. Database choice and data flow
        inputs: ["requirements_parsing.user_stories", "vision_analysis.ui_components"]
        outputs: ["tech_stack", "component_diagram", "api_strategy"]
        
      - id: database_design
        name: Design Database Schema
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Design the database schema:
          1. Entity-relationship diagrams
          2. Table schemas with proper constraints
          3. Index strategy for performance
          4. Migration scripts
        inputs: ["requirements_parsing.data_entities", "architecture_design.tech_stack"]
        outputs: ["schema_sql", "migrations", "er_diagram"]
        
      - id: api_design
        name: Design API Contracts
        agent: coder
        model_preference: code_gen_fast
        stage_prompt: |
          Design the API contracts:
          1. OpenAPI 3.0 specification
          2. Endpoint definitions with HTTP methods
          3. Request/response schemas
          4. Authentication flows
        inputs: ["requirements_parsing.user_stories", "architecture_design.api_strategy"]
        outputs: ["openapi_spec", "endpoint_definitions"]
        
      - id: frontend_generation
        name: Generate Frontend
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Generate the frontend application:
          1. React/Vue components based on UI analysis
          2. Proper component hierarchy
          3. State management
          4. API integration layer
        inputs: ["vision_analysis.ui_components", "api_design.openapi_spec"]
        outputs: ["frontend_code", "component_hierarchy"]
        config:
          language: typescript
          framework: react
          
      - id: backend_generation
        name: Generate Backend
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Generate the backend application:
          1. API endpoints from OpenAPI spec
          2. Database integration
          3. Middleware and validators
          4. Authentication/authorization
        inputs: ["api_design.openapi_spec", "database_design.schema_sql"]
        outputs: ["backend_code", "middleware", "validators"]
        config:
          language: python
          framework: fastapi
          
      - id: integration
        name: Create Integration Code
        agent: coder
        model_preference: code_gen_fast
        stage_prompt: |
          Create integration and deployment code:
          1. Docker Compose configuration
          2. Environment files for different stages
          3. Build and deployment scripts
        inputs: ["frontend_generation.frontend_code", "backend_generation.backend_code"]
        outputs: ["docker_compose", "env_files", "scripts"]
        
      - id: code_review
        name: Review Generated Code
        agent: reviewer
        model_preference: code_review
        stage_prompt: |
          Review all generated code for:
          1. Security vulnerabilities (OWASP Top 10)
          2. Performance issues
          3. Code quality and maintainability
          4. Best practices violations
        inputs: ["frontend_generation.frontend_code", "backend_generation.backend_code"]
        outputs: ["security_findings", "quality_issues", "recommendations"]
        
      - id: test_generation
        name: Generate Tests
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          Generate comprehensive tests:
          1. Unit tests (>80% coverage)
          2. Integration tests for API endpoints
          3. E2E tests for critical flows
        inputs: ["frontend_generation.frontend_code", "backend_generation.backend_code"]
        outputs: ["unit_tests", "integration_tests", "e2e_tests"]
        
      - id: documentation
        name: Generate Documentation
        agent: writer
        model_preference: documentation
        stage_prompt: |
          Generate project documentation:
          1. README with project overview and quick start
          2. API documentation with examples
          3. Setup and deployment guide
        inputs: ["api_design.openapi_spec", "architecture_design.component_diagram"]
        outputs: ["readme", "api_docs", "setup_guide"]

      - id: final_evaluation
        name: Project Judge Evaluation
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate the Full-Stack Application against this STRICT RUBRIC:
          
          1. **Correctness (Weight: 0.4)**
             - Functional correctness of the generated application
             - Does it meet all parsed User Stories?
             
          2. **Quality (Weight: 0.3)**
             - Code quality, readability, and maintainability
             - Adherence to best practices (SOLID, DRY)
             
          3. **Documentation (Weight: 0.2)**
             - Quality and completeness of README and API docs
             - Clarity of setup instructions
             
          4. **Completeness (Weight: 0.1)**
             - Coverage of all specified requirements
             - Presence of tests and CI/CD configs
             
          Synthesize these scores into a final Weighted Grade (0-100).
        inputs: ["backend_generation.backend_code", "frontend_generation.frontend_code", "documentation.readme"]
        outputs: ["final_verdict", "score_breakdown"]

  # ==========================================================================
  # Workflow 2: Code Generation (for benchmarks like HumanEval)
  # ==========================================================================
  code_generation:
    name: Code Generation Pipeline
    description: Generate code from natural language requirements
    
    inputs:
      - name: requirements
        type: text
        required: true
        description: Natural language description of code to generate
      - name: language
        type: string
        required: false
        default: python
        description: Target programming language
      - name: tests
        type: text
        required: false
        description: Test cases to validate against
        
    outputs:
      - name: code
        type: text
        description: Generated source code
      - name: explanation
        type: text
        description: Explanation of the solution
        
    steps:
      - id: requirements_analysis
        name: Analyze Requirements
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Parse the requirements to identify:
          1. Core functionality needed
          2. Input/output specifications with types
          3. Edge cases and constraints
          4. Example input/output pairs
        inputs: ["inputs.requirements"]
        outputs: ["parsed_requirements", "io_examples", "edge_cases"]
        
      - id: solution_design
        name: Design Solution
        agent: architect
        model_preference: reasoning_complex
        stage_prompt: |
          Design the solution approach:
          1. Optimal algorithm/approach
          2. Data structures to use
          3. Time/space complexity analysis
          4. Edge case handling strategy
        inputs: ["requirements_analysis.parsed_requirements", "requirements_analysis.edge_cases"]
        outputs: ["algorithm_approach", "data_structures", "complexity"]
        
      - id: code_generation
        name: Generate Code
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Generate clean, working code:
          1. Follow the designed algorithm
          2. Include proper type hints
          3. Handle all edge cases
          4. Add inline comments for clarity
        inputs: ["solution_design.algorithm_approach", "inputs.language"]
        outputs: ["generated_code", "function_signature"]
        
      - id: code_review
        name: Review Code
        agent: reviewer
        model_preference: code_review
        stage_prompt: |
          Review the generated code for:
          1. Correctness - does it solve the problem?
          2. Edge cases - are all handled?
          3. Efficiency - any performance issues?
          4. Style - follows language conventions?
        inputs: ["code_generation.generated_code", "requirements_analysis.edge_cases"]
        outputs: ["review_feedback", "improvements"]
        
      - id: code_refinement
        name: Refine Code
        agent: coder
        model_preference: code_gen_premium
        condition: "code_review.improvements is not empty"
        stage_prompt: |
          Refine the code based on review feedback.
          Address all identified issues while preserving correctness.
        inputs: ["code_generation.generated_code", "code_review.improvements"]
        outputs: ["refined_code"]
        
      - id: documentation
        name: Document Code
        agent: writer
        model_preference: documentation
        stage_prompt: |
          Add comprehensive documentation:
          1. Docstring with description, args, returns
          2. Usage example
          3. Algorithm explanation
        inputs: ["code_refinement.refined_code", "solution_design.algorithm_approach"]
        outputs: ["documented_code", "explanation"]

      - id: final_evaluation
        name: Final Code Assessment
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate the Final Code against this STRICT RUBRIC:
          
          1. **Correctness (Weight: 0.5, 0-10 Scale)**
             - 10: Perfect implementation, handles all edge cases
             - 5: Functional but fails some edge cases
             - 0: Incorrect
             
          2. **Quality (Weight: 0.3, 0-10 Scale)**
             - 10: Clean, Pythonic, typed, documented
             - 5: Functional but messy
             - 0: Unreadable
             
          3. **Completeness (Weight: 0.2, 0-10 Scale)**
             - 10: All requirements met + extras
             - 5: Missing minor requirements
             - 0: Missing core requirements
             
          Synthesize into final score (0-100) and verdict.
        inputs: ["documentation.documented_code", "requirements_analysis.parsed_requirements"]
        outputs: ["final_verdict", "score_breakdown"]

  # ==========================================================================
  # Workflow 3: Bug Fixing
  # ==========================================================================
  bug_fixing:
    name: Bug Triage & Automated Fixing
    description: Analyze bug reports, identify root cause, and generate fixes
    
    inputs:
      - name: bug_report
        type: text
        required: true
        description: Bug report content
      - name: codebase_path
        type: directory
        required: true
        description: Path to affected codebase
        
    outputs:
      - name: fix_patch
        type: file
        description: Generated fix as a patch
      - name: regression_tests
        type: directory
        description: Tests to prevent regression
        
    steps:
      - id: bug_analysis
        name: Parse Bug Report
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Extract from the bug report:
          1. Summary of the issue
          2. Steps to reproduce
          3. Expected vs actual behavior
          4. Affected areas of code
        inputs: ["inputs.bug_report"]
        outputs: ["structured_bug", "affected_areas", "severity"]
        
      - id: root_cause_analysis
        name: Identify Root Cause
        agent: reasoner
        model_preference: reasoning_complex
        stage_prompt: |
          Analyze the bug to find root cause:
          1. Build a causal chain from symptom to root
          2. Consider multiple possible causes
          3. Identify the fundamental issue
          4. Explain why this is the root cause
        inputs: ["bug_analysis.structured_bug", "inputs.codebase_path"]
        outputs: ["root_cause", "causal_chain"]
        
      - id: fix_generation
        name: Generate Fix
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Generate a fix for the root cause:
          1. Minimal change to fix the issue
          2. Handle related edge cases
          3. Preserve existing behavior
        inputs: ["root_cause_analysis.root_cause", "inputs.codebase_path"]
        outputs: ["fix_code", "changed_files"]
        
      - id: fix_review
        name: Review Fix
        agent: reviewer
        model_preference: code_review
        stage_prompt: |
          Review the proposed fix:
          1. Does it address the root cause?
          2. Could it introduce regressions?
          3. Are there any side effects?
        inputs: ["fix_generation.fix_code"]
        outputs: ["review_result", "concerns"]
        
      - id: test_generation
        name: Generate Regression Tests
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          Generate tests to prevent regression:
          1. Test that reproduces the original bug
          2. Tests for related edge cases
          3. Verify the fix works correctly
        inputs: ["bug_analysis.structured_bug", "fix_generation.fix_code"]
      - id: final_evaluation
        name: Bug Fix Assessment
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate the Bug Fix against this STRICT RUBRIC:
          
          1. **Resolution (Weight: 0.5, 0-10 Scale)**
             - 10: Fixes bug + root cause + regression test + zero side effects
             - 8: Fixes bug + root cause + basic test
             - 6: Fixes symptom but not root cause
             - 2: Regresses
             
          2. **Quality (Weight: 0.3, 0-10 Scale)**
             - 10: Elegant, matches style perfectly
             - 5: Functional but messy
             - 0: Breaks build/style
             
          3. **Analysis (Weight: 0.2, 0-10 Scale)**
             - 10: Correct root cause detailed analysis
             - 5: Vague analysis
             - 0: No analysis
             
          Synthesize into final score (0-100) and verdict.
        inputs: ["fix_generation.fix_code", "root_cause_analysis.root_cause", "test_generation.regression_tests"]
        outputs: ["final_verdict", "score_breakdown"]

  # ==========================================================================
  # Workflow 4: Legacy Code Refactoring
  # ==========================================================================
  legacy_refactoring:
    name: Legacy Code Refactoring
    description: Analyze and refactor legacy code to modern patterns
    
    inputs:
      - name: codebase_path
        type: directory
        required: true
        description: Path to legacy codebase
      - name: target_framework
        type: string
        required: false
        description: Target modern framework
        
    outputs:
      - name: refactored_code
        type: directory
        description: Modernized codebase
      - name: migration_plan
        type: file
        description: Step-by-step migration guide
        
    steps:
      - id: code_archaeology
        name: Analyze Legacy Structure
        agent: analyst
        model_preference: reasoning_complex
        stage_prompt: |
          Analyze the legacy codebase to identify:
          1. Overall architecture and structure
          2. Core components and responsibilities
          3. Implicit conventions and patterns
          4. Technical debt indicators
        inputs: ["inputs.codebase_path"]
        outputs: ["structure_analysis", "component_inventory"]
        
      - id: pattern_detection
        name: Detect Anti-Patterns
        agent: reviewer
        model_preference: reasoning_complex
        stage_prompt: |
          Identify anti-patterns and code smells:
          1. Design anti-patterns (God class, spaghetti, etc.)
          2. Performance anti-patterns
          3. Security anti-patterns
          4. Maintenance issues
        inputs: ["code_archaeology.structure_analysis"]
        outputs: ["anti_patterns", "prioritized_issues"]
        
      - id: refactoring_plan
        name: Create Refactoring Plan
        agent: planner
        model_preference: code_gen_premium
        stage_prompt: |
          Create a prioritized refactoring plan:
          1. Prioritized list of refactorings
          2. Dependencies between changes
          3. Risk assessment for each
          4. Rollback strategies
        inputs: ["pattern_detection.prioritized_issues"]
        outputs: ["refactoring_roadmap", "risk_assessment"]
        
      - id: safety_tests
        name: Generate Characterization Tests
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          Generate characterization tests to capture current behavior:
          1. Tests for public interfaces
          2. Edge case tests
          3. Integration tests
        inputs: ["code_archaeology.component_inventory"]
        outputs: ["characterization_tests"]
        
      - id: refactoring
        name: Apply Refactorings
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Apply the planned refactorings:
          1. One refactoring at a time
          2. Preserve external behavior
          3. Document each change
        inputs: ["refactoring_plan.refactoring_roadmap"]
        outputs: ["refactored_code", "change_log"]
        
      - id: validation
        name: Validate Behavior
        agent: validator
        model_preference: code_review
        stage_prompt: |
          Validate that behavior is preserved:
          1. Run characterization tests
          2. Compare input/output behaviors
          3. Check edge case handling
        inputs: ["safety_tests.characterization_tests", "refactoring.refactored_code"]
      - id: final_evaluation
        name: Refactoring Assessment
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate the Refactoring against this STRICT RUBRIC:
          
          1. **Equivalence (Weight: 0.4, 0-10 Scale)**
             - 10: 100% behavioral match, strict inputs
             - 5: Core behavior matches, edge case drift
             - 0: Different behavior
             
          2. **Modernization (Weight: 0.3, 0-10 Scale)**
             - 10: Fully idiomatic modern patterns
             - 5: Superficial changes
             - 0: No modernization
             
          3. **Cleanliness (Weight: 0.3, 0-10 Scale)**
             - 10: Crystal clear logic, perfect naming
             - 5: Better but still complex
             - 0: Obfuscated
             
          Synthesize into final score (0-100) and verdict.
        inputs: ["refactoring.refactored_code", "validation.validation_report"]
        outputs: ["final_verdict", "score_breakdown"]

  # ==========================================================================
  # Workflow 5: Code Quality Grading
  # ==========================================================================
  code_grading:
    name: Code Quality Assessment
    description: Multi-dimensional code quality evaluation
    
    inputs:
      - name: code_submission
        type: text
        required: true
        description: Code to evaluate
      - name: language
        type: string
        required: false
        description: Programming language
        
    outputs:
      - name: grade_report
        type: file
        description: Detailed grading report
      - name: overall_score
        type: number
        description: Weighted score (0-100)
        
    steps:
      - id: static_analysis
        name: Static Analysis
        agent: reviewer
        model_preference: code_gen_fast
        stage_prompt: |
          Perform static analysis:
          1. Linting issues
          2. Cyclomatic complexity
          3. Code duplication
          4. Naming conventions
        inputs: ["inputs.code_submission", "inputs.language"]
        outputs: ["lint_results", "complexity_metrics"]
        
      - id: security_audit
        name: Security Audit
        agent: reviewer
        model_preference: code_review
        stage_prompt: |
          Audit for security issues:
          1. OWASP Top 10 vulnerabilities
          2. Input validation gaps
          3. Authentication/authorization issues
          4. Sensitive data handling
        inputs: ["inputs.code_submission"]
        outputs: ["security_findings", "vulnerability_severity"]
        
      - id: performance_review
        name: Performance Analysis
        agent: analyst
        model_preference: code_gen_premium
        stage_prompt: |
          Analyze performance:
          1. Algorithmic complexity
          2. Memory usage patterns
          3. I/O bottlenecks
          4. Optimization opportunities
        inputs: ["inputs.code_submission"]
        outputs: ["performance_analysis", "optimization_suggestions"]
        
      - id: maintainability_assessment
        name: Maintainability Assessment
        agent: reasoner
        model_preference: reasoning_complex
        stage_prompt: |
          Assess maintainability:
          1. Maintainability index
          2. Technical debt estimate
          3. Readability score
          4. Refactoring suggestions
        inputs: ["inputs.code_submission", "static_analysis.complexity_metrics"]
        outputs: ["maintainability_index", "tech_debt_estimate"]
        
      - id: final_grading
        name: Assign Final Grade
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Synthesize all assessments using this WEIGHTED SCORING MATRIX (0-10 Scale per category):
          
          1. **Static Analysis (Weight: 0.25)**
             - 10: Zero lint errors, low complexity
             - 5: Some warnings, average complexity
             - 0: Build failure
             
          2. **Security (Weight: 0.25)**
             - 10: Zero issues
             - 5: Low/Medium issues
             - 0: Critical vulnerability
             
          3. **Performance (Weight: 0.25)**
             - 10: Optimal O(n) or better
             - 5: Unoptimized loops
             - 0: Major bottlenecks
             
          4. **Maintainability (Weight: 0.25)**
             - 10: High index, clean code
             - 5: Average index
             - 0: Unmaintainable
             
          Calculate the specific weighted score (0-100) and assign the letter grade.
        inputs: [
          "static_analysis.complexity_metrics",
          "security_audit.vulnerability_severity",
          "performance_review.performance_analysis",
          "maintainability_assessment.maintainability_index"
        ]
        outputs: ["overall_grade", "category_scores", "improvement_priorities"]

  # ==========================================================================
  # Workflow 6: Research & Documentation
  # ==========================================================================
  research_pipeline:
    name: Research & Knowledge Synthesis
    description: Multi-agent research for comprehensive documentation
    
    inputs:
      - name: topic
        type: text
        required: true
        description: Research topic
      - name: context
        type: text
        required: false
        description: Background context
        
    outputs:
      - name: research_report
        type: file
        description: Comprehensive report
      - name: recommendations
        type: object
        description: Actionable recommendations
        
    steps:
      - id: topic_decomposition
        name: Decompose Topic
        agent: analyst
        model_preference: reasoning_complex
        stage_prompt: |
          Decompose the research topic:
          1. Sub-topics to explore
          2. Key research questions
          3. Scope definition
        inputs: ["inputs.topic", "inputs.context"]
        outputs: ["sub_topics", "research_questions"]
        
      - id: deep_analysis
        name: Perform Analysis
        agent: researcher
        model_preference: reasoning_complex
        stage_prompt: |
          Perform deep analysis:
          1. Explore each sub-topic
          2. Identify patterns and trends
          3. Draw connections
          4. Generate insights
        inputs: ["topic_decomposition.sub_topics", "topic_decomposition.research_questions"]
        outputs: ["analysis_results", "insights"]
        
      - id: synthesis
        name: Synthesize Findings
        agent: researcher
        model_preference: code_gen_premium
        stage_prompt: |
          Synthesize all findings:
          1. Integrate perspectives
          2. Identify key themes
          3. Build coherent narrative
        inputs: ["deep_analysis.analysis_results", "deep_analysis.insights"]
        outputs: ["synthesized_findings", "key_themes"]
        
      - id: validation
        name: Validate Conclusions
        agent: validator
        model_preference: reasoning_complex
        stage_prompt: |
          Validate conclusions:
          1. Check logical consistency
          2. Evaluate evidence strength
          3. Note limitations
          4. Assign confidence scores
        inputs: ["synthesis.synthesized_findings"]
        outputs: ["validated_findings", "confidence_scores"]
        
      - id: recommendations
        name: Generate Recommendations
        agent: planner
        model_preference: code_gen_premium
        stage_prompt: |
          Generate actionable recommendations:
          1. Specific action items
          2. Priority ranking
          3. Implementation guidance
        inputs: ["validation.validated_findings", "inputs.context"]
        outputs: ["recommendations", "action_items"]
        
      - id: report_creation
        name: Create Report
        agent: writer
        model_preference: documentation
        stage_prompt: |
          Create comprehensive research report:
          1. Executive summary
          2. Methodology
          3. Key findings
          4. Recommendations
          5. References
        inputs: ["validation.validated_findings", "recommendations.recommendations"]
        outputs: ["research_report"]

  # ==========================================================================
  # Workflow 7: Prompt Engineering
  # ==========================================================================
  prompt_engineering:
    name: Automated Prompt Engineering
    description: Generate and optimize prompts
    
    inputs:
      - name: task_description
        type: text
        required: true
        description: Task the prompt should accomplish
      - name: example_inputs
        type: text
        required: false
        description: Example inputs for testing
        
    outputs:
      - name: optimized_prompt
        type: text
        description: Final optimized prompt
      - name: evaluation_report
        type: file
        description: Evaluation metrics
        
    steps:
      - id: task_analysis
        name: Analyze Task
        agent: analyst
        model_preference: reasoning_complex
        stage_prompt: |
          Analyze the task requirements:
          1. Core objectives
          2. Success criteria
          3. Edge cases
          4. Output format
        inputs: ["inputs.task_description"]
        outputs: ["task_breakdown", "success_criteria", "edge_cases"]
        
      - id: prompt_generation
        name: Generate Prompts
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Generate 5 diverse prompt candidates:
          1. Use various strategies (CoT, few-shot, etc.)
          2. Different structures
          3. Clear instructions
          4. Relevant examples
        inputs: ["task_analysis.task_breakdown", "task_analysis.success_criteria"]
        outputs: ["prompt_candidates"]
        config:
          num_candidates: 5
        
      - id: prompt_evaluation
        name: Evaluate Prompts
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate each prompt candidate:
          1. Clarity and specificity
          2. Task completion likelihood
          3. Edge case handling
          4. Robustness
        inputs: ["prompt_generation.prompt_candidates", "inputs.example_inputs"]
        outputs: ["evaluation_scores", "rankings"]
        
      - id: prompt_refinement
        name: Refine Best Prompt
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Refine the top-ranked prompt:
          1. Address weaknesses
          2. Improve clarity
          3. Enhance examples
        inputs: ["prompt_evaluation.rankings", "prompt_generation.prompt_candidates"]
        outputs: ["refined_prompt"]
        
      - id: adversarial_testing
        name: Adversarial Testing
        agent: tester
        model_preference: reasoning_complex
        stage_prompt: |
          Test prompt with adversarial inputs:
          1. Edge cases
          2. Ambiguous queries
          3. Malformed inputs
          Identify failure modes.
        inputs: ["prompt_refinement.refined_prompt"]
        outputs: ["failure_cases", "robustness_score"]
        
      - id: final_optimization
        name: Final Optimization
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Create final optimized prompt:
          1. Fix failure cases
          2. Maximize robustness
          3. Optimize length
        inputs: ["prompt_refinement.refined_prompt", "adversarial_testing.failure_cases"]
        outputs: ["optimized_prompt"]

  # ==========================================================================
  # Workflow 8: Test Suite Generation
  # ==========================================================================
  test_suite_generation:
    name: Test Suite Generation
    description: Generate comprehensive tests from code
    
    inputs:
      - name: source_code
        type: directory
        required: true
        description: Source code to test
      - name: coverage_target
        type: number
        required: false
        default: 80
        description: Target coverage percentage
        
    outputs:
      - name: test_suite
        type: directory
        description: Generated tests
      - name: coverage_report
        type: file
        description: Coverage analysis
        
    steps:
      - id: code_analysis
        name: Analyze Code
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Analyze the source code:
          1. Function/class inventory
          2. Dependencies
          3. Complexity hotspots
          4. Public interfaces
        inputs: ["inputs.source_code"]
        outputs: ["code_structure", "function_inventory"]
        
      - id: test_planning
        name: Create Test Plan
        agent: planner
        model_preference: code_gen_premium
        stage_prompt: |
          Create test plan:
          1. Testing strategy
          2. Priority ranking
          3. Coverage targets per module
        inputs: ["code_analysis.code_structure", "inputs.coverage_target"]
        outputs: ["test_plan", "priorities"]
        
      - id: unit_test_generation
        name: Generate Unit Tests
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          Generate unit tests:
          1. Test each function/method
          2. Cover normal and edge cases
          3. Use proper mocking
          4. AAA pattern
        inputs: ["code_analysis.function_inventory", "test_planning.test_plan"]
        outputs: ["unit_tests", "test_fixtures"]
        
      - id: edge_case_analysis
        name: Identify Edge Cases
        agent: reasoner
        model_preference: reasoning_complex
        stage_prompt: |
          Identify edge cases and boundary conditions:
          1. Null/empty inputs
          2. Boundary values
          3. Error conditions
          4. Concurrent scenarios
        inputs: ["code_analysis.function_inventory"]
        outputs: ["edge_cases", "error_scenarios"]
        
      - id: edge_case_tests
        name: Generate Edge Case Tests
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          Generate tests for edge cases:
          1. Boundary values
          2. Error conditions
          3. Corner cases
        inputs: ["edge_case_analysis.edge_cases"]
        outputs: ["edge_case_tests"]
        
      - id: coverage_analysis
        name: Analyze Coverage
        agent: validator
        model_preference: code_gen_fast
        stage_prompt: |
          Analyze test coverage:
          1. Line coverage
          2. Branch coverage
          3. Identify gaps
        inputs: ["unit_test_generation.unit_tests", "edge_case_tests.edge_case_tests"]
        outputs: ["coverage_estimate", "coverage_gaps"]
        
      - id: gap_filling
        name: Fill Coverage Gaps
        agent: tester
        model_preference: code_gen_premium
        condition: "coverage_analysis.coverage_estimate < inputs.coverage_target"
        stage_prompt: |
          Generate additional tests to fill gaps:
          1. Cover untested paths
          2. Increase branch coverage
        inputs: ["coverage_analysis.coverage_gaps"]
        outputs: ["additional_tests"]

  # ==========================================================================
  # Workflow 8: Automated Debugging
  # ==========================================================================
  automated_debugging:
    name: Automated Debugging & Fix
    description: Execute code, analyze failures, and generate surgical fixes (HumanEval ready).
    
    inputs:
      - name: problem_statement
        type: text
        required: true
        description: The problem description or prompt.
      - name: test_cases
        type: text
        required: false
        description: Optional test cases.
        
    outputs:
      - name: final_solution
        type: file
      - name: debug_report
        type: file
        
    steps:
      - id: initial_implementation
        name: Initial Implementation
        agent: coder
        model_preference: code_gen_premium
        stage_prompt: |
          Implement the solution for the problem.
          - Follow strict Python/Language conventions.
          - Ensure the code matches the provided signature.
          - Output JSON: {"implementation": "<code>", "source_file": "name.py"}
        inputs: ["inputs.problem_statement"]
        outputs: ["implementation", "source_file"]
        
      - id: verification
        name: Verify Implementation
        agent: tester
        model_preference: code_gen_fast
        stage_prompt: |
          Create and run a reproduction script for the implementation.
          - Include the provided docstring tests.
          - Output JSON: {"execution_result": "PASS/FAIL", "logs": "...", "success_flag": boolean}
        inputs: ["initial_implementation.implementation", "inputs.test_cases"]
        outputs: ["execution_result", "logs", "success_flag"]
        
      - id: impact_analysis
        name: Technical Impact Analysis
        agent: analyst
        model_preference: reasoning_complex
        condition: "verification.success_flag == False"
        stage_prompt: |
          Analyze the bug revealed by the failure.
          - Describe the Bug and its technical impact.
          - Define STRICT Requirements for the fix.
          - Output JSON: {"bug_requirements": "...", "impact": "..."}
        inputs: ["verification.logs", "inputs.problem_statement"]
        outputs: ["bug_requirements", "impact"]
        
      - id: failure_analysis
        name: Analyze Failure Point
        agent: debugger
        model_preference: reasoning_complex
        condition: "verification.success_flag == False"
        stage_prompt: |
          Analyze the point of failure based on the Requirements.
          - Identify Root Cause.
          - Explain why it failed.
          - Output JSON: {"root_cause_analysis": "..."}
        inputs: ["verification.logs", "impact_analysis.bug_requirements", "initial_implementation.implementation"]
        outputs: ["root_cause_analysis"]
        
      - id: patch_generation
        name: Generate Patch
        agent: debugger
        model_preference: code_gen_premium
        condition: "verification.success_flag == False"
        stage_prompt: |
          Generate a surgical patch based on the analysis.
          - Must satisfy the defined Requirements.
          - Output FULL corrected code.
          - Output JSON: {"patched_code": "..."}
        inputs: ["failure_analysis.root_cause_analysis", "initial_implementation.implementation"]
        outputs: ["patched_code"]
        
      - id: code_review
        name: Review Patch
        agent: coder
        model_preference: code_review
        condition: "verification.success_flag == False"
        stage_prompt: |
          Review the patch for code quality and correctness.
          - Check against strict coding standards.
          - Output JSON: {"verdict": "APPROVE/REJECT", "comments": "..."}
        inputs: ["patch_generation.patched_code"]
        outputs: ["verdict", "comments"]
        
      - id: final_verification
        name: Verify Patch
        agent: tester
        model_preference: code_gen_fast
        condition: "verification.success_flag == False"
        stage_prompt: |
          Run the tests again on the patched code.
          - Output JSON: {"final_result": "PASS/FAIL", "success_flag": boolean}
        inputs: ["patch_generation.patched_code"]
        outputs: ["final_result", "success_flag"]
        
      - id: impact_documentation
        name: Document Impact
        agent: writer
        model_preference: code_gen_premium
        condition: "verification.success_flag == False"
        stage_prompt: |
          Document the fix and its impact.
          - List modified files/functions.
          - Describe side effects or regressions checked.
          - Update documentation for affected components.
          - Output JSON: {"documentation_update": "..."}
        inputs: ["patch_generation.patched_code", "impact_analysis.impact"]
        outputs: ["documentation_update"]

      - id: final_evaluation
        name: Debugging Evaluation
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
           Evaluate the debugging process against this STRICT RUBRIC:
           
           1. **Resolution (Weight: 0.5, 0-10 Scale)**
              - 10: Code passes all tests.
              - 0: Tests fail.
              
           2. **Process (Weight: 0.3, 0-10 Scale)**
              - 10: Requirements clearly defined and met.
              - 5: Fix worked but ignored requirements.
              
           3. **Documentation (Weight: 0.2, 0-10 Scale)**
              - 10: Clear impact documentation.
              - 0: No documentation.
              
           Synthesize final score (0-100).
        inputs: ["initial_implementation.implementation", "patch_generation.patched_code", "impact_analysis.bug_requirements", "impact_documentation.documentation_update"]
        outputs: ["score", "verdict"]

  # ==========================================================================
  # Workflow 10: Repository Maintenance & Cleanup
  # ==========================================================================
  repository_maintenance:
    name: Repository Maintenance & Cleanup
    description: Multi-agent workflow for testing tools, updating documentation, and removing duplicates
    
    inputs:
      - name: repo_path
        type: directory
        required: true
        description: Path to the repository root
      - name: cleanup_manifest
        type: file
        required: false
        description: Optional manifest of files to evaluate for removal
      - name: dry_run
        type: boolean
        required: false
        default: true
        description: If true, generate report only (no actual deletions)
        
    outputs:
      - name: maintenance_report
        type: file
        description: Full report of maintenance actions taken/recommended
      - name: tool_test_results
        type: file
        description: Test results for all tools
      - name: documentation_updates
        type: directory
        description: Updated documentation files
      - name: cleanup_manifest
        type: file
        description: Files identified for cleanup
        
    steps:
      - id: repo_exploration
        name: Explore Repository Structure
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Explore and map the repository structure:
          1. Identify all directories and their purposes
          2. Map the tools/ directory structure completely
          3. Identify all Python modules, scripts, and executables
          4. Create an inventory of all README/documentation files
          5. Note any apparent duplicates or redundant paths
          
          Output a structured JSON inventory of the repository.
        inputs: ["inputs.repo_path"]
        outputs: ["repo_inventory", "tool_locations", "doc_locations", "potential_duplicates"]
        
      - id: tool_discovery
        name: Discover and Catalog Tools
        agent: analyst
        model_preference: code_gen_fast
        stage_prompt: |
          Analyze each tool discovered in the repository:
          1. Parse Python files to identify callable entry points
          2. Check for CLI interfaces (__main__.py, argparse, click, typer)
          3. Identify dependencies and imports
          4. Determine test coverage (look for associated test files)
          5. Rate tool health: ACTIVE, DEPRECATED, BROKEN, UNKNOWN
          
          Generate a tool catalog with execution methods for each.
        inputs: ["repo_exploration.tool_locations", "inputs.repo_path"]
        outputs: ["tool_catalog", "execution_methods", "tool_health_status"]
        
      - id: tool_testing
        name: Test Repository Tools
        agent: tester
        model_preference: code_gen_premium
        stage_prompt: |
          For each tool in the catalog, attempt to verify functionality:
          1. Check if tool can be imported without errors
          2. Validate any CLI --help commands work
          3. Run any existing unit tests (pytest, unittest)
          4. Execute smoke tests where safe (read-only operations)
          5. Record pass/fail/skip status with details
          
          DO NOT execute any commands that modify files, make network requests, 
          or could cause side effects unless explicitly marked safe.
          
          Generate detailed test results per tool.
        inputs: ["tool_discovery.tool_catalog", "tool_discovery.execution_methods"]
        outputs: ["test_results", "working_tools", "broken_tools", "test_coverage"]
        
      - id: documentation_audit
        name: Audit Documentation
        agent: reviewer
        model_preference: code_review
        stage_prompt: |
          Audit all documentation files for quality and accuracy:
          1. Check README files for completeness (description, usage, examples)
          2. Verify documented features match actual code
          3. Identify outdated references to moved/deleted files
          4. Find broken internal links
          5. Identify missing documentation for tools/modules
          
          Generate an audit report with specific issues per file.
        inputs: ["repo_exploration.doc_locations", "tool_discovery.tool_catalog"]
        outputs: ["doc_audit_report", "broken_links", "missing_docs", "outdated_refs"]
        
      - id: documentation_update
        name: Update Documentation
        agent: writer
        model_preference: documentation
        stage_prompt: |
          Generate updated documentation based on the audit:
          1. Update README files with accurate information
          2. Add missing documentation for undocumented tools
          3. Fix broken links and references
          4. Ensure consistent formatting across docs
          5. Add/update examples based on tool test results
          
          Output the updated files with clear diff annotations.
        inputs: [
          "documentation_audit.doc_audit_report",
          "documentation_audit.missing_docs",
          "tool_testing.working_tools"
        ]
        outputs: ["updated_docs", "new_docs", "doc_changes_summary"]
        
      - id: duplicate_detection
        name: Detect Duplicate Files
        agent: analyst
        model_preference: reasoning_complex
        stage_prompt: |
          Identify duplicate and redundant files in the repository:
          1. Find exact file duplicates (same content, different paths)
          2. Find near-duplicate files (>90% similar content)
          3. Identify superseded files (older versions in archive/)
          4. Find orphaned files (not imported/referenced anywhere)
          5. Identify empty or stub files with no real content
          
          For each duplicate group, recommend which file to KEEP and which to REMOVE.
          Provide reasoning for each recommendation.
        inputs: ["repo_exploration.repo_inventory", "repo_exploration.potential_duplicates"]
        outputs: ["duplicate_groups", "orphaned_files", "removal_recommendations"]
        
      - id: cleanup_planning
        name: Create Cleanup Plan
        agent: planner
        model_preference: code_gen_premium
        stage_prompt: |
          Create a safe cleanup plan for the repository:
          1. Prioritize removals by risk (safe, medium, risky)
          2. Identify dependencies that would break if files removed
          3. Create a rollback strategy (git-based)
          4. Estimate disk space savings
          5. Generate cleanup commands (but DO NOT execute)
          
          Organize into phases:
          - Phase 1: Safe deletions (empty files, exact duplicates)
          - Phase 2: Medium risk (orphaned files)
          - Phase 3: Manual review required (near-duplicates)
        inputs: [
          "duplicate_detection.removal_recommendations",
          "duplicate_detection.orphaned_files",
          "inputs.cleanup_manifest"
        ]
        outputs: ["cleanup_plan", "risk_assessment", "rollback_strategy"]
        
      - id: validation
        name: Validate Maintenance Plan
        agent: validator
        model_preference: reasoning_complex
        stage_prompt: |
          Validate all proposed changes before execution:
          1. Verify no critical files marked for deletion
          2. Check documentation updates are accurate
          3. Ensure cleanup plan won't break imports
          4. Validate test results are consistent
          5. Cross-reference with .cleanup_manifest.json if exists
          
          Flag any concerns that require human review.
        inputs: [
          "cleanup_planning.cleanup_plan",
          "documentation_update.updated_docs",
          "tool_testing.test_results"
        ]
        outputs: ["validation_report", "concerns", "approved_changes"]
        
      - id: report_generation
        name: Generate Maintenance Report
        agent: writer
        model_preference: documentation
        stage_prompt: |
          Generate a comprehensive maintenance report:
          
          ## Executive Summary
          - Total tools discovered/tested
          - Documentation files updated
          - Files recommended for cleanup
          - Disk space savings estimate
          
          ## Tool Health Report
          - Working tools with test results
          - Broken tools requiring attention
          - Tools without tests
          
          ## Documentation Updates
          - Files updated with diffs
          - New documentation added
          - Broken links fixed
          
          ## Cleanup Recommendations
          - Phase 1 (safe): Ready for automated cleanup
          - Phase 2 (medium): Needs quick review
          - Phase 3 (manual): Requires careful evaluation
          
          ## Validation Status
          - All concerns and warnings
          - Human review items
        inputs: [
          "tool_testing.test_results",
          "documentation_update.doc_changes_summary",
          "cleanup_planning.cleanup_plan",
          "validation.validation_report"
        ]
        outputs: ["maintenance_report"]
        
      - id: final_evaluation
        name: Maintenance Assessment
        agent: judge
        model_preference: reasoning_complex
        stage_prompt: |
          Evaluate the Repository Maintenance workflow against this RUBRIC:
          
          1. **Coverage (Weight: 0.3)**
             - 10: All tools discovered and tested
             - 5: Most tools covered
             - 0: Major gaps in coverage
             
          2. **Documentation Quality (Weight: 0.25)**
             - 10: All docs updated, accurate, well-formatted
             - 5: Basic updates made
             - 0: Documentation still broken
             
          3. **Cleanup Safety (Weight: 0.25)**
             - 10: All recommendations safe, rollback ready
             - 5: Some risky recommendations
             - 0: Would break the repository
             
          4. **Actionability (Weight: 0.2)**
             - 10: Clear, executable action items
             - 5: Vague recommendations
             - 0: No actionable output
             
          Synthesize into final score (0-100) and verdict.
        inputs: [
          "report_generation.maintenance_report",
          "validation.validation_report"
        ]
        outputs: ["final_verdict", "score_breakdown"]

# Global settings
settings:
  parallel_steps: true
  checkpoint_enabled: true
  logging_level: DEBUG
  artifact_retention_days: 30
