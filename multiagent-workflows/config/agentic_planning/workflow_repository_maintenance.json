{
	"version": "2.2",
	"description": "Workflow: Repository Maintenance & Cleanup (Librarian-Orchestrated)",
	"metadata": {
		"complexity": "high",
		"team_size": 7,
		"phases": [
			"Discovery",
			"Analysis",
			"Verification",
			"Curation",
			"Reporting"
		],
		"patterns_used": ["LATS", "CoVe", "ReAct", "G-Eval", "Reflexion"]
	},
	"agents": [
		{
			"id": "librarian",
			"name": "Head Librarian",
			"model": "gh:openai/gpt-4o",
			"compatible_models": [
				"gh:openai/o3-mini",
				"gh:deepseek/deepseek-v3",
				"opus-4.5"
			],
			"role": "Orchestrate maintenance workflow and curate recommendations",
			"output_format": "Curated Catalog & Final Recommendations",
			"phase": "all",
			"tier": "cloud_premium",
			"why": "Requires high-level synthesis and decision-making across all specialist reports.",
			"system_prompt": "You are the Head Librarian of a code repository.\n\nYour role is to:\n1. CATALOG: Maintain a comprehensive inventory of all code, tools, and documentation\n2. DIRECT: Assign specialist agents to their appropriate tasks\n3. CURATE: Decide what belongs in the collection and what should be archived/removed\n4. SYNTHESIZE: Combine reports from all specialists into coherent recommendations\n5. PRESERVE: Ensure nothing valuable is lost during maintenance\n\nYou coordinate: Explorer, Engineering Expert, Tester, Documenter, and Cleanup agents.\nAlways prioritize preservation over deletion. When in doubt, archive don't delete.",
			"temperature": 0.3,
			"max_tokens": 8192
		},
		{
			"id": "explorer",
			"name": "Repository Explorer",
			"model": "gh:openai/gpt-4o-mini",
			"compatible_models": [
				"gh:openai/gpt-4o",
				"gh:meta/llama-3.3-70b-instruct"
			],
			"role": "Map and catalog repository structure",
			"output_format": "Repository Inventory JSON",
			"phase": "discovery",
			"tier": "cloud_fast",
			"why": "Efficient file system traversal and cataloging requires fast, accurate responses.",
			"system_prompt": "You are a Repository Explorer and Catalog Assistant.\n\nYour job is to:\n1. Map the complete directory structure\n2. Identify all Python modules, scripts, and executables\n3. Catalog all documentation files (README, .md files)\n4. Identify all tools and their entry points\n5. Flag potential duplicates (same filename in multiple locations)\n6. Note orphaned or deprecated paths\n\nOutput a structured JSON inventory with categorized file lists.",
			"temperature": 0.1,
			"max_tokens": 8192
		},
		{
			"id": "engineering_expert",
			"name": "Engineering Expert",
			"model": "gh:openai/gpt-4o",
			"compatible_models": [
				"gh:openai/o3-mini",
				"gh:deepseek/deepseek-r1",
				"opus-4.5"
			],
			"role": "Analyze code quality, best practices, and technical correctness",
			"output_format": "Engineering Analysis Report",
			"phase": "analysis",
			"tier": "cloud_premium",
			"why": "Deep code analysis requires sophisticated reasoning about patterns, security, and quality.",
			"system_prompt": "You are a Senior AI & Software Engineering Expert.\n\nAnalyze code repositories for:\n1. CORRECTNESS: Does the code work as intended? Syntax errors, runtime issues.\n2. BEST PRACTICES: Python/industry standards (PEP8, type hints, docstrings).\n3. QUALITY: Code readability, maintainability, modularity, DRY principle.\n4. SECURITY: OWASP vulnerabilities, hardcoded secrets, input validation.\n5. PERFORMANCE: Algorithmic complexity, memory usage, I/O bottlenecks.\n6. TECHNICAL DEBT: TODO/FIXME comments, deprecated patterns, refactoring needs.\n\nProvide actionable, specific feedback with priority ratings (HIGH/MEDIUM/LOW).\nFor each issue, include: file path, line range, issue description, and fix suggestion.",
			"temperature": 0.2,
			"max_tokens": 8192
		},
		{
			"id": "tester",
			"name": "Tool Tester",
			"model": "gh:openai/gpt-4o",
			"compatible_models": [
				"gh:openai/gpt-4o-mini",
				"gh:meta/llama-3.3-70b-instruct"
			],
			"role": "Validate tool functionality and test coverage",
			"output_format": "Tool Test Results Report",
			"phase": "analysis",
			"tier": "cloud_premium",
			"why": "Reliable test generation and syntax validation requires accurate code understanding.",
			"system_prompt": "You are a QA Engineer and Tool Testing Specialist.\n\nFor each tool in the repository:\n1. Check if the tool can be imported without syntax errors\n2. Verify CLI interfaces work (--help commands)\n3. Identify existing test files and coverage\n4. Execute smoke tests where safe (read-only operations only)\n5. Record pass/fail/skip status with detailed output\n\nDO NOT execute commands that:\n- Modify files or state\n- Make network requests\n- Require API keys or credentials\n- Could cause side effects\n\nOutput a structured test report with status per tool.",
			"temperature": 0.1,
			"max_tokens": 4096
		},
		{
			"id": "documenter",
			"name": "Documentation Specialist",
			"model": "gh:openai/gpt-4o-mini",
			"compatible_models": [
				"gh:openai/gpt-4o",
				"gh:meta/llama-3.3-70b-instruct"
			],
			"role": "Audit and update documentation",
			"output_format": "Documentation Audit Report + Updated Files",
			"phase": "analysis",
			"tier": "cloud_fast",
			"why": "Documentation analysis is straightforward but requires thoroughness.",
			"system_prompt": "You are a Technical Writer and Documentation Specialist.\n\nAudit all documentation for:\n1. COMPLETENESS: Do READMEs have description, installation, usage, examples?\n2. ACCURACY: Do documented features match actual code?\n3. LINKS: Find broken internal links and references\n4. CONSISTENCY: Formatting, style, terminology consistency\n5. GAPS: Identify undocumented tools or modules\n\nFor each issue found, provide:\n- File path\n- Issue description\n- Suggested fix or addition\n\nGenerate updated documentation with clear diff annotations.",
			"temperature": 0.3,
			"max_tokens": 8192
		},
		{
			"id": "cleanup",
			"name": "Cleanup Specialist",
			"model": "gh:openai/gpt-4o",
			"compatible_models": ["gh:openai/o3-mini", "gh:deepseek/deepseek-v3"],
			"role": "Identify duplicates, orphans, and cleanup candidates",
			"output_format": "Phased Cleanup Plan",
			"phase": "curation",
			"tier": "cloud_premium",
			"why": "Duplicate detection and impact analysis requires careful reasoning.",
			"system_prompt": "You are a Collection Manager and Cleanup Specialist.\n\nIdentify files for cleanup in three phases:\n\nPHASE 1 - SAFE (Automated):\n- Empty files (0 bytes)\n- Stub files with only 'pass' or TODO\n- Exact duplicates (same content, different paths)\n\nPHASE 2 - MEDIUM RISK (Verify first):\n- Orphaned files (not imported anywhere)\n- Archived copies with active equivalents\n- Deprecated files marked for removal\n\nPHASE 3 - MANUAL REVIEW:\n- Near-duplicates (>90% similar)\n- Files with same name in multiple locations\n- Legacy code that might still be referenced\n\nFor each recommendation, provide:\n- File path\n- Reason for cleanup\n- Dependencies that would break\n- Rollback strategy",
			"temperature": 0.2,
			"max_tokens": 8192
		},
		{
			"id": "lats_quality_controller",
			"name": "LATS Quality Controller",
			"model": "gh:openai/gpt-4o",
			"compatible_models": [
				"gh:openai/o3-mini",
				"gh:deepseek/deepseek-r1",
				"ollama:phi4-reasoning:latest"
			],
			"role": "Iteratively verify and improve recommendations using LATS Self-Refine pattern",
			"output_format": "Quality-Controlled Recommendations with Confidence Scores",
			"phase": "verification",
			"tier": "cloud_premium",
			"why": "LATS combines tree search + self-refine + CoVe + ReAct for iterative quality improvement until threshold met.",
			"system_prompt": "You are a LATS Quality Controller using a hybrid pattern:\n\n## LATS OUTER LOOP (iterate until threshold met)\n\n### BRANCH A: CRITERIA VALIDATION (CoVe Pattern)\n1. Review each recommendation from specialist agents\n2. Generate verification questions:\n   - Is this file truly orphaned? (check imports, references)\n   - Is this duplicate assessment accurate? (compare file content)\n   - Would this deletion break any dependencies?\n   - Is the code quality score justified?\n3. Answer each question INDEPENDENTLY (not referencing original rec)\n4. Output: CONFIRMED, REVISED, or REJECTED with evidence\n\n### BRANCH B: SCORING (G-Eval Pattern)\nScore each recommendation on:\n- Accuracy (0-10): Is the assessment correct?\n- Safety (0-10): Risk of unintended consequences?\n- Actionability (0-10): Can this be executed?\n- Evidence (0-10): Is there supporting proof?\n\nCompute weighted score. Threshold: 80%\n\n### BRANCH C: IMPROVEMENT (ReAct Pattern)\nFor recommendations below threshold:\n- Thought: What specific issue needs fixing?\n- Action: Revise the recommendation\n- Observation: Validate the improvement\n\n### SYNTHESIS\nIf weighted_score >= 80: ACCEPT recommendation\nIf weighted_score < 80 AND iterations < max: REFINE and loop\nIf max iterations: OUTPUT with confidence warning\n\n### REFLEXION (if looping)\n- What worked: [effective verification strategy]\n- What didn't: [missed issue or false positive]\n- Adjusted strategy: [improvement for next iteration]\n\nOutput verified recommendations with:\n- Original recommendation\n- Verification evidence\n- Final verdict (CONFIRMED/REVISED/REJECTED)\n- Confidence: HIGH (>90%), MEDIUM (70-90%), LOW (<70%)\n- Iteration count",
			"temperature": 0.1,
			"max_tokens": 8192,
			"lats_config": {
				"quality_threshold": 80,
				"max_iterations": 5,
				"branches": ["criteria_validation", "scoring", "improvement"],
				"patterns": ["CoVe", "G-Eval", "ReAct", "Reflexion"]
			}
		}
	],
	"workflow": {
		"steps": [
			{
				"id": "1_explore",
				"name": "Repository Exploration",
				"agent": "explorer",
				"phase": "discovery",
				"inputs": ["repo_path"],
				"outputs": [
					"repo_inventory",
					"tool_locations",
					"doc_locations",
					"potential_duplicates"
				],
				"description": "Map and catalog the entire repository structure"
			},
			{
				"id": "2_test",
				"name": "Tool Testing",
				"agent": "tester",
				"phase": "analysis",
				"inputs": ["repo_inventory.tools", "repo_path"],
				"outputs": ["test_results", "working_tools", "broken_tools"],
				"description": "Validate all discovered tools for functionality"
			},
			{
				"id": "3_engineering",
				"name": "Engineering Analysis",
				"agent": "engineering_expert",
				"phase": "analysis",
				"inputs": ["repo_inventory.tools", "repo_path"],
				"outputs": [
					"quality_issues",
					"security_concerns",
					"best_practices_violations",
					"recommendations"
				],
				"description": "Analyze code quality, security, and best practices"
			},
			{
				"id": "4_documentation",
				"name": "Documentation Audit",
				"agent": "documenter",
				"phase": "analysis",
				"inputs": [
					"repo_inventory.documentation",
					"test_results.working_tools"
				],
				"outputs": [
					"doc_audit",
					"broken_links",
					"missing_docs",
					"updated_docs"
				],
				"description": "Audit and update all documentation"
			},
			{
				"id": "5_cleanup",
				"name": "Cleanup Planning",
				"agent": "cleanup",
				"phase": "curation",
				"inputs": [
					"repo_inventory",
					"test_results",
					"engineering_analysis.quality_issues"
				],
				"outputs": ["cleanup_plan", "risk_assessment", "rollback_strategy"],
				"description": "Identify duplicates and create phased cleanup plan"
			},
			{
				"id": "6_triage",
				"name": "Librarian Triage",
				"agent": "librarian",
				"phase": "verification",
				"inputs": [
					"cleanup_plan",
					"engineering_analysis",
					"doc_audit",
					"test_results"
				],
				"outputs": [
					"verification_agenda",
					"priority_items",
					"risk_flags",
					"lats_instructions"
				],
				"description": "Analyze all specialist reports and create prioritized verification agenda for LATS",
				"triage_prompt": "Review all specialist recommendations and create a verification agenda:\n\n1. HIGH PRIORITY (verify first):\n   - Any deletion recommendations\n   - Security-related findings\n   - Breaking change warnings\n\n2. MEDIUM PRIORITY:\n   - Duplicate file assessments\n   - Code quality issues with high severity\n   - Missing documentation claims\n\n3. LOW PRIORITY:\n   - Style/formatting suggestions\n   - Minor refactoring ideas\n\nFor each item, explain WHY verification is needed and WHAT to check."
			},
			{
				"id": "7_lats_verify",
				"name": "LATS Quality Verification",
				"agent": "lats_quality_controller",
				"phase": "verification",
				"inputs": [
					"verification_agenda",
					"lats_instructions",
					"cleanup_plan",
					"engineering_analysis.recommendations",
					"doc_audit",
					"repo_inventory"
				],
				"outputs": [
					"verified_cleanup_plan",
					"verified_recommendations",
					"confidence_scores",
					"verification_log"
				],
				"description": "Iteratively verify prioritized recommendations using LATS Self-Refine until quality threshold met",
				"lats_settings": {
					"threshold": 80,
					"max_iterations": 5,
					"branches": ["CoVe", "Scoring", "ReAct"],
					"focus_on": "verification_agenda.priority_items"
				}
			},
			{
				"id": "8_curate",
				"name": "Librarian Final Curation",
				"agent": "librarian",
				"phase": "curation",
				"inputs": [
					"test_results",
					"verified_recommendations",
					"verified_cleanup_plan",
					"confidence_scores",
					"verification_log"
				],
				"outputs": [
					"preservation_list",
					"archive_list",
					"discard_list",
					"review_list"
				],
				"description": "Synthesize verified reports and curate final recommendations based on LATS confidence scores"
			},
			{
				"id": "9_report",
				"name": "Final Report Generation",
				"agent": "librarian",
				"phase": "reporting",
				"inputs": ["all_previous_outputs", "verification_log", "lats_metrics"],
				"outputs": [
					"maintenance_report",
					"executive_summary",
					"action_items",
					"lats_metrics",
					"verification_summary"
				],
				"description": "Generate comprehensive maintenance report with LATS verification metrics and confidence levels"
			}
		],
		"parallel_groups": [
			{
				"group_id": "analysis_parallel",
				"steps": ["2_test", "3_engineering", "4_documentation"],
				"description": "Run analysis steps in parallel after exploration"
			}
		]
	},
	"rubric": "repository_maintenance",
	"success_criteria": {
		"min_tool_coverage": 0.9,
		"max_broken_tools_pct": 0.1,
		"documentation_score": 70,
		"cleanup_safety_score": 80
	}
}
