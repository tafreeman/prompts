# Universal Code Generator

AI-powered code/prompt generation with multi-model quality review.

## Overview

The Universal Code Generator applies a three-step workflow to create Tier 1 quality content:

1. **Generate** (Gemini 1.5 Pro) - Create initial draft
2. **Review** (Claude Sonnet 4) - Score against quality rubric (0-100)
3. **Refine** (Gemini 1.5 Pro) - Improve based on feedback

## Installation

```bash
# Install dependencies
pip install click

# Optional: Set model preferences via environment variables
â”‚   â”œâ”€â”€ ModelConfig (default models, temperatures)
â”‚   â””â”€â”€ PathConfig (templates, rubrics)
â”œâ”€â”€ LLMClient (tools/llm_client.py)
â”‚   â””â”€â”€ Provider dispatch (Gemini/Claude/GPT)
â”œâ”€â”€ Generator (tools/models/generator.py)
â”œâ”€â”€ Reviewer (tools/models/reviewer.py)
â”‚   â””â”€â”€ Quality rubric (tools/rubrics/quality_standards.json)
â””â”€â”€ Refiner (tools/models/refiner.py)
```

## Configuration

### Via Environment Variables

```bash
export GEN_MODEL="gemini-1.5-pro"        # Generation model
export REV_MODEL="claude-sonnet-4"       # Review model
export REF_MODEL="gemini-1.5-pro"        # Refinement model
```

### Via Code

```python
from tools.config import Config, ModelConfig
from tools.code_generator import UniversalCodeGenerator

custom_config = Config()
custom_config.models = ModelConfig(
    generator_model="gemini-2.0-flash-thinking",
    reviewer_model="claude-sonnet-4",
    refiner_model="gemini-1.5-pro"
)

generator = UniversalCodeGenerator(config=custom_config)
```

## Quality Rubric

The reviewer scores content against 5 criteria:

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Completeness** | 25% | All required sections present |
| **Example Quality** | 30% | Realistic, detailed scenarios |
| **Specificity** | 20% | Actionable, concrete content |
| **Format Adherence** | 15% | Valid YAML, markdown structure |
| **Enterprise Quality** | 10% | References frameworks, metrics |

**Scoring**:

- **90-100**: Tier 1 (Excellent)
- **75-89**: Tier 2 (Good)
- **60-74**: Tier 3 (Acceptable)
- **<60**: Tier 4 (Poor)

## Current Status

âœ… **Phases A-D Complete**

- Core generator class
- Quality review with rubric
- Refinement loop
- CLI interface (interactive + non-interactive)

â³ **Phase E: In Progress**

- Real API integration (currently mocked)
- Regression testing
- Documentation

## Examples

### Example 1: Generate Business Prompt

```bash
python -m tools.cli.main create \
  --category business \
  --use-case "Project Risk Register for IT Transformations" \
  --variables '{"project_name": "ERP Modernization", "risk_categories": "Technical, Financial, Organizational"}' \
  --output risk-register.md
```

### Example 2: Interactive Wizard

```bash
$ python -m tools.cli.main interactive

ðŸŽ¯ Universal Code Generator - Interactive Mode
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ðŸ“ Category: business
ðŸ“ Use Case (be specific): Create executive budget tracker
ðŸ“ Enter variables (leave empty to finish):
Variable name: project_name
Value for 'project_name': Cloud Platform Migration

ðŸš€ Generating content...
âœ… Generation Complete!
Review Score: 88
```

## Testing

```bash
# Run core workflow test
python -m tools.test_generator

# Run CLI test
python -m tools.cli.test_cli
```

## File Structure

```
tools/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ config.py                  # Configuration management
â”œâ”€â”€ llm_client.py              # LLM provider abstraction
â”œâ”€â”€ code_generator.py          # Main UniversalCodeGenerator class
â”œâ”€â”€ test_generator.py          # Core verification tests
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ generator.py           # Generation step
â”‚   â”œâ”€â”€ reviewer.py            # Quality review step
â”‚   â””â”€â”€ refiner.py             # Refinement step
â”œâ”€â”€ rubrics/
â”‚   â””â”€â”€ quality_standards.json # Tier 1 quality criteria
â””â”€â”€ cli/
    â”œâ”€â”€ main.py                # CLI entry point
    â”œâ”€â”€ interactive.py         # Interactive wizard
    â””â”€â”€ test_cli.py            # CLI tests
```

## Next Steps

1. **Connect Real APIs**: Replace mocked LLMClient with actual API calls
2. **Template System**: Build template selection logic
3. **Batch Processing**: Add `upgrade-stubs` command for bulk prompt upgrades
4. **Performance Tracking**: Log generation times and costs

## Support

For issues or questions, refer to:

- `walkthrough.md` - Detailed implementation walkthrough
- `implementation_plan.md` - Full architecture plan
- `quality_standards.json` - Complete rubric criteria
