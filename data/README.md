# Data

This directory contains structured data files that define the taxonomy, configuration, and metadata for the Enterprise AI Prompt Library. These files drive validation, filtering, learning paths, and database operations.

## üìÅ Directory Structure

```text
data/
‚îú‚îÄ‚îÄ db/                    # üìä Database storage files
‚îÇ   ‚îú‚îÄ‚îÄ evaluations.json   # Prompt evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ prompts.json       # Prompt metadata index
‚îÇ   ‚îî‚îÄ‚îÄ rubrics.json       # Evaluation rubrics
‚îú‚îÄ‚îÄ learning-tracks/       # üéì Structured learning paths
‚îÇ   ‚îú‚îÄ‚îÄ architect-depth.yml
‚îÇ   ‚îú‚îÄ‚îÄ engineer-quickstart.yml
‚îÇ   ‚îî‚îÄ‚îÄ functional-productivity.yml
‚îú‚îÄ‚îÄ audiences.yml          # üë• Target user personas
‚îú‚îÄ‚îÄ platforms.yml          # üñ•Ô∏è  AI platform definitions
‚îî‚îÄ‚îÄ topics.yml            # üè∑Ô∏è  Topic taxonomy
```

## üìä Core Data Files

### audiences.yml

Defines target user personas and their typical needs.

**Purpose**: Maps prompts to specific user roles for better discoverability.

**Schema**:

```yaml
audiences:

  - id: junior-engineer

    name: Junior Engineer
    description: Engineers with 0-2 years of experience
    typical_needs:

      - Copy-paste templates
      - Clear step-by-step instructions

    learning_track: engineer-quickstart
```

**Supported Audiences**:

- `junior-engineer`: 0-2 years experience, needs templates
- `senior-engineer`: 3+ years experience, needs customizable patterns
- `solution-architect`: System design and architecture focus
- `qa-engineer`: Testing and quality assurance
- `business-analyst`: Requirements and documentation
- `project-manager`: Status reports and risk assessment
- `functional-team`: Non-technical productivity users

**Usage**: Referenced in prompt frontmatter via `audience` field.

### platforms.yml

Defines AI platforms and their capabilities.

**Purpose**: Ensures prompts specify compatible platforms for optimal results.

**Example Platforms**:

- `github-copilot`: GitHub Copilot (Chat, Edits, Agents)
- `claude`: Anthropic Claude (3.x, Sonnet, Opus)
- `chatgpt`: OpenAI ChatGPT (GPT-4, GPT-4o)
- `m365-copilot`: Microsoft 365 Copilot
- `windows-copilot`: Windows Copilot

**Usage**: Referenced in prompt frontmatter via `platforms` field.

### topics.yml

Defines the topic taxonomy for categorizing and filtering prompts.

**Purpose**: Enables tag-based search and filtering across the library.

**Schema**:

```yaml
topics:

  - id: code-generation

    name: Code Generation
    description: Prompts for generating code, functions, classes
```

**Supported Topics**:

- `code-generation`: Generate code, functions, classes
- `debugging`: Identify and fix bugs
- `refactoring`: Improve code structure and quality
- `testing`: Generate tests and test strategies
- `documentation`: Write docs, comments, READMEs
- `analysis`: Data analysis and insights
- `governance`: Compliance, risk, and audit
- `business`: Business documents and communications
- `productivity`: Workflow efficiency
- `architecture`: System design decisions
- `security`: Security review and best practices
- `performance`: Performance optimization

**Usage**: Referenced in prompt frontmatter via `topics` or `tags` field.

## üéì Learning Tracks

Curated learning paths for different user personas, defined in `learning-tracks/`.

### architect-depth.yml

**Audience**: Solution architects, tech leads, senior engineers  
**Focus**: System design, architecture patterns, trade-off analysis  
**Duration**: Advanced, self-paced

**Contents**:

- Architecture decision records
- System design prompts
- Trade-off analysis frameworks
- Enterprise patterns

### engineer-quickstart.yml

**Audience**: Junior and senior engineers, QA engineers  
**Focus**: Code generation, testing, debugging, refactoring  
**Duration**: Beginner to intermediate, 2-4 hours

**Contents**:

- Your first prompt (15 min)
- Code generation basics
- Test-driven prompting
- Debugging patterns

### functional-productivity.yml

**Audience**: Business analysts, project managers, functional teams  
**Focus**: Document creation, email drafting, data analysis  
**Duration**: Beginner-friendly, 1-2 hours

**Contents**:

- Document generation
- Meeting summaries
- Status reports
- Data insights

## üìä Database Storage (db/)

JSON-based storage for evaluation results and metadata.

### evaluations.json

Stores evaluation results for all prompts.

**Schema**:

```json
{
  "prompt_id": "developers/code-generation",
  "criteria": {
    "clarity": 9.0,
    "effectiveness": 8.0,
    "structure": 9.0,
    "specificity": 8.0,
    "completeness": 8.0
  },
  "score": 8.4,
  "tier": 1,
  "timestamp": "2025-11-30T12:00:00Z"
}
```

**Generated By**: evaluation runs (see `python -m tools.prompteval ...`)

### prompts.json

Index of all prompts with metadata for fast lookup.

**Schema (representative)**:

```json
{
  "id": "c6708ed9-b75e-4fb4-bdf1-690fd725c16c",
  "name": "generate-python-function.md",
  "versions": [
    {
      "version": "1.0",
      "content": "",
      "metadata": {
        "migrated": true
      },
      "created_at": "2026-01-16T17:52:51.477882"
    }
  ]
}
```

#### Example: prompt frontmatter ‚Üí `prompts.json` entry

The library‚Äôs source-of-truth prompt content lives in Markdown files under `prompts/`. The database record tracks an ID/version and may include metadata.

**1) Prompt file with YAML frontmatter** (example: `prompts/developers/generate-python-function.md`):

```yaml
---
name: Generate Python Function
description: Generate a Python function from requirements, with clear typing and edge-case handling.
type: template
pattern: react
difficulty: intermediate
response_format: text
---
```

**2) Corresponding `data/db/prompts.json` entry** (illustrative):

```json
{
  "id": "c6708ed9-b75e-4fb4-bdf1-690fd725c16c",
  "name": "generate-python-function.md",
  "versions": [
    {
      "version": "1.0",
      "content": "",
      "metadata": {
        "path": "prompts/developers/generate-python-function.md",
        "frontmatter": {
          "name": "Generate Python Function",
          "description": "Generate a Python function from requirements, with clear typing and edge-case handling.",
          "type": "template",
          "pattern": "react",
          "difficulty": "intermediate",
          "response_format": "text"
        }
      },
      "created_at": "2026-01-16T17:52:51.477882"
    }
  ]
}
```

**Generated By**: Prompt indexer during validation

### rubrics.json

Evaluation rubrics and scoring criteria.

**Schema**:

```json
{
  "clarity": {
    "description": "How clear and understandable is the prompt?",
    "weight": 0.25,
    "criteria": [
      "Unambiguous language",
      "Clear objectives",
      "Minimal jargon"
    ]
  }
}
```

**Used By**: Prompt evaluation tooling (e.g., `tools.prompteval`)

## üîß Using Data Files

### For Contributors

When adding a new prompt, reference valid values from these files:

```yaml
---
title: "My New Prompt"
audience:

  - junior-engineer      # Must exist in audiences.yml

platforms:

  - github-copilot       # Must exist in platforms.yml
  - claude

tags:

  - code-generation      # Must exist in topics.yml

---
```

### For Validation

The validation tool checks prompts against these schemas:

```bash
# Validate all prompts against data schemas
python tools/validate_prompts.py --all

# Check for invalid audience values
grep -r "audience:" prompts/ | # Check against audiences.yml
```

#### Expected output format (validation)

`python tools/validate_prompts.py --all` prints:

- A list of files with issues (only when issues exist)
- A final summary line: `Files with issues: N`
- Exit code:
  - `0` when `N == 0`
  - `1` when `N > 0`

**Example (success):**

```text
==================================================
Files with issues: 0
```

**Example (failure):**

```text
prompts/developers/example.md:
  - Missing frontmatter: name
  - Missing frontmatter: description
  - Missing section: Example

==================================================
Files with issues: 1
```

#### Troubleshooting: common validation failures

##### 1) Missing frontmatter fields

- **Symptom**:
  - `Missing frontmatter: name`
  - `Missing frontmatter: description`
  - `Missing frontmatter: type (defaulting to 'how_to')`
- **Fix**: Add/fill frontmatter at the top of the Markdown file (see `prompts/templates/prompt-template.md`).

##### 2) Invalid enum values

- **Symptom**:
  - `Invalid frontmatter: difficulty 'expert' must be one of: beginner, intermediate, advanced`
  - `Invalid frontmatter: response_format 'xml' must be one of: text, json_object, json_schema`
- **Fix**: Replace the value with an allowed one, or remove the field if it‚Äôs not needed.

##### 3) Missing required sections

- **Symptom**:
  - `Missing section: Prompt`
  - `Missing section: Variables`
  - `Missing section: Example`
- **Fix**: Add the missing `##` headings. For prompts, the standard expectation is:
  - `## Description`
  - `## Prompt`
  - `## Variables`
  - `## Example`

##### 4) YAML frontmatter parsing issues

- **Symptom**: A file that *looks* like it has frontmatter still reports missing fields.
- **Fix**: Check for common YAML pitfalls:
  - inconsistent indentation
  - tabs instead of spaces
  - missing closing `---`
  - duplicate frontmatter blocks

### For Evaluation (PromptEval)

Prompt evaluation writes (or prints) structured JSON results including per-file scores and improvement suggestions.

```bash
# Evaluate a folder (tiered). Writes JSON output to a file.
python -m tools.prompteval prompts/analysis --tier 2 --verbose --ci -o results/analysis-tier2.json

# Evaluate a single file. Prints JSON output to stdout.
python -m tools.prompteval prompts/developers/some-prompt.md --tier 2
```

#### Expected output format (evaluation)

By default, `python -m tools.prompteval ...` prints JSON to stdout. With `-o/--output`, it writes JSON to the specified file.

**Batch evaluation output** is a JSON object containing a `results` array and summary fields:

```json
{
  "results": [
    {
      "file": "...",
      "score": 8.9,
      "grade": "B",
      "passed": true,
      "criteria": {
        "clarity": 9.0,
        "effectiveness": 9.0,
        "structure": 9.0,
        "specificity": 9.0,
        "completeness": 8.0
      },
      "improvements": [],
      "model": "gh:gpt-4o-mini",
      "method": "geval",
      "tier": 2,
      "duration": 3.12,
      "error": null
    }
  ],
  "total": 1,
  "passed": 1,
  "failed": 0,
  "errors": 0,
  "avg_score": 8.9,
  "duration": 3.12
}
```

### For Learning Path Navigation

Use learning tracks to guide users:

```python
# Load learning track
import yaml
with open("data/learning-tracks/engineer-quickstart.yml") as f:
    track = yaml.safe_load(f)

# Display recommended sequence
for module in track["modules"]:
    print(f"- {module['title']} ({module['duration']})")
```

## üìù Extending the Data Schema

### Adding a New Audience

1. Edit `audiences.yml`:

   ```yaml

   - id: new-role

     name: New Role
     description: Description of the role
     typical_needs:

       - Need 1
       - Need 2

     learning_track: appropriate-track
   ```

2. Update relevant learning tracks if needed

3. Commit and push changes

### Adding a New Topic

1. Edit `topics.yml`:

   ```yaml

   - id: new-topic

     name: New Topic
     description: What this topic covers
   ```

2. Tag existing prompts if applicable

3. Update documentation to reference the new topic

### Adding a New Platform

1. Edit `platforms.yml`:

   ```yaml

   - id: new-platform

     name: New Platform
     description: Platform description
     capabilities:

       - Feature 1
       - Feature 2

   ```

2. Test prompts on the new platform

3. Update prompt frontmatter with `platforms: [new-platform]`

## üîÑ Data Maintenance

### Update Frequency

- **audiences.yml**: Updated when new personas are identified (quarterly)
- **platforms.yml**: Updated when new AI platforms are added (as needed)
- **topics.yml**: Updated when new categories emerge (quarterly)
- **db/*.json**: Updated automatically during evaluation runs

### Validation

All data files are validated on commit:

```bash
# Validate YAML syntax
yamllint data/*.yml data/learning-tracks/*.yml

# Validate JSON syntax
jq . data/db/*.json

# Validate prompt schema (frontmatter + required sections)
python tools/validate_prompts.py --all

# Validate registry schema (prompts/registry.yaml)
python tools/validators/registry_validator.py
```

## üìä Data-Driven Features

These data files enable several key features:

1. **Smart Filtering**: Filter prompts by audience, platform, or topic
2. **Learning Paths**: Guided sequences for different personas
3. **Validation**: Ensure prompts meet quality standards
4. **Analytics**: Track prompt usage and effectiveness
5. **Recommendations**: Suggest prompts based on user context

## ü§ù Contributing

When contributing data changes:

1. **Maintain backward compatibility**: Don't remove existing IDs
2. **Add, don't delete**: Deprecate old values, don't remove them
3. **Document changes**: Update this README with new fields
4. **Test validation**: Ensure existing prompts still validate

See [CONTRIBUTING.md](../CONTRIBUTING.md) for detailed guidelines.

## üìö Related Resources

- **[Docs](../docs/)**: Documentation and guides
- **[Prompt Library](../prompts/)**: The actual prompt collection
- **[Tools](../tools/)**: CLI and validation utilities
- **[PromptEval](../tools/prompteval/)**: Prompt evaluation runner

## üìÑ License

All data files are licensed under [MIT License](../LICENSE).

---

**Questions about data schemas?** Open an issue in the [main repository](https://github.com/tafreeman/prompts).
