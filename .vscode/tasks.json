{
	"version": "2.0.0",
	"inputs": [
		{
			"id": "evalFolder",
			"type": "pickString",
			"description": "Select folder to evaluate",
			"options": [
				"prompts/",
				"prompts/advanced/",
				"prompts/analysis/",
				"prompts/business/",
				"prompts/creative/",
				"prompts/developers/",
				"prompts/governance/",
				"prompts/m365/",
				"prompts/reasoning/",
				"prompts/socmint/",
				"prompts/system/"
			],
			"default": "prompts/"
		},
		{
			"id": "evalTier",
			"type": "pickString",
			"description": "Select evaluation tier",
			"options": [
				"1 - Quick Triage (FREE, <1s)",
				"2 - Single Model (FREE, 30s)",
				"3 - Cross-Validate (FREE, 3 models)",
				"4 - Full Pipeline (FREE, 5 models)",
				"5 - Premium (FREE, all gh-models)"
			],
			"default": "1 - Quick Triage (FREE, <1s)"
		}
	],
	"tasks": [
		{
			"label": "ðŸ“Š Eval: Run Tiered Evaluation",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"${input:evalTier/^(\\d).*//$1/}"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 0 - Structural Only (instant)",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"0"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 1 - Local Quick (phi4)",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"1"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 2 - Local G-Eval (default)",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"2"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 3 - Local Cross-Validate",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"3"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 4 - Cloud Quick (gpt-4o-mini)",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"4"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Tier 5 - Cloud Cross-Validate",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${input:evalFolder}",
				"--tier",
				"5"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“Š Eval: Current File Only",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"${file}",
				"--tier",
				"${input:evalTier/^(\\d).*//$1/}"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ“‹ Eval: List All Tiers",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"--list-tiers"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			}
		},
		{
			"label": "ðŸ“‹ Eval: List Available Models",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"prompteval",
				"--list-models"
			],
			"options": {
				"cwd": "${workspaceFolder}/tools"
			},
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			}
		},
		{
			"label": "ðŸ§ª Run Python Tests",
			"type": "shell",
			"command": "python",
			"args": [
				"-m",
				"pytest",
				"testing/",
				"-v"
			],
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			},
			"group": "test"
		},
		{
			"label": "ðŸ” Validate All Prompts",
			"type": "shell",
			"command": "python",
			"args": [
				"tools/validate_prompts.py",
				"--all"
			],
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			}
		},
		{
			"label": "ðŸ“ˆ Analyze Prompt Library",
			"type": "shell",
			"command": "python",
			"args": [
				"tools/analyzers/prompt_analyzer.py",
				"--all"
			],
			"problemMatcher": [],
			"presentation": {
				"reveal": "always",
				"panel": "new"
			}
		}
	]
}