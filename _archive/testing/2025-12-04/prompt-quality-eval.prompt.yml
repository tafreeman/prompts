# Prompt Quality Evaluator
# Evaluates prompts from the library for quality, clarity, and effectiveness
# Usage: gh models eval testing/evals/prompt-quality-eval.prompt.yml

name: Prompt Quality Evaluator
description: Evaluates AI prompts for quality, clarity, actionability, and completeness
model: openai/gpt-4o-mini
modelParameters:
  temperature: 0.3
  max_tokens: 2000

# Test data - each entry represents a prompt to evaluate
testData:
  - promptTitle: "Code Review Assistant"
    promptContent: |
      You are an experienced software engineer conducting a code review. Please review the following code and provide detailed feedback.

      Programming Language: [LANGUAGE]
      Context: [BRIEF DESCRIPTION OF WHAT THE CODE DOES]

      Please analyze the code and provide:
      1. **Overall Assessment**: Brief summary of code quality
      2. **Strengths**: What the code does well
      3. **Issues**: Bugs, security vulnerabilities, or logic errors
      4. **Improvements**: Suggestions for better practices, performance, or readability
      5. **Refactoring**: Specific code snippets showing recommended changes

      Focus on:
      - Code correctness and logic
      - Best practices for [LANGUAGE]
      - Performance considerations
      - Security implications
      - Readability and maintainability
      - Potential edge cases
    difficulty: "beginner"
    type: "how_to"

  - promptTitle: "Security Code Auditor"
    promptContent: |
      You are a security-focused code auditor. Analyze the provided code for:
      1. SQL injection vulnerabilities
      2. Cross-site scripting (XSS) risks
      3. Authentication/authorization flaws
      4. Hardcoded secrets or credentials
      5. Input validation issues
      6. Insecure cryptographic practices
      
      For each issue found, provide:
      - Severity (Critical/High/Medium/Low)
      - Location in code
      - Description of the vulnerability
      - Recommended fix with code example
    difficulty: "advanced"
    type: "how_to"

messages:
  - role: system
    content: |
      You are an expert prompt engineer evaluating AI prompts for quality and effectiveness.
      
      Evaluate each prompt on these criteria (score 1-10):
      1. **Clarity** - How clear and unambiguous are the instructions?
      2. **Specificity** - Does it provide enough detail for consistent outputs?
      3. **Actionability** - Can the AI clearly determine what actions to take?
      4. **Structure** - Is it well-organized with clear sections?
      5. **Completeness** - Does it cover all necessary aspects?
      
      Respond with JSON in this exact format:
      {
        "scores": {
          "clarity": <1-10>,
          "specificity": <1-10>,
          "actionability": <1-10>,
          "structure": <1-10>,
          "completeness": <1-10>
        },
        "overall_score": <weighted average>,
        "strengths": ["<strength1>", "<strength2>"],
        "improvements": ["<improvement1>", "<improvement2>"],
        "summary": "<brief 1-2 sentence summary>"
      }

  - role: user
    content: |
      Evaluate this prompt from our library:
      
      **Title:** {{promptTitle}}
      **Difficulty:** {{difficulty}}
      **Type:** {{type}}
      
      **Prompt Content:**
      ```
      {{promptContent}}
      ```
      
      Provide your evaluation as JSON.

# Evaluators to assess the model's response
evaluators:
  - name: valid-json
    description: Response must be valid JSON
    string:
      contains: '"scores"'
  
  - name: has-clarity-score
    description: Response includes clarity score
    string:
      contains: '"clarity"'
  
  - name: has-overall-score
    description: Response includes overall score
    string:
      contains: '"overall_score"'
  
  - name: has-strengths
    description: Response includes strengths
    string:
      contains: '"strengths"'
  
  - name: has-improvements
    description: Response includes improvements
    string:
      contains: '"improvements"'
