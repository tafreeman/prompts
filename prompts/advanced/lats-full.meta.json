{
  "id": "lats-full",
  "title": "LATS Self-Refine: Iterative Multi-Branch Prompt Evaluator",
  "description": "Full LATS evaluator combining LATS, Self-Refine, ToT, ReAct, and CoVe patterns for comprehensive prompt evaluation",
  "version": "1.0.0",
  "category": "evaluation",
  "tags": ["lats", "self-refine", "tree-of-thoughts", "react", "cove", "iterative-refinement"],
  
  "prompt_file": "lats-full.prompt.txt",
  "lite_version": "lats-lite.prompt.txt",
  "documentation": "lats-self-refine-evaluator.md",
  
  "variables": {
    "PROMPT_CONTENT": {
      "required": true,
      "description": "The full text of the prompt to evaluate",
      "max_length": 4000
    },
    "QUALITY_THRESHOLD": {
      "required": false,
      "default": "80",
      "description": "Minimum acceptable score (0-100) to stop iteration"
    },
    "MAX_ITERATIONS": {
      "required": false,
      "default": "5",
      "description": "Maximum refinement loops before stopping"
    },
    "GRADING_CRITERIA": {
      "required": false,
      "default": "{\"clarity\": 25, \"effectiveness\": 30, \"specificity\": 20, \"completeness\": 25}",
      "description": "JSON object with criteria names and weights"
    }
  },
  
  "criteria": {
    "clarity": {"weight": 25, "description": "Goal and role clarity"},
    "effectiveness": {"weight": 30, "description": "Produces good results"},
    "specificity": {"weight": 20, "description": "Precise instructions with examples"},
    "completeness": {"weight": 25, "description": "Edge cases and output format"}
  },
  
  "branches": {
    "A": {
      "name": "Criteria Validation",
      "pattern": "CoVe",
      "purpose": "Verify grading criteria against research"
    },
    "B": {
      "name": "Scoring & Feedback", 
      "pattern": "G-Eval + Self-Refine",
      "purpose": "Score prompt and generate improvements"
    },
    "C": {
      "name": "Implementation",
      "pattern": "ReAct",
      "purpose": "Apply fixes with thought-action-observe cycle"
    }
  },
  
  "model_compatibility": {
    "optimized_for": ["gpt-4", "gpt-4o", "claude-3-opus", "o1", "deepseek-r1"],
    "requires_context": 8192,
    "estimated_tokens": {
      "input": 2000,
      "output": 4000
    }
  },
  
  "performance": {
    "size_bytes": 5500,
    "size_tokens": 1500,
    "recommended_for": "cloud models with large context windows"
  },
  
  "research_foundation": [
    {"pattern": "LATS", "paper": "Zhou et al., 2023", "arxiv": "2310.04406"},
    {"pattern": "Self-Refine", "paper": "Madaan et al., 2023", "arxiv": "2303.17651"},
    {"pattern": "ToT", "paper": "Yao et al., 2023", "arxiv": "2305.10601"},
    {"pattern": "ReAct", "paper": "Yao et al., 2022", "arxiv": "2210.03629"},
    {"pattern": "CoVe", "paper": "Dhuliawala et al., 2023", "arxiv": "2309.11495"},
    {"pattern": "Reflexion", "paper": "Shinn et al., 2023", "arxiv": "2303.11366"}
  ],
  
  "output_format": {
    "branch_a_criteria_json": true,
    "branch_b_scores_table": true,
    "branch_b_suggestions_json": true,
    "branch_c_before_after": true,
    "synthesis_decision_json": true,
    "reflexion_learnings": true,
    "final_summary_table": true,
    "improvement_history": true
  }
}
